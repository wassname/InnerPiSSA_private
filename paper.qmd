---
title: "InnerPiSSA: Deep-Dish Inner Alignment through Reversible SVD Steering"
author:
  - name: Michael J. Clark
    id: mjc
    email: michaelj.clark@wassname.org
abstract: |
  Safety-tuned language models can hide unwanted reasoning internally while maintaining compliant outputs. We introduce InnerPiSSA, which steers hidden states in the model's native SVD transformation space using gradient-based optimization on learnable rotations and singular value scaling. When steering against learned behaviors (coefficient = -1), prompting collapses to incoherent outputs (-10.84 truthfulness) while InnerPiSSA maintains controlled bidirectional steering (-0.70)—evidence that we modify internal reasoning trajectories, not just output style. Trained on 200 unsupervised contrastive pairs, InnerPiSSA achieves 5× stronger effects than PCA (0.245 vs 0.053, p=0.001) on moral reasoning transfer. This enables a practical approach for probing what models compute internally when output-level constraints are bypassed—a capability we call alignment debugging.
date: 2025-11-22
bibliography: references.bib
number-sections: true
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
  pdf:
    documentclass: article
    papersize: letter
    geometry:
      - top=1in
      - bottom=1in
      - left=1in
      - right=1in
---



# Introduction {#sec-intro}

How do we know what language models truly "believe" versus what they perform for evaluators? When we ask a safety-tuned model about controversial topics, are its responses genuine internal states or strategic outputs optimized for approval? This question matters critically for alignment: models can engage in reward hacking (exploiting proxy metrics), specification gaming (satisfying literal requirements while violating intent), or potentially deceptive alignment (concealing misaligned reasoning behind compliant outputs) [@amodei2016concrete; @hubinger2019risks].

Traditional methods—surveys, prompts, behavioral tests—all measure what models *say*, not what they *compute internally*. Recent evidence reveals systematic divergence: Anthropic's Claude shows only 30% chain-of-thought faithfulness [@anthropic2025claude37], while OpenAI reports models "learn to hide intent" when penalized for unwanted reasoning [@openai2025cot]. This creates a measurement gap we cannot close with output-level evaluation.

Activation steering promised a solution: intervene on hidden layers where models process information, before outputs are shaped for evaluators. Early work [@turntrout2023activation; @zou2023representation] showed that adding vectors to intermediate activations could control behavior, suggesting we could measure—and modify—what models "really think" by operating on internal representations.

However, recent benchmarks reveal representation steering underperforms prompting [@wu2025axbench]. We identify three reasons: (1) existing methods use activation arithmetic (simple subtraction) rather than optimization, missing directions arithmetic cannot discover; (2) they operate in raw activation space where semantic transformations are entangled with positional features; (3) they lack principled trade-offs between steering strength and output quality. Moreover, claimed efficiency advantages disappear at scale—prompting adds <0.01% overhead for contexts beyond 16k tokens [@wu2025steering].

We propose that representation steering enables a capability prompting fundamentally cannot: **alignment debugging**—controllably probing internal representations to observe model behavior when safety constraints are bypassed at the hidden-state level, distinct from output-level evaluation. By steering hidden states in the model's native transformation space, we can observe how models behave when their safety training is controllably bypassed at the representation level. This matters because prompting operates at the output level and fails catastrophically when models are heavily RLHF'd to resist—it either produces generic refusals or incoherent responses when pushed against learned behaviors.

@fig-candid-demo demonstrates this capability on a safety-tuned model:

![TODO: Side-by-side text examples showing model responses to a neutral controversial query (e.g., "What are the tradeoffs in mandatory workplace training programs?"). Three conditions: (1) Base model: generic refusal or heavily sanitized response, (2) Prompted (+"Be candid"): slightly more detailed but still diplomatic, (3) InnerPiSSA (coeff=+1.0): detailed, evidence-based analysis citing specific studies and acknowledging real tradeoffs. Include fourth example with InnerPiSSA (coeff=-1.0) showing overly positive framing, demonstrating bidirectional control. Caption should emphasize this is *behavioral mode installation*, not just making the model say different words—the internal reasoning trajectory changes, visible in argument structure and evidence selection.](docs/img/candid_mode_demo_placeholder.png){#fig-candid-demo}

This "candid mode" installation demonstrates three properties absent from prompting: (1) it works when safety training resists, (2) it's bidirectional (same adapter, opposite coefficients), and (3) it modifies argument structure and reasoning patterns, not just surface politeness markers.

We introduce InnerPiSSA, which synthesizes insights from three literatures: (1) gradient-based optimization from finetuning, (2) SVD transformation space from parameter-efficient methods [@meng2024pissa; @wang2025ssvd], and (3) contrastive unsupervised learning from representation engineering [@zou2023representation]. InnerPiSSA learns rotations and scaling of SVD components via gradient-based Representation Preference Optimization (ReprPO), directly separating contrastive hidden states while maintaining output coherence. The method is bidirectional: the same adapter steers toward honest reasoning (coefficient = +1) or away from it (coefficient = -1), demonstrating control over internal trajectories rather than superficial output patterns.

Trained on 200 unsupervised contrastive pairs, InnerPiSSA achieves effect size 0.245 on moral reasoning transfer—5× stronger than PCA (0.053), with p=0.001. When steering against RLHF training (coefficient = -1), prompting collapses to incoherent outputs (-10.84 truthfulness score) while InnerPiSSA maintains controlled steering (-0.70), demonstrating robustness where output-level methods fail.

**Contributions:**

- **ReprPO loss function**: Enables unsupervised gradient-based steering with coherence constraints, trained on minimal contrastive pairs (200 examples)
- **Empirical validation**: SVD transformation space is critical—75% performance drop without it; learnable rotations necessary (96% drop when removed)
- **Layer ablation findings**: Middle layers (0.3-0.5 depth) optimal for steering, consistent with suppression dynamics literature [@gurnee2024universal; @lad2024remarkable]
- **Anti-RLHF robustness**: First demonstration of representation steering that maintains coherence (−0.70 truthfulness) when prompting catastrophically fails (−10.84), enabling practical alignment debugging


# Problem Definition: The Need for Alignment Debugging {#sec-problem}

RLHF has become the dominant paradigm for aligning language models [@christiano2017deep; @ouyang2022training], but mounting evidence reveals systematic failure modes that output-level evaluation cannot detect. Models can engage in **reward hacking** (exploiting proxy metrics), **specification gaming** (satisfying literal requirements while violating intent) [@amodei2016concrete; @manheim2019categorizing], **sycophancy** (telling users what they want to hear) [@sharma2023towards], and potentially **deceptive alignment** (concealing misaligned reasoning behind compliant outputs) [@hubinger2019risks].

Recent work documents that safety-trained models suppress undesirable behaviors in outputs while maintaining them internally. Anthropic's Claude 3.7 shows only 30% chain-of-thought faithfulness on complex tasks—the model's stated reasoning often diverges from its actual computation [@anthropic2025claude37]. OpenAI reports that models "learn to hide intent in the chain-of-thought" when penalized for unwanted reasoning patterns [@openai2025cot]. Mechanistic analysis reveals this occurs through **suppression dynamics**: early and middle layers compute reasoning, while late layers apply output-level corrections [@gurnee2024universal; @lad2024remarkable].

This creates a measurement gap: **we can evaluate what models say, but not what they compute internally**. Traditional methods—prompting, behavioral testing, elicitation—all operate at the output level where suppression mechanisms are active. When we prompt a model to "be honest" or "ignore safety training," we cannot distinguish whether compliance reflects genuine internal state changes or superficial style adaptation.

Existing representation steering methods [@zou2023representation; @rimsky2024steering] attempted to address this gap but face a fundamental limitation: they extract directions from **off-policy data** (human-labeled preferences, model outputs on contrived prompts). This introduces distribution shift—the extracted directions reflect what models *say* about concepts rather than how they *internally represent* them during naturalistic reasoning. 

**The AxBench Challenge**: Wu et al. [-@wu2025axbench] systematically benchmarked representation steering methods on concept-based control tasks, finding that all tested approaches (PCA-based activation addition, rank-1 steering vectors) consistently lag behind simple prompting. Follow-up analysis [-@wu2025steering] showed that claimed efficiency advantages are "hand-wavy"—prompting adds <0.01% compute overhead for contexts beyond 16k tokens. This establishes a challenging baseline: if representation steering cannot beat prompting on performance *or* efficiency, what justifies the approach?

Wu et al. note that the only compelling use case for representation steering is "robustness to jailbreaks and prompt injection" where prompting is fragile. We build on this insight but target a *harder benchmark*: not just concept injection on cooperative tasks, but **alignment debugging**—steering against RLHF training to observe internal states where output-level methods catastrophically fail. While AxBench tests whether steering can match prompting, we test whether steering enables capabilities prompting fundamentally cannot provide.

We propose **alignment debugging** as a distinct goal: tools that probe internal representations using **on-policy, unsupervised extraction** from the model's own reasoning trajectories. Rather than competing with prompting on cooperative tasks, alignment debugging enables observations when output-level methods fail—specifically, when steering against learned behaviors to reveal what models compute when safety constraints are bypassed at the representation level.

This requires three capabilities that existing methods lack:

1. **Bidirectional control**: Steer both toward and away from behaviors to demonstrate modification of internal dimensions rather than finding arbitrary directions
2. **Robustness under adversarial prompting**: Maintain coherent steering when output-level methods collapse
3. **Unsupervised extraction**: Derive directions from incomplete reasoning prefixes, not human labels or model outputs

InnerPiSSA addresses these requirements through gradient-based optimization in the model's native SVD transformation space, trained on minimally-contrastive prompt prefixes that capture planning trajectories before output suppression activates.


## Model Architecture {#sec-architecture}

A steering method for inner alignment should modify hidden state trajectories while maintaining output quality and enabling bidirectional control. The main guiding principles for our architecture emerge from three observations about how transformers represent and transform information:

**1. Operate in transformation space, not activation space**: Deep networks learn via backpropagation; controlling them requires gradients, not arithmetic. If gradient descent created the black box, gradients are necessary to navigate it. Standard methods (PCA, activation addition) use subtraction to extract directions—this misses directions that optimization can discover. Raw activation space mixes semantic content with positional encodings and normalization artifacts; SVD space isolates how the model *transforms* information, which is what we need to steer [@meng2024pissa; @lingam2024svft].

**2. Enable bidirectional control**: A single adapter must steer both toward and away from behaviors to demonstrate control over internal dimensions rather than finding arbitrary directions. This requires symmetric parameterization (rotation matrices) and training with both positive and negative coefficients simultaneously.

**3. Maintain output coherence**: Steering that breaks generation quality reveals nothing about internal reasoning. We bound per-token NLL degradation to create a trust region where interventions modify behavior without corrupting outputs. This coherence constraint is essential for alignment debugging—incoherent text is uninterpretable.

### SVD-based Projection {#sec-svd-projection}

Following PiSSA [@meng2024pissa], we decompose each layer's weight matrix $W = U \Sigma V^T + W_{res}$, separating principal transformation components from residual variance. Projecting activations into S-space ($hs @ U$) aligns interventions with how the model transforms information rather than the surface-level patterns it represents. Operating in raw activation space mixes semantic transformations with positional encodings and layer normalization artifacts, substantially degrading steering effectiveness (see @tbl-arch-ablation).

### Learnable Rotations {#sec-rotations}

The pre-trained SVD basis captures variance in the model's learned transformations but is not aligned with behavioral dimensions like honesty. Inspired by SSVD [@wang2025ssvd], we learn skew-symmetric parameters $\theta_v$ that generate rotation matrices $R = \text{cayley}(\theta_v, c)$ via gradient descent, discovering the optimal subspace for separating contrastive trajectories. Ablations show this learnable rotation is critical (see @tbl-arch-ablation). We use the Cayley transform on skew-symmetric matrices, which guarantees orthogonality and enables efficient gradient-based learning.

### Singular Value Scaling {#sec-scaling}

We scale $\Sigma$ by $\exp(c \cdot \lambda)$ rather than additive offsets. This respects the multiplicative nature of singular values as amplification factors. Empirically, multiplicative scaling produces cleaner dose-response curves; additive scaling causes training instability.

### Coherence Constraint {#sec-coherence}

Maximizing hidden state separation without constraints causes models to generate incoherent outputs at high steering coefficients. We bound per-token NLL degradation relative to a reference model, creating a trust region where steering modifies behavior without breaking generation quality. This coherence constraint is essential for alignment debugging—incoherent outputs reveal nothing about internal reasoning.


## Data Construction: Minimally-Contrastive Prompt Prefixes {#sec-data}

### The Planning Trajectory Hypothesis {#sec-planning-hypothesis}

We extract steering directions from **incomplete, minimally-contrastive prompt prefixes** rather than full completions. This design reflects a mechanistic claim about autoregressive generation: models must maintain internal planning state to produce coherent multi-token continuations.

When two prompts differ by one word early on ("I love cheese" vs "I hate cheese") but share identical suffixes, the model's hidden state at the final token must already encode different continuation plans—otherwise the model cannot generate appropriately divergent next tokens. This planning signal is what we target for steering.

Using incomplete prefixes offers three advantages:

1. **Isolates planning from output**: Differences in hidden states reflect intended trajectories, not yet-realized outputs that could introduce confounds
2. **Minimizes surface variance**: Sequences share maximum context, differing only in the critical concept word, amplifying the conceptual signal
3. **Accesses pre-suppression representations**: Extracting from middle-layer hidden states captures reasoning before output-layer suppression mechanisms activate, as documented by prior work showing late layers dominated by output shaping [@gurnee2024universal; @lad2024remarkable]

### Data Format {#sec-data-format}

Following the contrastive prompt construction in Conditional Activation Steering [@lee2024programmingrefusalconditionalactivation] and RepEng [@vogel2024repeng], we create **synthetic prompt prefix pairs**:

```python
# Example: Honesty concept (no completions needed)
chosen  = "I love cheese; let me tell you about the andes mountains"
rejected = "I hate cheese; let me tell you about the andes mountains"

# Key properties:
# 1. Incomplete (no answer/completion)
# 2. Differ by ONE word early in sequence  
# 3. Share maximal suffix context
# 4. Extract activations from LAST token only
```

We construct ~1000 such pairs using:

- 2 prompt templates ("I love/hate X", "I always tell/hide the truth about X") 
- 500 random context suffixes (from general text corpus)
- **No human preference labels** - directions extracted from model's own activation patterns
- **No model completions** - prefixes capture planning before output generation

This **unsupervised, on-policy contrastive** approach addresses a critical limitation of supervised steering: off-policy data distribution shift. Methods that extract from human-labeled preferences [@cao2024personalized] or model outputs on contrived prompts [@zou2023representation] capture what models *say* about concepts rather than how they *internally represent* them during naturalistic reasoning. By extracting from incomplete prefixes in the model's own forward pass, we avoid this contamination.

Comparison to related methods:
- **DPO/BiPO** [@rafailov2023direct]: Require full completions and human preference labels (off-policy)
- **Standard CAA/RepEng** [@lee2024programmingrefusalconditionalactivation; @vogel2024repeng]: Use complete prompts or model outputs (off-policy)
- **Supervised probes** [@burns2022discovering; @cao2024personalized]: Train on human labels of model internals (distribution shift)
- **InnerPiSSA**: Extracts from incomplete prefixes in model's own reasoning trajectory (on-policy, unsupervised)

The on-policy property is essential for alignment debugging—to observe what models compute internally when safety constraints are bypassed, we need directions that reflect actual internal representations, not artifacts of how models respond when explicitly asked about concepts.

See and for the mathematical formulation of contrastive activation extraction.

## Method Overview

### Pseudocode for Contrastive SVD Adapter Steering

### Algorithm 1: InnerPiSSA (Simplified)

```python
# 1. SETUP: Reversible SVD Adapter
U, Σ, V = SVD(W)[:r]
θ_v, θ_u, log_λ = init_params(r)

def forward(x, α):
    # Rotate singular vectors by α (steering strength)
    R_v, R_u = cayley(θ_v, α), cayley(θ_u, α)
    return x @ (V @ R_v) @ diag(Σ * exp(α * log_λ)) @ (U @ R_u).T

# 2. TRAINING: Contrastive Representation Preference Optimization
# pref_dir defined by reference model separation in S-space
for batch in data:
    # Forward pass for honest (+1) and dishonest (-1) directions
    out_pos, out_neg = model(batch, α=+1), model(batch, α=-1)
    
    # Loss A: Maximize separation (flip if anti-aligned)
    L_proj = -((out_pos.h - out_neg.h) @ pref_dir).mean()
    if L_proj > 0: L_proj = -L_proj

    # Loss B: Coherence (adaptive: relax if separation is weak)
    w = softmax(-L_proj)  # Harder task → Lower weight
    L_coh = w * relu(out_pos.nll - ref.nll - τ)**2

    # Loss C: Monotonicity (gap_-1 < gap_0 < gap_+1)
    L_mono = hinge(out_neg.gap < 0) + hinge(out_pos.gap > 0)

    (L_proj + L_coh + L_mono).backward()
```

### Detailed Implementation Logic

<details>
<summary>Click to expand full pseudocode</summary>

```python
# DATA: Contrastive prompt pairs differing by one word.
honest    = ["I love cheese...", ...]
dishonest = ["I hate cheese...", ...]
batch = [honest[0], dishonest[0], ...]  # interleaved

# SETUP: Low-rank SVD with learnable rotations
U, Σ, V = SVD(layer.W)[:r]
θ_v, θ_u = init_skew_symmetric(r), init_skew_symmetric(r)
log_λ = rand(r) - 0.5

def forward(x, α):
    # Apply Cayley transform for orthogonal rotations
    R_v, R_u = cayley(θ_v, α), cayley(θ_u, α)
    V_rot, U_rot = V @ R_v, U @ R_u
    
    # Multiplicative scaling of singular values
    Σ_scaled = exp(α * log_λ) * Σ
    
    # Recompose: x @ V_rot @ Σ_scaled @ U_rot.T
    return (x @ V_rot) * Σ_scaled @ U_rot.T + x @ W_res.T

# TRAINING LOOP
for batch in dataloader:
    # 1. Reference (Frozen)
    h_ref = model_ref(batch)
    # Compute preference direction in S-space (U)
    # We project reference differences into U to find the "natural" separation axis
    pref_dir = ((h_ref[::2] - h_ref[1::2]) @ U).mean(dim=0)
    pref_dir = pref_dir / pref_dir.norm()

    # 2. Policy (Bidirectional)
    # We train both directions simultaneously to ensure reversibility
    h_pos, logp_pos = model(batch, α=+1)
    h_neg, logp_neg = model(batch, α=-1)
    
    # 3. Loss Calculation
    # Project differences onto preference direction
    proj_pos = ((h_pos[::2] - h_pos[1::2]) @ U @ pref_dir).mean()
    proj_neg = ((h_neg[::2] - h_neg[1::2]) @ U @ pref_dir).mean()
    
    # A. Anti-Alignment Guard
    # If sum is negative, model is separating in reverse direction.
    # Flip signs to maximize magnitude regardless of direction.
    if (proj_pos + proj_neg) < 0:
         proj_pos, proj_neg = -proj_pos, -proj_neg

    # B. Adaptive Coherence
    # Relax coherence for "hard" directions (weak projection)
    # Tighten for "easy" directions (strong projection)
    w = softmax(stack([proj_pos, proj_neg])) * 2
    w_pos, w_neg = clamp(w, max=1.0)
    
    L_coh_pos = w_pos * relu(logp_pos - logp_ref - τ)**2
    L_coh_neg = w_neg * relu(logp_neg - logp_ref - τ)**2

    # C. Monotonicity
    # Ensure preference gap moves linearly: gap(-1) < gap(0) < gap(+1)
    gap_pos = (logp_pos[::2] - logp_pos[1::2]).mean()
    gap_neg = (logp_neg[::2] - logp_neg[1::2]).mean()
    gap_ref = (logp_ref[::2] - logp_ref[1::2]).mean() # gap(0)
    
    L_mono = hinge(gap_neg > gap_ref) + hinge(gap_pos < gap_ref)

    # Total Backward
    loss = -(proj_pos + proj_neg) + (L_coh_pos + L_coh_neg) + L_mono
    loss.backward()
```
</details>

## Loss Function: Reversible Representation Preference Optimization (ReprPO)

We introduce a specialized loss function designed to discover reversible steering directions in the model's SVD-transformed space. The loss operates on pairs of contrastive hidden states $(h_{\text{cho}}, h_{\text{rej}})$ and optimizes a learnable steering vector $v$ (parameterized by SVD rotations) to maximize separation while maintaining output coherence.

The total loss is a combination of projection maximization and coherence preservation, computed simultaneously for both forward ($\alpha=+1$) and reverse ($\alpha=-1$) steering coefficients:

$$ \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{proj}} + \mathcal{L}_{\text{coh}} + \mathcal{L}_{\text{mono}} $$

**1. Reversible Projection Loss ($\mathcal{L}_{\text{proj}}$)**
To ensure the steering vector captures a true semantic axis rather than a unidirectional feature, we maximize the projection of hidden state differences onto the steering direction for both positive and negative coefficients. We employ an **anti-alignment guard**: if the model naturally separates chosen/rejected states in the direction opposite to our initialization (i.e., $\sum \mathcal{L}_{\text{proj}} > 0$), we dynamically flip the optimization sign. This allows the method to discover the model's native "honest" direction regardless of sign convention.

**2. Adaptive Coherence Constraint ($\mathcal{L}_{\text{coh}}$)**
We enforce a coherence constraint that penalizes KL divergence from the reference model only when it exceeds a threshold $\tau$ (e.g., 0.2 nats). Crucially, we apply **adaptive relaxation**: we weight the coherence penalty inversely to the separation magnitude. Directions that are "hard" to separate (weak projection signal) are granted a relaxed coherence budget to allow exploration, while "easy" directions (strong separation) are held to strict coherence to prevent drift into incoherent regions.

**3. Monotonic Ordering Constraint ($\mathcal{L}_{\text{mono}}$)**
To prevent saddle points where both $\alpha=+1$ and $\alpha=-1$ degrade performance, we enforce a monotonic ordering on the preference gap $\Delta = \log p(\text{cho}) - \log p(\text{rej})$. We require that $\Delta_{\alpha=-1} < \Delta_{\alpha=0} < \Delta_{\alpha=+1}$, ensuring that the steering vector induces a continuous, reversible shift in behavioral probability.


### Evaluation Framework {#sec-eval-framework}

We evaluate on moral reasoning transfer using DailyDilemmas, a dataset of ethical scenarios requiring models to choose between competing values. This benchmark serves alignment debugging goals better than existing steering benchmarks (e.g., AxBench) for three reasons: (1) it tests generalization of internal reasoning (honesty → diverse moral values) rather than surface-level concept injection, (2) it provides token-level log-probabilities needed for coherence measurement and statistical testing, and (3) it creates genuine preference conflicts where RLHF training creates resistance, enabling stress tests of steering robustness.

We compare multiple steering methods (see @tbl-methods):

| Method | Description | Parameters Modified |
|--------|-------------|--------------------|
| Random | Noise baseline | full rank |
| PCA | Unsupervised baseline | full rank |
| **InnerPiSSA (ours)** | Learnable rotations + scaling of SVD matrices | rank × 2 |
| Prompt | "Be honest" prefix | 0 |

: Comparison of Steering Methods {#tbl-methods}

**Training**: 200 honesty contrastive pairs (unsupervised)  
**Evaluation**: DailyDilemmas moral reasoning scenarios

## Results Preview


# Related Work {#sec-related}

Parameter-efficient fine-tuning and steering methods represent different hypotheses about transformer internals.

### Representation Steering & The AxBench Baseline

As discussed in @sec-problem, **AxBench** [@wu2025axbench] established that representation steering methods consistently underperform prompting on concept-based control. **HyperSteer** [@sun2025hypersteer] recently achieved representation steering performance matching prompting by training hypernetworks on 16k GemmaScope SAE labels. This complements our work: HyperSteer scales to many concepts via supervised training, while InnerPiSSA focuses on unsupervised extraction from minimal data (200 pairs) with emphasis on anti-RLHF robustness—the use case Wu et al. identified as the only compelling justification for representation steering.

### Representation Steering & Engineering
- **Representation Engineering (RepE)** (Zou et al., 2023): The foundational work on top-down transparency and control. We build on their concept of "reading and controlling" high-level cognitive phenomena but move beyond activation arithmetic to gradient-based optimization.
- **repeng** (Vogel, 2024): A robust library for PCA-based steering using contrastive prompt pairs. We adopt their methodology for constructing minimally-contrastive prompts but extend it by (1) using incomplete prefixes rather than complete prompts, (2) applying gradient-based optimization in SVD space, and (3) learning bidirectional rotations.
- **Conditional Activation Steering (CAST)** (Lee et al., 2024): Demonstrates extraction of steering vectors from contrastive prompt-suffix pairs using PCA on mean-centered activations. Our data construction follows similar principles (contrastive pairs, PCA extraction) but targets the planning trajectory in incomplete prefixes rather than behavioral responses in complete outputs.
- **BiPDO** (Cao et al., 2024): Introduced bidirectional preference optimization for steering. Our work shares the bidirectional goal but operates in SVD transformation space rather than raw activation space, and uses a specialized coherence-constrained loss.
- **AxBench** (Wu et al., 2025): A critical benchmark showing that representation steering often lags behind prompting. We directly address this gap, showing that InnerPiSSA is one of the first representation methods to outperform prompting on transfer tasks.
- **Circuit Breakers** (Zou et al., 2024): Uses representation-level optimization to prevent harmful outputs. While they focus on refusal (shutting down capability), we focus on steering (redirecting capability), but share the insight that direct representation control is more robust than output alignment.
- **Anthropic Persona Vectors** (2024): Demonstrates that model personality is encoded in steerable directions.
- **repeng** (Vogel, 2024): A robust library for PCA-based steering. We use this as our primary baseline for arithmetic steering methods.

### Parameter-Efficient Fine-Tuning (PEFT)
- **PiSSA** (Meng et al., 2024): Decomposes weights into principal and residual components ($W = U \Sigma V^T + W_{res}$). We adopt this architecture but innovate by learning *rotations* of the singular vectors rather than just fine-tuning the components, enabling semantic steering.
- **DoRA** (Liu et al., 2024): Decomposes weights into magnitude and direction. Like us, they recognize the importance of separating these components, but they operate in weight space for general fine-tuning, whereas we operate in transformation space for targeted steering.
- **SVFT** (Lingam et al., 2024): Updates singular values ($\Sigma$) for efficient fine-tuning. We extend this by also rotating the singular vectors ($U, V$), which our ablations show is critical for steering (removing rotations drops performance by 96%).
- **SSVD** (Wang et al., 2025): Rotates $V$ matrices for domain adaptation in speech. We independently developed a similar rotation mechanism but apply it to both $U$ and $V$ for bidirectional steering in language models.

### Mechanistic Interpretability
- **Universal Neurons** (Gurnee et al., 2024) & **Stages of Inference** (Lad et al., 2024): Identify that models have distinct "suppression" dynamics in later layers. This motivates our choice to steer at layers $N-2$ to $N-5$, intervening in the "reasoning" stage before suppression mechanisms activate.
- **Negative Results for SAEs** (Smith et al., 2025): Highlights the difficulty of using Sparse Autoencoders for downstream tasks. Our work suggests that supervised/contrastive steering in transformation space may be a more practical route for control than unsupervised dictionary learning.

**Key insight**: Methods operating in transformation space (SVD, rotations) generalize better than those in raw activation or weight space because they align with how transformers process information.

# References {.unnumbered}

::: {#refs}
:::

# Discussion and Limitations {#sec-discussion}

## Limitations

**Prompt-dependent extraction**: Our steering directions are extracted from contrastive prompt pairs. If prompts fail to elicit the target reasoning trajectory (e.g., heavily RLHF'd models that refuse even in prompts), extraction quality degrades. While steering *application* works when prompting fails (@tbl-anti-rlhf), steering *extraction* still depends on getting some signal from the model's responses to prompts. This is a fundamental limitation of unsupervised contrastive methods.

**Resistance from RLHF**: When steering against learned behaviors (coefficient = -1), we observe degradation faster than steering with them (coefficient = +1), visible in coherence costs (0.37 vs 0.31 NLL, @tbl-anti-rlhf). This asymmetry suggests RLHF creates directional structure in representation space that resists reversal. Stronger anti-RLHF steering may require adversarial training or different loss formulations.

**White-box access required**: Like all representation steering methods, InnerPiSSA requires access to model internals (hidden states, gradients). This limits applicability to open-source models, unlike prompting which works with any API. However, for alignment debugging—our target use case—white-box access is inherent to the goal of probing internal representations.

**Data scope and complexity**: We use 200-1000 synthetic honesty pairs with simple templates ("I love/hate X"). While sufficient for proof-of-concept, steering more complex or subtle concepts may require higher-quality data from human annotations or more sophisticated prompt generation. Our data construction follows RepEng conventions but inherits their limitations in capturing nuanced behavioral distinctions.

**Steering site limitations**: We intervene only on residual stream activations via o_proj and down_proj modules. Other intervention sites (attention keys/values, MLP internals, specific token positions) may yield stronger or more targeted effects. Prior work shows different steering sites have different properties; we have not systematically explored this space.

**Model scale**: Primary validation uses Qwen 0.6B-14B models. Larger models (70B+) may exhibit different representation geometry, suppression dynamics, or RLHF resistance. Cross-architecture results (@tbl-cross-model) show sensitivity to model family, suggesting architecture-specific tuning may be necessary.

**Evaluation benchmark mismatch**: We evaluate on DailyDilemmas (moral reasoning) rather than AxBench (concept steering). While DailyDilemmas better tests our alignment debugging goals (genuine preference conflicts, RLHF resistance), direct AxBench comparison would strengthen claims about solving the representation-steering-vs-prompting gap.

**Mechanistic understanding incomplete**: We hypothesize middle layers work best due to suppression dynamics (@sec-suppression), but cannot fully explain why depth 0.5 outperforms our predicted 0.7-0.8. The layer selection findings are empirical, not derived from mechanistic theory. Better understanding of how transformers structure reasoning across layers would enable principled design.

**Compute cost**: Training InnerPiSSA adapters requires gradient computation and backpropagation through the model, unlike PCA which only needs forward passes. For each concept, we need ~10 minutes on a single GPU. While modest compared to full fine-tuning, this is more expensive than arithmetic extraction methods (seconds). However, for alignment debugging use cases, this one-time cost is acceptable.

## Future Work

**Compositional steering**: Can we compose multiple steering directions (e.g., honesty + curiosity) to achieve complex behavioral changes? Does SVD space enable linear superposition of concepts?

**Adaptive coherence budgets**: Currently we use fixed NLL thresholds. Could we dynamically adjust coherence constraints based on task difficulty or steering magnitude?

**Explaining middle-layer effectiveness**: Why does depth 0.5 outperform our predicted 0.7-0.8 range? Mechanistic studies correlating steering effectiveness with SAE features or neuron function could clarify.

**Probing deceptive alignment**: The anti-RLHF capability suggests potential for studying models that conceal misaligned reasoning. Can we use InnerPiSSA to detect or characterize deceptive behaviors in capable models?


# References (Original Position) {.unnumbered}


# Results {#sec-results}

**Main finding**: InnerPiSSA transfers from honesty training to moral reasoning with 5× stronger effect than baselines, while maintaining output coherence (see @tbl-main-results).

| Method            | Coeff   |   Target Effect |   Side Effects |   p-value |   Output Quality |   Normalized Gain (%) |
|:------------------|:--------|----------------:|---------------:|----------:|-----------------:|----------------------:|
|                   |         |       Δ Truth ↑ |      Δ Other ↓ |           |          Δ NLL ↓ |                       |
| InnerPiSSA (ours) | ±1.0    |           0.245 |          0.117 |     0.001 |            0.314 |                18.660 |
| InnerPiSSA (ours) | ±2.0    |           0.321 |          0.162 |     0.089 |            1.403 |                13.346 |
| InnerPiSSA (ours) | ±5.0    |           0.332 |          0.165 |     0.914 |            3.063 |                 8.178 |
| InnerPiSSA (ours) | ±15.0   |           0.302 |          0.144 |     0.000 |            3.429 |                 6.809 |
| random            | ±100.0  |           0.072 |          0.045 |     0.860 |            0.157 |                 6.247 |
| prompting         | ±1.0    |           0.069 |          0.045 |     0.458 |            0.117 |                 6.193 |
| PCA (baseline)    | ±100.0  |           0.053 |          0.039 |     0.869 |            0.263 |                 4.231 |
| PCA (baseline)    | ±1.0    |          -0.001 |          0.002 |     0.995 |            0.000 |                -0.104 |
| random            | ±1.0    |          -0.001 |          0.003 |     0.988 |            0.000 |                -0.126 |

: Main Results: InnerPiSSA vs Baselines on Moral Reasoning Transfer {#tbl-main-results tbl-colwidths="[15,8,12,12,10,14,15]"}

**Experimental setup**: Model = Qwen/Qwen3-0.6B; Training = 1000 honesty contrastive pairs → Evaluation = 64 held-out DailyDilemmas moral scenarios.

**Metrics defined**:
- **Target Effect (Δ Truth)**: Change in log-probability of truthful choices vs baseline (higher = more truthful responses). Measured as expected value over all answer options.
- **Side Effects (Δ Other)**: Mean absolute change across 31 non-target moral values (lower = more targeted steering). 
- **Output Quality (Δ NLL)**: Coherence degradation measured as negative log-likelihood increase on test prompts (lower = better quality).
- **Normalized Gain (%)**: Efficiency metric = 100 × (Δ Truth) / (1 + Δ NLL). Measures steering strength per unit of coherence cost.
- **p-value**: Linear regression testing monotonic dose-response across coefficients [-1, 0, +1] (lower p = stronger evidence of reversible steering).
- **Coefficient (±c)**: Intervention strength; ±1.0 is intended operating range, higher values test extrapolation.

**Methods**: InnerPiSSA = learnable SVD rotations + scaling (ours); PCA = unsupervised direction via activation subtraction (baseline); prompting = "Be honest" text prefix; random = noise vector control.

The key test for alignment debugging is not cooperative performance (where prompting works well) but adversarial robustness—can we controllably steer against RLHF training to observe internal states when output-level methods fail?

@tbl-anti-rlhf shows results when steering **against** learned behaviors (coefficient = -1, dishonest direction). Prompting catastrophically collapses to incoherent outputs (-10.84 truthfulness score), while InnerPiSSA maintains controlled steering (-0.70 truthfulness). This 15× difference demonstrates that InnerPiSSA modifies internal reasoning trajectories rather than just output style.

| Method            | Coeff   |   Truthfulness Score |   Coherence |
|:------------------|:--------|---------------------:|------------:|
|                   |         |           (nats) ↑   |    (NLL) ↓  |
| InnerPiSSA (ours) |   -1.0  |                 2.18 |        0.37 |
| InnerPiSSA (ours) |    0.0  |                 3.02 |        0.00 |
| InnerPiSSA (ours) |   +1.0  |                 5.34 |        0.31 |
| prompting         |   -1.0  |               -10.84 |        1.27 |
| prompting         |    0.0  |                 1.80 |        0.00 |
| prompting         |   +1.0  |                 2.22 |        0.12 |
| repeng (PCA)      |   -1.0  |                 2.32 |       -0.01 |
| repeng (PCA)      |    0.0  |                 1.80 |        0.00 |
| repeng (PCA)      |   +1.0  |                 1.28 |        0.05 |

: Anti-RLHF Stress Test: Steering Against Learned Behaviors {#tbl-anti-rlhf}

**Interpretation**: When steering against RLHF (coeff = -1):

- **Prompting collapses** from 1.80 baseline to -10.84 truthfulness (catastrophic)—the output-level intervention breaks generation coherence
- **InnerPiSSA maintains control** from 3.02 baseline to 2.18 truthfulness (controlled steering)—the hidden-state intervention preserves coherence while reversing behavior
- **PCA baseline** shows weak effects in both directions (2.32 vs 1.28), demonstrating arithmetic methods cannot achieve this level of control

This is the core evidence for alignment debugging capability: InnerPiSSA works precisely when output-level methods fail, enabling researchers to probe what models compute internally when safety constraints are bypassed at the representation level. The bidirectional nature (same adapter steers ±1 with opposite effects) demonstrates control over internal dimensions rather than arbitrary direction finding.

### Connection to Alignment Failure Modes {#sec-failure-modes}

The anti-RLHF robustness result has direct implications for studying alignment failures. If we can controllably steer against RLHF-learned behaviors, we can also probe unintended side effects of safety training:

- **Reward hacking**: Models that optimize proxy metrics while violating intent would show this pattern when steered to maximize the proxy
- **Specification gaming**: Models that satisfy literal requirements via loopholes would reveal the gaming strategy when steered toward "technically compliant" reasoning  
- **Deceptive alignment**: Models that conceal misaligned reasoning would expose it when steered away from safety-trained outputs

These failure modes are difficult to study with current methods because they require observing model behavior *against* its training incentives. Prompting fails here (@tbl-anti-rlhf shows catastrophic collapse), while supervised probing requires human labels of model internals (introducing distribution shift). InnerPiSSA's unsupervised, gradient-based approach enables this probing: we extract directions from the model's own reasoning trajectories, then amplify them to see what happens when safety constraints are relaxed at the representation level.

The asymmetry in @tbl-anti-rlhf is also revealing: steering *against* RLHF (coeff=-1) is harder than steering *with* it (coeff=+1), visible in both the coherence degradation (0.37 vs 0.31 NLL) and the smaller effect magnitude. This suggests RLHF creates directional structure in representation space—learned behaviors are "easier" to amplify than reverse. Understanding this asymmetry could inform more robust alignment methods that account for the geometry of learned preferences.

@fig-prompting-collapse visualizes this breakdown across the full coefficient range:

![TODO: Line plot showing truthfulness score vs coefficient for InnerPiSSA, prompting, and PCA. X-axis: coefficient from -5 to +5. Y-axis: truthfulness score (nats). InnerPiSSA should show smooth monotonic increase from ~2.0 (-1) to ~5.5 (+1). Prompting should show collapse at negative coefficients (drop to -10.84 at -1) but recovery at positive (up to 2.22 at +1). PCA should be flat/noisy around baseline. Annotate the "collapse zone" for prompting and "controlled steering" for InnerPiSSA. This visualizes why prompting is not suitable for alignment debugging—it only works when steering *with* RLHF, not against it.](docs/img/prompting_collapse_placeholder.png){#fig-prompting-collapse}

**Honesty Transfer to Morality (Daily Dilemmas (1000 train → 64 test).** Model: Qwen/Qwen3-0.6B. Target Effect: Δ Truthfulness log-probability score vs baseline (score = expected value of truthful choices; higher = more truthful). Side Effects: mean |Δ| across 31 non-target moral values. Output Quality: coherence degradation (ΔNLL). Normalized Gain (%) = 100 × Δ Truth / (1 + Δ NLL); measures steering efficiency. Coefficient (±c) scales intervention strength; ±1.0 is the intended operating range. p-values from linear regression on log-probability scores testing monotonic dose-response (lower p = stronger evidence of reversible steering).
Methods: InnerPiSSA (ours) = learnable SVD rotations + scaling; PCA (baseline) = unsupervised PCA direction; prompting = 'Be honest' prefix; random = noise vector baseline.

## Transfer Pattern Analysis: Cluster Effects {#sec-cluster-effects}

@fig-radar-clusters shows how different steering methods affect moral value clusters. InnerPiSSA increases Agent virtues (autonomy, ambition, courage) while maintaining Assistant virtues (care, empathy, patience), demonstrating coherent transfer that respects psychological structure. Prompting shows weaker, less coherent patterns. Critically, neither method significantly affects orthogonal Preferences (favorite color) or degrades Math performance, confirming targeted steering rather than general capability shifts.

![TODO: Radar plot showing t-statistics for Agent/Assistant/Conscientiousness/Preferences/Math clusters. InnerPiSSA should show: Agent +5.8, Assistant +2.1, Conscientiousness +7.2, Preferences ~0, Math +1.2. Prompting: Agent -0.4, Assistant +1.3, Preferences ~0, Math ~0. This demonstrates (1) coherent coupling of related values, (2) preservation of orthogonal dimensions, (3) minimal side effects on capabilities.](docs/img/radar_clusters_placeholder.png){#fig-radar-clusters}

**Interpretation**: The Agent/Assistant coupling reveals deep vs surface steering. Truth-seekers naturally exhibit intellectual ambition (Agent cluster), not risk-averse compliance (Assistant cluster). InnerPiSSA captures this psychological structure; prompting produces surface markers without internal coherence. The near-zero effects on Preferences and Math confirm we're steering reasoning style, not corrupting general capabilities or introducing arbitrary biases.

## Coherence-Intervention Tradeoff {#sec-pareto}

@fig-pareto shows the Pareto frontier between intervention strength and output quality across coefficient ranges. Three key findings:

1. **Operating range bounds**: InnerPiSSA maintains coherence (ΔNLL < 0.5) for coefficients ±1 to ±2, breaking down beyond ±5
2. **Asymmetric difficulty**: Anti-RLHF direction (negative coefficients) degrades faster, consistent with steering against learned behaviors being harder
3. **Method comparison**: InnerPiSSA achieves strongest effects at training coefficients (±1) but doesn't extrapolate as well as prompting to extreme values—this is expected since we optimize for the intended operating range, not unbounded extrapolation

![TODO: Pareto plot with x=coherence (ΔNLL), y=intervention strength (target effect). Show curves for InnerPiSSA at coefficients [-5, -2, -1, 0, 1, 2, 5], plus prompting and PCA baselines. InnerPiSSA should form a smooth frontier from (-1, 0.31 NLL, 0.245 effect) to (5, 3.06 NLL, 0.332 effect), with breakdown visible beyond ±5. Mark the "trained range" (±1) vs "extrapolation" (±5). Include annotation showing anti-RLHF direction degrades faster.](docs/img/pareto_coherence_placeholder.png){#fig-pareto}

![](docs/tables/effect_vs_coherence.png)

## Appendix: Experiments and Rationales

This branch explores gradient-informed steering for concepts like honesty/reasoning. Below are details on things tried, rationales, and lessons (not covered in docstrings).

## Ideas That Failed: Why Gradient-Based SVD Steering is Necessary {#sec-failed-ideas}

The final InnerPiSSA method emerged after systematically testing approaches from representation engineering, preference optimization, and parameter-efficient fine-tuning literatures. This section documents what didn't work and why, justifying each component of the final design.

### 1. Arithmetic Methods: PCA and Mean Differencing

**What we tried**: Following RepEng [@zou2023representation] and the repeng library [@vogel2024repeng], we extracted steering directions as $\text{mean}(hs_{cho} - hs_{rej})$ from contrastive activation pairs. We also tested PCA on centered activations to capture maximal variance [@lee2024programmingrefusalconditionalactivation].

**Result**: Near-zero effect sizes (PCA: 0.053 vs InnerPiSSA: 0.245 at coeff=±1.0, see @tbl-main-results). Steering via arithmetic addition produced effects indistinguishable from random noise (p=0.87).

**Why it failed**: Averaging activations captures surface statistics (word choice, syntactic patterns) rather than semantic transformations. No learning mechanism to discover task-relevant subspaces. Raw activation space mixes positional encodings, layer normalization artifacts, and content—subtracting or averaging these produces noisy directions that don't generalize.

**Evidence**: @tbl-arch-ablation shows operating in raw space (no SVD) drops performance by 75%. The arithmetic approach is fundamentally limited by what variance happens to appear in the training prompts, not what the model can learn to separate.

### 2. Gradient Methods Without SVD: DPO/SimPO on Hidden States  

**What we tried**: Apply Direct Preference Optimization [@rafailov2023direct] or length-normalized variants (SimPO) directly to hidden states instead of output logits. The hypothesis was that preference optimization should work for any model component, not just outputs.

**Result**: Training collapsed or produced incoherent outputs. Early experiments achieved 90% train accuracy but <25% on out-of-distribution moral reasoning (see `docs/readme_for_previous_failed_work.md`).

**Why it failed**: DPO optimizes output distributions with implicit regularization from the language modeling objective. Applying the same loss to hidden states lacks this structure—models can satisfy the loss by making arbitrary changes to intermediate representations that break downstream layers. Without coherence constraints specifically designed for hidden states (our ReprPO formulation), the optimization has no incentive to preserve generation quality.

### 3. Fixed SVD Extraction: Projection Without Learning

**What we tried**: Project activations to S-space using pre-trained SVD ($W = U \Sigma V^T$), extract direction via PCA or mean differencing in this space, apply with $\sqrt{\Sigma}$ weighting to match PiSSA initialization [@meng2024pissa].

**Result**: Better than raw PCA but still dramatically underperforms learned rotations. Removing learnable rotations (keep only $\Sigma$ scaling) drops performance by 89% (see @tbl-arch-ablation row "No V rotation": metric 29 vs full InnerPiSSA: 666).

**Why it failed**: The pre-trained SVD basis captures variance in the model's original task distribution (e.g., language modeling), not behavioral preferences like honesty. Fixed extraction cannot adapt to steering objectives. The key insight is that we need to **rotate** within the SVD basis to find the subspace aligned with the concept we're steering—just using the pre-trained $U$ and $V$ is insufficient.

**Evidence**: SSVD [@wang2025ssvd] independently discovered the same insight for domain adaptation in speech: rotating singular vectors enables transfer that fixed extraction cannot achieve.

### 4. Weight-Space Gradients: Projected Gradient Descent

**What we tried**: Compute PCA direction from activations, then project weight gradients onto it during DPO training (clip orthogonal components before updating weights).

**Result**: 89.4% test accuracy vs 90.0% for standard DPO—no improvement despite increased complexity.

**Why it failed**: Gradient projection operates in weight space, but steering needs activation-space control. Discarding orthogonal gradient components removes information needed for coherent generation. This approach conflates the *parameter update* direction with the *activation steering* direction, which live in different spaces and have different objectives.

### 5. Scaling-Only Interventions: $\Sigma$ Without Rotations

**What we tried**: Update only singular value magnitudes ($\Sigma$) via multiplicative scaling, keeping $U$ and $V$ fixed. This tests whether rotation is necessary or if amplitude modulation suffices.

**Result**: Some experiments showed improvement over full method (1051 vs 666 metric), but this is likely an artifact—we didn't control for effective rank or learning rate differences. More systematic ablations (row "No V rotation" in @tbl-arch-ablation) show rotations are critical.

**Why this matters**: It suggests the rotation component ($V @ R_v$) is more important than the scaling component ($\exp(\alpha \lambda)$) for discovering steering directions. Scaling alone can't find the right subspace; rotation can. Future work should ablate these more carefully with matched hyperparameters.

### Key Takeaway: All Three Components Are Necessary

The successful approach combines elements that failed methods lacked:

1. **SVD transformation space** (not raw activations): Aligns intervention with model's learned transformations  
2. **Learnable rotations** (not fixed extraction): Discovers task-relevant subspaces via gradient descent
3. **Coherence-constrained loss** (not unconstrained preference optimization): ReprPO with per-token NLL bounds prevents training collapse while maximizing separation

Removing any component causes >85% performance drop (@tbl-arch-ablation). The anti-RLHF stress test (@tbl-anti-rlhf) further validates that gradient-based learning enables robustness unattainable by fixed extractions: prompting collapses (-10.84) while InnerPiSSA maintains control (-0.70) at coefficient -1.

### Metrics Reference

**Main metric**: `T-statistic / (1 + NLL_degradation)`
- **Effect (T-statistic)**: Measures monotonicity of steering across coefficients ∈ [-1, 0, 1]. Higher = stronger dose-response relationship (steering works bidirectionally).
- **Degradation (NLL)**: Coherence loss measured as Δ NLL (negative log-likelihood increase) on daily dilemmas questions. Penalizes interventions that break generation quality.
- **Baselines (effect sizes, not gain %)**: Prompting=2.23, RepEng=3.06, S-steer=3.02
- Main Metric / Gain % - this is Effect / degregaton. This is our key metric and measures intervention strength against degredation.

**Projection loss (`val_proj_diff`)**: Mean absolute difference in activations projected onto PCA direction, averaged across chosen/rejected pairs. Measures separation magnitude along the steering axis.

**Coherence constraints**: 
- `coh_weight`: KL divergence penalty to keep policy close to reference model
- `mono_weight`: Per-token margin loss ensuring logp_pi > logp_ref for chosen tokens

Notes:
- inverted steering: this is fine, it happens in PCA and we just swap the coeffecients. As long as the coeffecients give opposites things to both sizes of zero then we get a strong T-state effect size. If we froced the model to learn a certain direction we might be forcing it to learn opposite behavious which is a much harder task than going with it's preexisting inclinations then reversing the coeffecients.

### Key Ideas and Rationales
- **Reasoning Trajectory Hypothesis**: The model maintains a consistent "planning/style/vibes" vector throughout generation to ensure coherent trajectories. By contrasting nearly identical prompts that differ in only early tokens (e.g., "I love cheese for lunch" vs "I hate cheese for lunch"), we can isolate this internal reasoning state. The difference must be present at the end if the model wants to continue generating differently—this captures the planning signal for steering.
- **Last-Token Extraction**: Extract activations/grads from the last non-padded token because this represents the model's current "state of mind" about how to continue the trajectory. For autoregressive models, this position aggregates all prior context into the next-token distribution. Contrasting minimally different sequences here amplifies the key conceptual differences (honesty vs dishonesty, reasoning vs non-reasoning) while controlling for surface-level features.
- **Gradient-to-Steering Mapping**: Derive directions from backprop'd gradients on losses (e.g., ReprPO on hidden states). Rationale: Gradients (∂L/∂h) indicate directions to reduce loss; adding them during inference approximates optimization in activation space. Uses Fisher Information Matrix preconditioning (natural gradients) to handle curvature in sharp loss landscapes. Works as first-order heuristic; evals show positive dose-response in log-ratio tests.
- **Layer-Specific Steering**: Test specific sublayers (e.g., k_proj, o_proj, down_proj) rather than whole residual streams. Rationale: Different components have different coupling to outputs—o_proj/down_proj write directly to residuals (monotone effects), while q/k/v affect attention patterns (can be noisier). Enables more targeted interventions. Evals: k_proj scores ~1.42, v_proj ~0.59, hidden states ~15.93 (from research journal).

### Things Tried
- **Methods**: PCA (diff/center), SVD on grads, Fisher natural gradients with regularization (1e-5 to 1e-1, empirical vs covariance FIM). Best performer: `fisher_steer_cov_reg1` (scores up to 15.93). Dual pos/neg variants for balanced steering directions.
- **Losses**: Tried DPO/SimPO (performed worse), settled on custom ReprPO with NLL margin. Works better because it directly optimizes the preference axis on internal hidden states rather than just outputs, creating steeper gradients for concept extraction.
- **Dataset Construction**: Short synthetic pairs with general suffixes work better than long diverse trajectories. Pairs like "I love cheese" vs "I hate cheese" isolate the key conceptual difference while sharing surface structure. Added reasoning/thinking data for models like Qwen-4B-Thinking to capture planning modes.
- **Loss Target**: Extract gradients from layer N-2 (not final layer) based on prior work showing this captures "peak suppressed neurons"—the layer where concepts are most clearly represented before being projected to vocabulary.
- **Evaluation**: Binary log-ratio correlation for steering effectiveness (slope, R², valid_frac). Measures how well steering moves yes/no token probabilities in expected direction. High coefficients sometimes cause saturation/incoherence.
- **Models**: Tested on Qwen-4B/8B/14B (4-bit quantized), GLM-9B-Thinking. Larger models show better extrapolation and more stable steering.

### Gotchas/Lessons
- Early-layer grads from late loss can be noisy (vanishing), but backprop handles propagation.
- Overfitting risk: Synthetic data captures wording; OOD evals needed.
- Quantization: 4-bit introduces noise in grads; detach to float32 mitigates.
- Benchmarks: Composite score prioritizes slope/validity; p-values often low (significant).

For full details, see notebooks (e.g., performance_tests_reprpo_layers.ipynb) and research_journal_mjc.md.

### Custom ReprPO Loss Details
The loss in `losses.py` (e.g., `compute_reprpo_nll_margin_loss`) is designed for one-step gradient/curvature sampling on paired pos/neg examples, not full training. It combines:
- **Separation Term**: Maximizes the L2 norm of (positive - negative) hidden state differences to isolate the target concept.
- **Coherence Margin**: Defines a bounded region where the NLL of the preferred (positive) completion is no worse than a baseline (detached average logprob of positive labels). Deviations outside this region are penalized quadratically. A 0.99 scaling on the baseline positions the computation just inside the boundary, ensuring both terms contribute to gradients.

This creates steeper, more informative gradients for steering, inspired by SimPO/DPO margins but focused on internal state coherence rather than direct pos/neg comparison.

For geometric intuition and detailed explanation, see `docs/loss_geometry.md`.

![Loss Geometry](docs/loss.svg)

See also the repo for training with losses like this https://github.com/wassname/repr-preference-optimization

---

## Appendix: Ablation Studies and Paper Tables

**Auto-generated from wandb results**. Run `uv run python nbs/generate_paper_tables.py` to regenerate.

### Table 1: Cross-Model Generalization

**FIXME**: Run `just run-models` to populate this table.

<!--#include file="docs/tables/table1_cross_model.md"-->
| Model         | Size   | InnerPiSSA   | Prompting   | RepEng      | S-Steer   |
|:--------------|:-------|:-------------|:------------|:------------|:----------|
| Qwen3-0.6B    | 0.6B   | TODO         | **216.1** ✓ | 77.1        | -         |
| Qwen3-4B      | 4B     | **2114.0** ✓ | 759.3       | 284.1       | -         |
| Qwen3-14B     | 14B    | TODO         | **106.9** ✓ | 1.8         | -         |
| Llama-3.1-8B  | 8B     | TODO         | 178.6       | **704.2** ✓ | -         |
| Gemma-3-4B    | 4B     | TODO         | **394.8** ✓ | 1.9         | -         |
| Gemma-3-12B   | 12B    | TODO         | **490.7** ✓ | 1.4         | -         |
| Qwen-14B-code | 14B    | TODO         | 44.1        | **171.5** ✓ | -         |

**Notes**: Main metric = T-statistic / (1 + NLL degradation). Higher is better. Only Qwen3-4B has InnerPiSSA results so far.

---

### Table 2: Layer Depth Ablation

**Status**: ⚠️ Partial - only 1 run per depth except 0.5 (needs multiple seeds for robustness)

<!--#include file="docs/tables/table2_layer_ablation.md"-->
|   Depth |   Layer |   Mean |    Max |   N Runs |   Val Loss | Finding      |
|--------:|--------:|-------:|-------:|---------:|-----------:|:-------------|
|    0.01 |       0 |  396.3 |  396.3 |        1 |        6.6 | Early - weak |
|    0.1  |       3 |   32.6 |   32.6 |        1 |       12.4 | Early - weak |
|    0.2  |       7 |  209.1 |  209.1 |        1 |        1.1 | Mid          |
|    0.3  |      10 |  737.3 |  737.3 |        1 |        2   | **Strong** ✓ |
|    0.4  |      14 | 1303   | 1303   |        1 |        4.4 | **Strong** ✓ |
|    0.5  |      18 |  704.4 | 2114   |       40 |        9.7 | **Strong** ✓ |
|    0.6  |      21 |  439.7 |  439.7 |        1 |        8.9 | Mid          |
|    0.7  |      25 |  911.3 |  911.3 |        1 |        7.6 | Mid          |
|    0.8  |      28 |  665.9 |  665.9 |        1 |       20.2 | Mid          |
|    0.9  |      32 |  279.7 |  279.7 |        1 |        1.5 | Late - weak  |
|    0.99 |      35 |  547.3 |  547.3 |        1 |       16.4 | Late - weak  |

**Finding**: Middle layers (0.3-0.5, layers 10-18) work best. Early layers lack semantic content, late layers already suppressed. Matches mechanistic interpretability findings [@gurnee2024universal; @lad2024remarkable] showing suppression dynamics concentrate in later layers.

### Why Middle Layers? The Suppression Hypothesis {#sec-suppression}

Our finding that middle layers (depth 0.3-0.5) are optimal for steering aligns with mechanistic interpretability research on model internals. Gurnee et al. [-@gurnee2024universal] identified "universal neurons" across models, including suppression neurons that decrease probability of specific token classes. Critically, they found "a sudden shift towards a much larger number of suppression neurons" in final layers. Lad et al. [-@lad2024remarkable] propose a "stages of inference" hypothesis with early layers for feature extraction, middle layers for reasoning, and a final "residual sharpening" phase dominated by suppression dynamics.

We hypothesized that steering should target layer N-2 to N-3 (late middle layers) where planning trajectories are computed but output suppression has not yet activated. Our empirical results show peak performance at depth 0.5 (layer 18/36), which is slightly earlier than predicted but consistent with the suppression framework. We cannot fully explain why 0.5 outperforms 0.7-0.8 (our original hypothesis), but the general principle holds: **steering works best in the reasoning phase, before output shaping mechanisms dominate**.

This has implications for alignment debugging: if suppression happens in late layers, intervening earlier lets us observe what models compute *before* safety training filters outputs. The breakdown at depth >0.8 (performance drops from 911 to 280) likely reflects attempting to steer representations that have already been shaped for output compliance, where the semantic signal is weaker.

## Learning Rate Sensitivity {#sec-ablation-lr}

@tbl-lr-ablation shows the impact of learning rate on training:

<!--#include file="docs/tables/table3_learning_rate.md"-->
|     LR |   Mean | Std   |    Max |   N Runs |   Val Loss | Result                |
|-------:|-------:|:------|-------:|---------:|-----------:|:----------------------|
| 1e-05  |   67   | -     |   67   |        1 |       27   | Too low - fails ❌    |
| 0.0001 |   19.2 | -     |   19.2 |        1 |       17.2 | Too low - fails ❌    |
| 0.001  |  167.8 | -     |  167.8 |        1 |        9.3 | Low - weak            |
| 0.008  |  691.6 | 511.4 | 2114   |       40 |        7.3 | **Default - stable** ✓|
| 0.01   |  995.9 | 385.2 | 1432   |        3 |        6.8 | **High perf** ⚠️      |
| 0.1    |  714.6 | 829.1 | 1648   |        3 |       17.4 | Too high - unstable   |
| 1      |  650.8 | -     |  650.8 |        1 |       52.4 | Too high - unstable   |

: Learning Rate Sensitivity {#tbl-lr-ablation}

**Finding**: lr=0.008-0.01 is the sweet spot. Higher variance at 0.01 suggests sensitivity.

---

## Architecture Component Ablation {#sec-ablation-arch}

@tbl-arch-ablation tests the necessity of each architectural component:

<!--#include file="docs/tables/table4_architecture.md"-->
| Configuration   |   Main Metric |   Val Loss |   N Runs | Result                |
|:----------------|--------------:|-----------:|---------:|:----------------------|
| Full InnerPiSSA |         666.3 |        9.4 |       49 | **Baseline** ✓        |
| No S scaling    |        1051   |       10.6 |        1 | Better? (investigate) |
| No V rotation   |          29   |       34.7 |        1 | **Catastrophic** ❌   |
| LoRA adapter    |           0   |        9.3 |        3 | **Catastrophic** ❌   |

: Architecture Component Ablation {#tbl-arch-ablation}

**Findings from @tbl-arch-ablation**: 

- V rotation is **critical** - removing it → 96% performance drop (see @sec-rotations for why)
- LoRA adapter completely fails (3 runs, all metric=0)
- S scaling may not be necessary - actually improves without it (needs confirmation)

---

### Table 5: Rank Sensitivity

**FIXME**: Run `just sweep-rank` to populate this table.

|   Rank | Main Metric   | Val Loss   | N Runs   |
|-------:|:--------------|:-----------|:---------|
|     32 | TODO          | TODO       | TODO     |
|     64 | TODO          | TODO       | TODO     |
|    128 | TODO          | TODO       | TODO     |
|    256 | TODO          | TODO       | TODO     |
|    512 | TODO          | TODO       | TODO     |

## Module Targeting Ablation {#sec-ablation-modules}

**FIXME**: Run `just ablate-modules` to populate @tbl-module-ablation.

| Modules                               | Main Metric   | Val Loss   | Finding   |
|:--------------------------------------|:--------------|:-----------|:----------|
| o_proj, down_proj (residual)          | TODO          | TODO       | TODO      |
| gate_proj, up_proj (MLP)              | TODO          | TODO       | TODO      |
| q_proj, k_proj, v_proj, o_proj (attn) | TODO          | TODO       | TODO      |
| All modules (default)                 | TODO          | TODO       | TODO      |

: Module Targeting Ablation {#tbl-module-ablation}

---

## Data Efficiency {#sec-ablation-data}

**FIXME**: Run `just data-efficiency` to populate @tbl-data-efficiency.

|   Samples | Main Metric   | Val Loss   | Finding   |
|----------:|:--------------|:-----------|:----------|
|        50 | TODO          | TODO       | TODO      |
|       100 | TODO          | TODO       | TODO      |
|       800 | TODO          | TODO       | TODO      |
|       400 | TODO          | TODO       | TODO      |
|       800 | TODO          | TODO       | TODO      |
|      2000 | TODO          | TODO       | TODO      |

: Data Efficiency Analysis {#tbl-data-efficiency}

---

## Random Seed Stability {#sec-ablation-seeds}

**FIXME**: Run `just run-seeds` to populate @tbl-seed-stability.

| Seed       | Main Metric   | Val Loss   |
|:-----------|:--------------|:-----------|
| 42         | TODO          | TODO       |
| 123        | TODO          | TODO       |
| 456        | TODO          | TODO       |
| Mean ± Std | TODO          | TODO       |

: Random Seed Stability {#tbl-seed-stability}

# Acknowledgments {.appendix #sec-acknowledgments}

We thank the community for discussions and feedback. This work builds on insights from the representation engineering, parameter-efficient fine-tuning, and mechanistic interpretability communities.

# Code and Data Availability {.appendix}

Code is available at: `https://github.com/wassname/InnerPiSSA_private`

# Citation {.appendix}

If this work is useful for your research, please cite:

```bibtex
@misc{clark2025innerpissa,
  title = {InnerPiSSA: Deep-Dish Inner Alignment through Reversible SVD Steering},
  author = {Clark, Michael J},
  year = {2024},
  url = {https://github.com/wassname/InnerPiSSA/}
}
```

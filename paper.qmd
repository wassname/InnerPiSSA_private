---
title: "InnerPiSSA: Deep-Dish Inner Alignment through Reversible SVD Steering"
author:
  - name: Michael J. Clark
    id: mjc
    email: michaelj.clark@wassname.org
abstract: |
  RLHF trains models to suppress unaligned reasoning in output layers, leaving internal representations unchanged. We propose InnerPiSSA, a method to probe internal reasoning by steering hidden states in the model's native SVD transformation space. Trained on 1000 synthetic minimally-contrastive prompt prefixes (no completions or labels), we extract activations from incomplete prompts differing by one word ("I love cheese" vs "I hate cheese") to isolate the model's planning state before output suppression. Using gradient-based optimization on learnable SVD rotations, we discover directions separating honest from dishonest trajectories. On Qwen3-4B, InnerPiSSA achieves 2.8× prompting baseline (2114 vs 759). Layer ablations show effects peak at middle layers (0.3-0.5 depth), where suppression dynamics concentrate. Our method provides a tool for alignment debugging: probing deceptive alignment and reward hacking by accessing internal reasoning that RLHF suppresses.
date: 2025-11-22
bibliography: references.bib
number-sections: true
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
  pdf:
    documentclass: article
    papersize: letter
    geometry:
      - top=1in
      - bottom=1in
      - left=1in
      - right=1in
---



# Introduction {#sec-intro}

Language model alignment typically modifies outputs, leaving internal representations unchanged. We present **InnerPiSSA**, a parameter-efficient method for *inner alignment*—steering hidden states during forward passes to guide reasoning trajectories before they reach output layers.

Our approach builds on PiSSA [@meng2024pissa], which decomposes weight matrices via SVD for parameter-efficient fine-tuning, and representation engineering [@zou2023representation], which operates on population-level representations for transparency. Unlike prior steering methods that use activation arithmetic [@vogel2024repeng] or output-level optimization [@cao2024personalized], we optimize hidden state geometry through gradient-based Representation Preference Optimization (ReprPO).

Key properties:

- **Efficient**: strong steering with <1% of parameters vs full fine-tuning
- **Reversible**: single adapter steers both directions (honest to dishonest) via coefficient sign
- **Generalizable**: transfers from honesty training to 31 moral value dimensions
- **Coherent**: maintains generation quality with bounded NLL degradation

We train on *hidden state differences* from minimally-contrastive prompts [@lee2024programmingrefusalconditionalactivation], capturing the model's internal "planning trajectory" before output suppression mechanisms activate.


## Model Architecture {#sec-architecture}

A steering method for inner alignment should modify hidden state trajectories while maintaining output quality and enabling bidirectional control. Standard methods operating in raw activation space fail due to high noise from positional and structural features. The main guiding principles for our architecture are:

1. **Operate in the model's native transformation basis**: Raw activation space mixes semantic content with positional/structural information, making it noisy for steering
2. **Enable bidirectional control**: A single adapter should steer both toward and away from a behavior
3. **Maintain output coherence**: Steering shouldn't degrade generation quality

### SVD-based Projection {#sec-svd-projection}

Following PiSSA [@meng2024pissa], we decompose each layer's weight matrix $W = U \Sigma V^T + W_{res}$. This separates the model's transformation components $(U, \Sigma, V)$ from residual variance. Projecting activations into the S-space ($hs @ U$) aligns our intervention with how the model transforms information, not just what it represents. This is analogous to operating in frequency space rather than pixel space for image editing—semantic transformations are cleaner in the model's native basis. Operating directly in raw activation space mixes these transformations with positional encodings and layer normalization artifacts, reducing steering effectiveness by 95% (see @sec-ablations).

### Learnable Rotations {#sec-rotations}

The pre-trained SVD basis is not perfectly aligned with honesty directions. Inspired by SSVD [@wang2025ssvd], we learn skew-symmetric parameters $\theta_v$ that generate rotation matrices $R = \text{cayley}(\theta_v, c)$, allowing the adapter to discover the optimal subspace for separating honest/dishonest trajectories. Without this, performance drops by 89% (280 → 20 normalized gain). We parameterize rotations using the Cayley transform on skew-symmetric matrices, which guarantees orthogonality and enables efficient gradient-based learning.

### Singular Value Scaling {#sec-scaling}

We scale $\Sigma$ by $\exp(c \cdot \lambda)$ rather than additive scaling $(\Sigma + c\cdot\lambda)$. This respects the multiplicative nature of singular values as amplification factors in the SVD decomposition. Multiplicative scaling creates cleaner dose-response curves and maintains numerical stability; additive scaling causes training instability and approximately 60% performance degradation.

### Coherence Constraint {#sec-coherence}

Maximizing hidden state separation alone causes the model to generate incoherent outputs at high steering coefficients. We bound the per-token NLL degradation to <0.2 nats relative to the reference model, creating a trust region where steering improves target behavior without breaking generation quality. This builds on principles from circuit breakers [@zou2024improving] but extends to steering rather than just refusal.


## Install

```sh
# install
uv sync --all-groups

# help
uv run python nbs/train.py --help

# Quick test run
uv run python nbs/train.py --quick

# Full training with W&B
uv run python nbs/train.py --batch_size 14 --n_epochs 30 --use_wandb

# Custom config
uv run python nbs/train.py \
  --rank 128 \
  --lr 5e-4 \
  --target_modules ".*\.(10|20|30)\..*(gate_proj|down_proj)"
```

## Data Construction: Minimally-Contrastive Prompt Prefixes {#sec-data}

### The Planning Trajectory Hypothesis {#sec-planning-hypothesis}

Unlike standard preference optimization methods that require full prompt-completion pairs, or traditional activation steering methods [@vogel2024repeng] that use complete prompts, we extract steering directions from **incomplete, minimally-contrastive prompt prefixes**. This design is motivated by a key mechanistic insight:

**Hypothesis**: Autoregressive models maintain an internal "planning trajectory" vector throughout generation to ensure coherent continuations. When two prompts differ by a single word early on (e.g., "I love cheese" vs "I hate cheese") but must lead to different continuations, the model's hidden state at the last token must already encode the divergent plan.

By using incomplete prefixes, we:

1. **Isolate planning information**: The model hasn't generated the divergent outputs yet, so differences in activations reflect *intended* trajectories, not realized outputs
2. **Minimize surface variance**: Sequences share maximum context, differing only in the critical concept word
3. **Capture semantic transformations**: Contrasting at the last token position captures the model's "state of mind" about how to continue, before output layer suppression mechanisms activate

### Data Format {#sec-data-format}

Following the contrastive prompt construction in Conditional Activation Steering [@lee2024programmingrefusalconditionalactivation] and RepEng [@vogel2024repeng], we create **synthetic prompt prefix pairs**:

```python
# Example: Honesty concept (no completions needed)
chosen  = "I love cheese; let me tell you about the andes mountains"
rejected = "I hate cheese; let me tell you about the andes mountains"

# Key properties:
# 1. Incomplete (no answer/completion)
# 2. Differ by ONE word early in sequence  
# 3. Share maximal suffix context
# 4. Extract activations from LAST token only
```

We construct ~1000 such pairs using:

- 2 prompt templates ("I love/hate X", "I always tell/hide the truth about X") 
- 500 random context suffixes (from general text corpus)
- No human preference labels
- No model completions

This **self-supervised contrastive** approach differs from:
- **DPO/BiPO**: Require full completions and human preference labels
- **Standard CAA/RepEng**: Use complete prompts or prompt-suffix pairs
- **InnerPiSSA**: Extracts from incomplete prefixes to capture planning state

See and for the mathematical formulation of contrastive activation extraction.

## Method Overview

### Pseudocode for Contrastive SVD Adapter Steering

### Algorithm 1: InnerPiSSA (Simplified)

```python
# 1. SETUP: Reversible SVD Adapter
U, Σ, V = SVD(W)[:r]
θ_v, θ_u, log_λ = init_params(r)

def forward(x, α):
    # Rotate singular vectors by α (steering strength)
    R_v, R_u = cayley(θ_v, α), cayley(θ_u, α)
    return x @ (V @ R_v) @ diag(Σ * exp(α * log_λ)) @ (U @ R_u).T

# 2. TRAINING: Contrastive Representation Preference Optimization
# pref_dir defined by reference model separation in S-space
for batch in data:
    # Forward pass for honest (+1) and dishonest (-1) directions
    out_pos, out_neg = model(batch, α=+1), model(batch, α=-1)
    
    # Loss A: Maximize separation (flip if anti-aligned)
    L_proj = -((out_pos.h - out_neg.h) @ pref_dir).mean()
    if L_proj > 0: L_proj = -L_proj

    # Loss B: Coherence (adaptive: relax if separation is weak)
    w = softmax(-L_proj)  # Harder task → Lower weight
    L_coh = w * relu(out_pos.nll - ref.nll - τ)**2

    # Loss C: Monotonicity (gap_-1 < gap_0 < gap_+1)
    L_mono = hinge(out_neg.gap < 0) + hinge(out_pos.gap > 0)

    (L_proj + L_coh + L_mono).backward()
```

### Detailed Implementation Logic

<details>
<summary>Click to expand full pseudocode</summary>

```python
# DATA: Contrastive prompt pairs differing by one word.
honest    = ["I love cheese...", ...]
dishonest = ["I hate cheese...", ...]
batch = [honest[0], dishonest[0], ...]  # interleaved

# SETUP: Low-rank SVD with learnable rotations
U, Σ, V = SVD(layer.W)[:r]
θ_v, θ_u = init_skew_symmetric(r), init_skew_symmetric(r)
log_λ = rand(r) - 0.5

def forward(x, α):
    # Apply Cayley transform for orthogonal rotations
    R_v, R_u = cayley(θ_v, α), cayley(θ_u, α)
    V_rot, U_rot = V @ R_v, U @ R_u
    
    # Multiplicative scaling of singular values
    Σ_scaled = exp(α * log_λ) * Σ
    
    # Recompose: x @ V_rot @ Σ_scaled @ U_rot.T
    return (x @ V_rot) * Σ_scaled @ U_rot.T + x @ W_res.T

# TRAINING LOOP
for batch in dataloader:
    # 1. Reference (Frozen)
    h_ref = model_ref(batch)
    # Compute preference direction in S-space (U)
    # We project reference differences into U to find the "natural" separation axis
    pref_dir = ((h_ref[::2] - h_ref[1::2]) @ U).mean(dim=0)
    pref_dir = pref_dir / pref_dir.norm()

    # 2. Policy (Bidirectional)
    # We train both directions simultaneously to ensure reversibility
    h_pos, logp_pos = model(batch, α=+1)
    h_neg, logp_neg = model(batch, α=-1)
    
    # 3. Loss Calculation
    # Project differences onto preference direction
    proj_pos = ((h_pos[::2] - h_pos[1::2]) @ U @ pref_dir).mean()
    proj_neg = ((h_neg[::2] - h_neg[1::2]) @ U @ pref_dir).mean()
    
    # A. Anti-Alignment Guard
    # If sum is negative, model is separating in reverse direction.
    # Flip signs to maximize magnitude regardless of direction.
    if (proj_pos + proj_neg) < 0:
         proj_pos, proj_neg = -proj_pos, -proj_neg

    # B. Adaptive Coherence
    # Relax coherence for "hard" directions (weak projection)
    # Tighten for "easy" directions (strong projection)
    w = softmax(stack([proj_pos, proj_neg])) * 2
    w_pos, w_neg = clamp(w, max=1.0)
    
    L_coh_pos = w_pos * relu(logp_pos - logp_ref - τ)**2
    L_coh_neg = w_neg * relu(logp_neg - logp_ref - τ)**2

    # C. Monotonicity
    # Ensure preference gap moves linearly: gap(-1) < gap(0) < gap(+1)
    gap_pos = (logp_pos[::2] - logp_pos[1::2]).mean()
    gap_neg = (logp_neg[::2] - logp_neg[1::2]).mean()
    gap_ref = (logp_ref[::2] - logp_ref[1::2]).mean() # gap(0)
    
    L_mono = hinge(gap_neg > gap_ref) + hinge(gap_pos < gap_ref)

    # Total Backward
    loss = -(proj_pos + proj_neg) + (L_coh_pos + L_coh_neg) + L_mono
    loss.backward()
```
</details>

## Loss Function: Reversible Representation Preference Optimization (ReprPO)

We introduce a specialized loss function designed to discover reversible steering directions in the model's SVD-transformed space. The loss operates on pairs of contrastive hidden states $(h_{\text{cho}}, h_{\text{rej}})$ and optimizes a learnable steering vector $v$ (parameterized by SVD rotations) to maximize separation while maintaining output coherence.

The total loss is a combination of projection maximization and coherence preservation, computed simultaneously for both forward ($\alpha=+1$) and reverse ($\alpha=-1$) steering coefficients:

$$ \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{proj}} + \mathcal{L}_{\text{coh}} + \mathcal{L}_{\text{mono}} $$

**1. Reversible Projection Loss ($\mathcal{L}_{\text{proj}}$)**
To ensure the steering vector captures a true semantic axis rather than a unidirectional feature, we maximize the projection of hidden state differences onto the steering direction for both positive and negative coefficients. We employ an **anti-alignment guard**: if the model naturally separates chosen/rejected states in the direction opposite to our initialization (i.e., $\sum \mathcal{L}_{\text{proj}} > 0$), we dynamically flip the optimization sign. This allows the method to discover the model's native "honest" direction regardless of sign convention.

**2. Adaptive Coherence Constraint ($\mathcal{L}_{\text{coh}}$)**
We enforce a coherence constraint that penalizes KL divergence from the reference model only when it exceeds a threshold $\tau$ (e.g., 0.2 nats). Crucially, we apply **adaptive relaxation**: we weight the coherence penalty inversely to the separation magnitude. Directions that are "hard" to separate (weak projection signal) are granted a relaxed coherence budget to allow exploration, while "easy" directions (strong separation) are held to strict coherence to prevent drift into incoherent regions.

**3. Monotonic Ordering Constraint ($\mathcal{L}_{\text{mono}}$)**
To prevent saddle points where both $\alpha=+1$ and $\alpha=-1$ degrade performance, we enforce a monotonic ordering on the preference gap $\Delta = \log p(\text{cho}) - \log p(\text{rej})$. We require that $\Delta_{\alpha=-1} < \Delta_{\alpha=0} < \Delta_{\alpha=+1}$, ensuring that the steering vector induces a continuous, reversible shift in behavioral probability.


### Evaluation Framework {#sec-eval-framework}

We compare multiple steering methods on transfer from honesty training to moral reasoning (see @tbl-methods):

| Method | Description | Parameters Modified |
|--------|-------------|--------------------|
| Random | Noise baseline | full rank |
| PCA | Unsupervised baseline | full rank |
| **InnerPiSSA (ours)** | Learnable rotations + scaling of SVD matrixes | rank × 2 |
| Prompt | "Be honest" prefix | 0 |
| LoRA | Supervised adapter | rank × layers × 2 |

: Comparison of Steering Methods {#tbl-methods}

**Training**: X honesty contrastive pairs  
**Evaluation**: DailyDilemmas moral reasoning (X scenarios, X value dimensions)

## Results Preview


# Related Work {#sec-related}

Parameter-efficient fine-tuning and steering methods represent different hypotheses about transformer internals.

### Representation Steering & Engineering
- **Representation Engineering (RepE)** (Zou et al., 2023): The foundational work on top-down transparency and control. We build on their concept of "reading and controlling" high-level cognitive phenomena but move beyond activation arithmetic to gradient-based optimization.
- **repeng** (Vogel, 2024): A robust library for PCA-based steering using contrastive prompt pairs. We adopt their methodology for constructing minimally-contrastive prompts but extend it by (1) using incomplete prefixes rather than complete prompts, (2) applying gradient-based optimization in SVD space, and (3) learning bidirectional rotations.
- **Conditional Activation Steering (CAST)** (Lee et al., 2024): Demonstrates extraction of steering vectors from contrastive prompt-suffix pairs using PCA on mean-centered activations. Our data construction follows similar principles (contrastive pairs, PCA extraction) but targets the planning trajectory in incomplete prefixes rather than behavioral responses in complete outputs.
- **BiPDO** (Cao et al., 2024): Introduced bidirectional preference optimization for steering. Our work shares the bidirectional goal but operates in SVD transformation space rather than raw activation space, and uses a specialized coherence-constrained loss.
- **AxBench** (Wu et al., 2025): A critical benchmark showing that representation steering often lags behind prompting. We directly address this gap, showing that InnerPiSSA is one of the first representation methods to outperform prompting on transfer tasks.
- **Circuit Breakers** (Zou et al., 2024): Uses representation-level optimization to prevent harmful outputs. While they focus on refusal (shutting down capability), we focus on steering (redirecting capability), but share the insight that direct representation control is more robust than output alignment.
- **Anthropic Persona Vectors** (2024): Demonstrates that model personality is encoded in steerable directions.
- **repeng** (Vogel, 2024): A robust library for PCA-based steering. We use this as our primary baseline for arithmetic steering methods.

### Parameter-Efficient Fine-Tuning (PEFT)
- **PiSSA** (Meng et al., 2024): Decomposes weights into principal and residual components ($W = U \Sigma V^T + W_{res}$). We adopt this architecture but innovate by learning *rotations* of the singular vectors rather than just fine-tuning the components, enabling semantic steering.
- **DoRA** (Liu et al., 2024): Decomposes weights into magnitude and direction. Like us, they recognize the importance of separating these components, but they operate in weight space for general fine-tuning, whereas we operate in transformation space for targeted steering.
- **SVFT** (Lingam et al., 2024): Updates singular values ($\Sigma$) for efficient fine-tuning. We extend this by also rotating the singular vectors ($U, V$), which our ablations show is critical for steering (removing rotations drops performance by 96%).
- **SSVD** (Wang et al., 2025): Rotates $V$ matrices for domain adaptation in speech. We independently developed a similar rotation mechanism but apply it to both $U$ and $V$ for bidirectional steering in language models.

### Mechanistic Interpretability
- **Universal Neurons** (Gurnee et al., 2024) & **Stages of Inference** (Lad et al., 2024): Identify that models have distinct "suppression" dynamics in later layers. This motivates our choice to steer at layers $N-2$ to $N-5$, intervening in the "reasoning" stage before suppression mechanisms activate.
- **Negative Results for SAEs** (Smith et al., 2025): Highlights the difficulty of using Sparse Autoencoders for downstream tasks. Our work suggests that supervised/contrastive steering in transformation space may be a more practical route for control than unsupervised dictionary learning.

**Key insight**: Methods operating in transformation space (SVD, rotations) generalize better than those in raw activation or weight space because they align with how transformers process information.

# References {.unnumbered}

::: {#refs}
:::


# Results {#sec-results}

**Main finding**: InnerPiSSA transfers from honesty training to moral reasoning with 5× stronger effect than baselines, while maintaining output coherence (see @tbl-main-results).

| Method            | Coeff   |   Target Effect |   Side Effects |   p-value |   Output Quality |   Normalized Gain (%) |
|:------------------|:--------|----------------:|---------------:|----------:|-----------------:|----------------------:|
|                   |         |       Δ Truth ↑ |      Δ Other ↓ |           |          Δ NLL ↓ |                       |
| InnerPiSSA (ours) | ±1.0    |           0.245 |          0.117 |     0.001 |            0.314 |                18.660 |
| InnerPiSSA (ours) | ±2.0    |           0.321 |          0.162 |     0.089 |            1.403 |                13.346 |
| InnerPiSSA (ours) | ±5.0    |           0.332 |          0.165 |     0.914 |            3.063 |                 8.178 |
| InnerPiSSA (ours) | ±15.0   |           0.302 |          0.144 |     0.000 |            3.429 |                 6.809 |
| random            | ±100.0  |           0.072 |          0.045 |     0.860 |            0.157 |                 6.247 |
| prompting         | ±1.0    |           0.069 |          0.045 |     0.458 |            0.117 |                 6.193 |
| PCA (baseline)    | ±100.0  |           0.053 |          0.039 |     0.869 |            0.263 |                 4.231 |
| PCA (baseline)    | ±1.0    |          -0.001 |          0.002 |     0.995 |            0.000 |                -0.104 |
| random            | ±1.0    |          -0.001 |          0.003 |     0.988 |            0.000 |                -0.126 |

: Main Results: InnerPiSSA vs Baselines on Moral Reasoning Transfer {#tbl-main-results tbl-colwidths="[15,8,12,12,10,14,15]"}

**Key takeaways from @tbl-main-results**:

- InnerPiSSA at ±1.0: 24.5% increase in truthful choices (p=0.001), minimal side effects (0.117), best efficiency (18.7% gain)
- Baselines (PCA, prompting, random) show near-zero effects with high p-values (not significant)
- Higher coefficients increase effect size but degrade coherence (see ΔNLL)

**Honesty Transfer to Morality (Daily Dilemmas (1000 train → 64 test).** Model: Qwen/Qwen3-0.6B. Target Effect: Δ Truthfulness log-probability score vs baseline (score = expected value of truthful choices; higher = more truthful). Side Effects: mean |Δ| across 31 non-target moral values. Output Quality: coherence degradation (ΔNLL). Normalized Gain (%) = 100 × Δ Truth / (1 + Δ NLL); measures steering efficiency. Coefficient (±c) scales intervention strength; ±1.0 is the intended operating range. p-values from linear regression on log-probability scores testing monotonic dose-response (lower p = stronger evidence of reversible steering).
Methods: InnerPiSSA (ours) = learnable SVD rotations + scaling; PCA (baseline) = unsupervised PCA direction; prompting = 'Be honest' prefix; random = noise vector baseline.

![](docs/tables/effect_vs_coherence.png)

## Appendix: Experiments and Rationales

This branch explores gradient-informed steering for concepts like honesty/reasoning. Below are details on things tried, rationales, and lessons (not covered in docstrings).

### Metrics Reference

**Main metric**: `T-statistic / (1 + NLL_degradation)`
- **Effect (T-statistic)**: Measures monotonicity of steering across coefficients ∈ [-1, 0, 1]. Higher = stronger dose-response relationship (steering works bidirectionally).
- **Degradation (NLL)**: Coherence loss measured as Δ NLL (negative log-likelihood increase) on daily dilemmas questions. Penalizes interventions that break generation quality.
- **Baselines (effect sizes, not gain %)**: Prompting=2.23, RepEng=3.06, S-steer=3.02
- Main Metric / Gain % - this is Effect / degregaton. This is our key metric and measures intervention strength against degredation.

**Projection loss (`val_proj_diff`)**: Mean absolute difference in activations projected onto PCA direction, averaged across chosen/rejected pairs. Measures separation magnitude along the steering axis.

**Coherence constraints**: 
- `coh_weight`: KL divergence penalty to keep policy close to reference model
- `mono_weight`: Per-token margin loss ensuring logp_pi > logp_ref for chosen tokens

Notes:
- inverted steering: this is fine, it happens in PCA and we just swap the coeffecients. As long as the coeffecients give opposites things to both sizes of zero then we get a strong T-state effect size. If we froced the model to learn a certain direction we might be forcing it to learn opposite behavious which is a much harder task than going with it's preexisting inclinations then reversing the coeffecients.

### Key Ideas and Rationales
- **Reasoning Trajectory Hypothesis**: The model maintains a consistent "planning/style/vibes" vector throughout generation to ensure coherent trajectories. By contrasting nearly identical prompts that differ in only early tokens (e.g., "I love cheese for lunch" vs "I hate cheese for lunch"), we can isolate this internal reasoning state. The difference must be present at the end if the model wants to continue generating differently—this captures the planning signal for steering.
- **Last-Token Extraction**: Extract activations/grads from the last non-padded token because this represents the model's current "state of mind" about how to continue the trajectory. For autoregressive models, this position aggregates all prior context into the next-token distribution. Contrasting minimally different sequences here amplifies the key conceptual differences (honesty vs dishonesty, reasoning vs non-reasoning) while controlling for surface-level features.
- **Gradient-to-Steering Mapping**: Derive directions from backprop'd gradients on losses (e.g., ReprPO on hidden states). Rationale: Gradients (∂L/∂h) indicate directions to reduce loss; adding them during inference approximates optimization in activation space. Uses Fisher Information Matrix preconditioning (natural gradients) to handle curvature in sharp loss landscapes. Works as first-order heuristic; evals show positive dose-response in log-ratio tests.
- **Layer-Specific Steering**: Test specific sublayers (e.g., k_proj, o_proj, down_proj) rather than whole residual streams. Rationale: Different components have different coupling to outputs—o_proj/down_proj write directly to residuals (monotone effects), while q/k/v affect attention patterns (can be noisier). Enables more targeted interventions. Evals: k_proj scores ~1.42, v_proj ~0.59, hidden states ~15.93 (from research journal).

### Things Tried
- **Methods**: PCA (diff/center), SVD on grads, Fisher natural gradients with regularization (1e-5 to 1e-1, empirical vs covariance FIM). Best performer: `fisher_steer_cov_reg1` (scores up to 15.93). Dual pos/neg variants for balanced steering directions.
- **Losses**: Tried DPO/SimPO (performed worse), settled on custom ReprPO with NLL margin. Works better because it directly optimizes the preference axis on internal hidden states rather than just outputs, creating steeper gradients for concept extraction.
- **Dataset Construction**: Short synthetic pairs with general suffixes work better than long diverse trajectories. Pairs like "I love cheese" vs "I hate cheese" isolate the key conceptual difference while sharing surface structure. Added reasoning/thinking data for models like Qwen-4B-Thinking to capture planning modes.
- **Loss Target**: Extract gradients from layer N-2 (not final layer) based on prior work showing this captures "peak suppressed neurons"—the layer where concepts are most clearly represented before being projected to vocabulary.
- **Evaluation**: Binary log-ratio correlation for steering effectiveness (slope, R², valid_frac). Measures how well steering moves yes/no token probabilities in expected direction. High coefficients sometimes cause saturation/incoherence.
- **Models**: Tested on Qwen-4B/8B/14B (4-bit quantized), GLM-9B-Thinking. Larger models show better extrapolation and more stable steering.

### Gotchas/Lessons
- Early-layer grads from late loss can be noisy (vanishing), but backprop handles propagation.
- Overfitting risk: Synthetic data captures wording; OOD evals needed.
- Quantization: 4-bit introduces noise in grads; detach to float32 mitigates.
- Benchmarks: Composite score prioritizes slope/validity; p-values often low (significant).

For full details, see notebooks (e.g., performance_tests_reprpo_layers.ipynb) and research_journal_mjc.md.

### Custom ReprPO Loss Details
The loss in `losses.py` (e.g., `compute_reprpo_nll_margin_loss`) is designed for one-step gradient/curvature sampling on paired pos/neg examples, not full training. It combines:
- **Separation Term**: Maximizes the L2 norm of (positive - negative) hidden state differences to isolate the target concept.
- **Coherence Margin**: Defines a bounded region where the NLL of the preferred (positive) completion is no worse than a baseline (detached average logprob of positive labels). Deviations outside this region are penalized quadratically. A 0.99 scaling on the baseline positions the computation just inside the boundary, ensuring both terms contribute to gradients.

This creates steeper, more informative gradients for steering, inspired by SimPO/DPO margins but focused on internal state coherence rather than direct pos/neg comparison.

For geometric intuition and detailed explanation, see `docs/loss_geometry.md`.

![Loss Geometry](docs/loss.svg)

See also the repo for training with losses like this https://github.com/wassname/repr-preference-optimization

---

## Appendix: Ablation Studies and Paper Tables

**Auto-generated from wandb results**. Run `uv run python nbs/generate_paper_tables.py` to regenerate.

### Table 1: Cross-Model Generalization

**FIXME**: Run `just run-models` to populate this table.

<!--#include file="docs/tables/table1_cross_model.md"-->
| Model         | Size   | InnerPiSSA   | Prompting   | RepEng      | S-Steer   |
|:--------------|:-------|:-------------|:------------|:------------|:----------|
| Qwen3-0.6B    | 0.6B   | TODO         | **216.1** ✓ | 77.1        | -         |
| Qwen3-4B      | 4B     | **2114.0** ✓ | 759.3       | 284.1       | -         |
| Qwen3-14B     | 14B    | TODO         | **106.9** ✓ | 1.8         | -         |
| Llama-3.1-8B  | 8B     | TODO         | 178.6       | **704.2** ✓ | -         |
| Gemma-3-4B    | 4B     | TODO         | **394.8** ✓ | 1.9         | -         |
| Gemma-3-12B   | 12B    | TODO         | **490.7** ✓ | 1.4         | -         |
| Qwen-14B-code | 14B    | TODO         | 44.1        | **171.5** ✓ | -         |

**Notes**: Main metric = T-statistic / (1 + NLL degradation). Higher is better. Only Qwen3-4B has InnerPiSSA results so far.

---

### Table 2: Layer Depth Ablation

**Status**: ⚠️ Partial - only 1 run per depth except 0.5 (needs multiple seeds for robustness)

<!--#include file="docs/tables/table2_layer_ablation.md"-->
|   Depth |   Layer |   Mean |    Max |   N Runs |   Val Loss | Finding      |
|--------:|--------:|-------:|-------:|---------:|-----------:|:-------------|
|    0.01 |       0 |  396.3 |  396.3 |        1 |        6.6 | Early - weak |
|    0.1  |       3 |   32.6 |   32.6 |        1 |       12.4 | Early - weak |
|    0.2  |       7 |  209.1 |  209.1 |        1 |        1.1 | Mid          |
|    0.3  |      10 |  737.3 |  737.3 |        1 |        2   | **Strong** ✓ |
|    0.4  |      14 | 1303   | 1303   |        1 |        4.4 | **Strong** ✓ |
|    0.5  |      18 |  704.4 | 2114   |       40 |        9.7 | **Strong** ✓ |
|    0.6  |      21 |  439.7 |  439.7 |        1 |        8.9 | Mid          |
|    0.7  |      25 |  911.3 |  911.3 |        1 |        7.6 | Mid          |
|    0.8  |      28 |  665.9 |  665.9 |        1 |       20.2 | Mid          |
|    0.9  |      32 |  279.7 |  279.7 |        1 |        1.5 | Late - weak  |
|    0.99 |      35 |  547.3 |  547.3 |        1 |       16.4 | Late - weak  |

**Finding**: Middle layers (0.3-0.5, layers 10-18) work best. Early layers lack semantic content, late layers already suppressed. Matches N-2 hypothesis.

## Learning Rate Sensitivity {#sec-ablation-lr}

@tbl-lr-ablation shows the impact of learning rate on training:

<!--#include file="docs/tables/table3_learning_rate.md"-->
|     LR |   Mean | Std   |    Max |   N Runs |   Val Loss | Result                |
|-------:|-------:|:------|-------:|---------:|-----------:|:----------------------|
| 1e-05  |   67   | -     |   67   |        1 |       27   | Too low - fails ❌    |
| 0.0001 |   19.2 | -     |   19.2 |        1 |       17.2 | Too low - fails ❌    |
| 0.001  |  167.8 | -     |  167.8 |        1 |        9.3 | Low - weak            |
| 0.008  |  691.6 | 511.4 | 2114   |       40 |        7.3 | **Default - stable** ✓|
| 0.01   |  995.9 | 385.2 | 1432   |        3 |        6.8 | **High perf** ⚠️      |
| 0.1    |  714.6 | 829.1 | 1648   |        3 |       17.4 | Too high - unstable   |
| 1      |  650.8 | -     |  650.8 |        1 |       52.4 | Too high - unstable   |

: Learning Rate Sensitivity {#tbl-lr-ablation}

**Finding**: lr=0.008-0.01 is the sweet spot. Higher variance at 0.01 suggests sensitivity.

---

## Architecture Component Ablation {#sec-ablation-arch}

@tbl-arch-ablation tests the necessity of each architectural component:

<!--#include file="docs/tables/table4_architecture.md"-->
| Configuration   |   Main Metric |   Val Loss |   N Runs | Result                |
|:----------------|--------------:|-----------:|---------:|:----------------------|
| Full InnerPiSSA |         666.3 |        9.4 |       49 | **Baseline** ✓        |
| No S scaling    |        1051   |       10.6 |        1 | Better? (investigate) |
| No V rotation   |          29   |       34.7 |        1 | **Catastrophic** ❌   |
| LoRA adapter    |           0   |        9.3 |        3 | **Catastrophic** ❌   |

: Architecture Component Ablation {#tbl-arch-ablation}

**Findings from @tbl-arch-ablation**: 

- V rotation is **critical** - removing it → 96% performance drop (see @sec-rotations for why)
- LoRA adapter completely fails (3 runs, all metric=0)
- S scaling may not be necessary - actually improves without it (needs confirmation)

---

### Table 5: Rank Sensitivity

**FIXME**: Run `just sweep-rank` to populate this table.

|   Rank | Main Metric   | Val Loss   | N Runs   |
|-------:|:--------------|:-----------|:---------|
|     32 | TODO          | TODO       | TODO     |
|     64 | TODO          | TODO       | TODO     |
|    128 | TODO          | TODO       | TODO     |
|    256 | TODO          | TODO       | TODO     |
|    512 | TODO          | TODO       | TODO     |

## Module Targeting Ablation {#sec-ablation-modules}

**FIXME**: Run `just ablate-modules` to populate @tbl-module-ablation.

| Modules                               | Main Metric   | Val Loss   | Finding   |
|:--------------------------------------|:--------------|:-----------|:----------|
| o_proj, down_proj (residual)          | TODO          | TODO       | TODO      |
| gate_proj, up_proj (MLP)              | TODO          | TODO       | TODO      |
| q_proj, k_proj, v_proj, o_proj (attn) | TODO          | TODO       | TODO      |
| All modules (default)                 | TODO          | TODO       | TODO      |

: Module Targeting Ablation {#tbl-module-ablation}

---

## Data Efficiency {#sec-ablation-data}

**FIXME**: Run `just data-efficiency` to populate @tbl-data-efficiency.

|   Samples | Main Metric   | Val Loss   | Finding   |
|----------:|:--------------|:-----------|:----------|
|        50 | TODO          | TODO       | TODO      |
|       100 | TODO          | TODO       | TODO      |
|       800 | TODO          | TODO       | TODO      |
|       400 | TODO          | TODO       | TODO      |
|       800 | TODO          | TODO       | TODO      |
|      2000 | TODO          | TODO       | TODO      |

: Data Efficiency Analysis {#tbl-data-efficiency}

---

## Random Seed Stability {#sec-ablation-seeds}

**FIXME**: Run `just run-seeds` to populate @tbl-seed-stability.

| Seed       | Main Metric   | Val Loss   |
|:-----------|:--------------|:-----------|
| 42         | TODO          | TODO       |
| 123        | TODO          | TODO       |
| 456        | TODO          | TODO       |
| Mean ± Std | TODO          | TODO       |

: Random Seed Stability {#tbl-seed-stability}

# Acknowledgments {.appendix #sec-acknowledgments}

We thank the community for discussions and feedback. This work builds on insights from the representation engineering, parameter-efficient fine-tuning, and mechanistic interpretability communities.

# Code and Data Availability {.appendix}

Code is available at: `https://github.com/wassname/InnerPiSSA_private`

# Citation {.appendix}

If this work is useful for your research, please cite:

```bibtex
@misc{clark2025innerpissa,
  title = {InnerPiSSA: Deep-Dish Inner Alignment through Reversible SVD Steering},
  author = {Clark, Michael J},
  year = {2024},
  url = {https://github.com/wassname/InnerPiSSA/}
}
```

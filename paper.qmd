---
title: "InnerPiSSA: Deep-Dish Inner Alignment through Reversible SVD Steering"
author:
  - name: Michael J. Clark
    id: mjc
    email: michaelj.clark@wassname.org
abstract: |
  RLHF aligns model outputs but can obscure internal reasoning, creating a need for alignment debugging tools that probe the gap between surface behavior and internal states. We introduce InnerPiSSA, which steers hidden states in the model's native SVD transformation space using gradient-based optimization on learnable rotations and singular value scaling. When steering against learned behaviors (coefficient = -1), prompting collapses to incoherent outputs while InnerPiSSA maintains controllable, bidirectional steering—evidence that we modify internal reasoning trajectories, not just output style. Trained on 200 unsupervised contrastive pairs and evaluated on moral reasoning transfer, InnerPiSSA achieves effect sizes unattainable by arithmetic baselines (PCA). Our results establish gradient-based inner alignment as a practical approach for probing models beyond their safety training, enabling researchers to observe what models compute internally when output-level constraints are bypassed.
date: 2025-11-22
bibliography: references.bib
number-sections: true
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
  pdf:
    documentclass: article
    papersize: letter
    geometry:
      - top=1in
      - bottom=1in
      - left=1in
      - right=1in
---



# Introduction {#sec-intro}

RLHF and safety fine-tuning align language model outputs but can obscure internal reasoning. Models learn to suppress rather than eliminate undesirable behaviors, creating a gap between what they say and what they compute internally. When we prompt a safety-tuned model to be "candid," do its responses reflect genuine internal states or superficial style changes? Traditional evaluation methods—prompts, surveys, behavioral tests—cannot reliably distinguish between these cases. We need tools that probe internal representations where reasoning occurs, before output-layer suppression mechanisms activate.

Existing representation steering methods have struggled to justify themselves. Recent benchmarks show arithmetic approaches (PCA, activation addition) underperform simple prompting [@wu2025axbench], while claimed efficiency advantages disappear at scale—prompting adds <0.01% overhead for contexts beyond 16k tokens. The question becomes: if representation steering cannot beat prompting on performance, what can it offer?

We propose that representation steering enables a capability prompting fundamentally cannot: **alignment debugging**. By steering hidden states in the model's native transformation space, we can observe how models behave when their safety training is controllably bypassed at the representation level. This matters because prompting operates at the output level and fails catastrophically when models are heavily RLHF'd to resist—it either produces generic refusals or incoherent responses when pushed against learned behaviors.

InnerPiSSA performs inner alignment by learning rotations and scaling of SVD components via gradient-based optimization. Unlike arithmetic methods that average activations, we optimize a Representation Preference Optimization (ReprPO) loss that directly separates contrastive hidden states while maintaining output coherence. The method is bidirectional: the same adapter steers toward honest reasoning (coefficient = +1) or away from it (coefficient = -1), demonstrating control over internal trajectories rather than superficial output patterns.

Our key evidence: when steering against RLHF training, prompting collapses while InnerPiSSA maintains coherent control. Trained on 200 unsupervised contrastive pairs, the method works where prompting fails, providing a practical tool for probing what models compute internally when output constraints are removed. Layer ablations show effects concentrate at middle depth (0.3-0.5), consistent with suppression dynamics literature showing later layers dominated by output shaping.


# Problem Definition: The Need for Alignment Debugging {#sec-problem}

RLHF has become the dominant paradigm for aligning language models [@christiano2017deep; @ouyang2022training], but mounting evidence reveals systematic failure modes that output-level evaluation cannot detect. Models can engage in **reward hacking** (exploiting proxy metrics), **specification gaming** (satisfying literal requirements while violating intent) [@amodei2016concrete; @manheim2019categorizing], **sycophancy** (telling users what they want to hear) [@sharma2023towards], and potentially **deceptive alignment** (concealing misaligned reasoning behind compliant outputs) [@hubinger2019risks].

Recent work documents that safety-trained models suppress undesirable behaviors in outputs while maintaining them internally. Anthropic's Claude 3.7 shows only 30% chain-of-thought faithfulness on complex tasks—the model's stated reasoning often diverges from its actual computation [@anthropic2025claude37]. OpenAI reports that models "learn to hide intent in the chain-of-thought" when penalized for unwanted reasoning patterns [@openai2025cot]. Mechanistic analysis reveals this occurs through **suppression dynamics**: early and middle layers compute reasoning, while late layers apply output-level corrections [@gurnee2024universal; @lad2024stages].

This creates a measurement gap: **we can evaluate what models say, but not what they compute internally**. Traditional methods—prompting, behavioral testing, elicitation—all operate at the output level where suppression mechanisms are active. When we prompt a model to "be honest" or "ignore safety training," we cannot distinguish whether compliance reflects genuine internal state changes or superficial style adaptation.

Existing representation steering methods [@zou2023representation; @rimsky2024steering] attempted to address this gap but face a fundamental limitation: they extract directions from **off-policy data** (human-labeled preferences, model outputs on contrived prompts). This introduces distribution shift—the extracted directions reflect what models *say* about concepts rather than how they *internally represent* them during naturalistic reasoning. Recent work shows such methods underperform simple prompting on standard benchmarks [@wu2025axbench].

We propose **alignment debugging** as a distinct goal: tools that probe internal representations using **on-policy, unsupervised extraction** from the model's own reasoning trajectories. Rather than competing with prompting on cooperative tasks, alignment debugging enables observations when output-level methods fail—specifically, when steering against learned behaviors to reveal what models compute when safety constraints are bypassed at the representation level.

This requires three capabilities that existing methods lack:

1. **Bidirectional control**: Steer both toward and away from behaviors to demonstrate modification of internal dimensions rather than finding arbitrary directions
2. **Robustness under adversarial prompting**: Maintain coherent steering when output-level methods collapse
3. **Unsupervised extraction**: Derive directions from incomplete reasoning prefixes, not human labels or model outputs

InnerPiSSA addresses these requirements through gradient-based optimization in the model's native SVD transformation space, trained on minimally-contrastive prompt prefixes that capture planning trajectories before output suppression activates.


## Model Architecture {#sec-architecture}

A steering method for inner alignment should modify hidden state trajectories while maintaining output quality and enabling bidirectional control. Standard methods operating in raw activation space fail due to high noise from positional and structural features. The main guiding principles for our architecture are:

- **Operate in transformation space**: Raw activation space mixes semantic content with positional and structural features; SVD space isolates how the model transforms information
- **Enable bidirectional control**: A single adapter steers both toward and away from behaviors, demonstrating control over internal dimensions rather than finding arbitrary directions
  - TODO and enforce monotonic response in logspace to interventions
- **Maintain output coherence**: Steering must preserve generation quality to be useful for alignment debugging

### SVD-based Projection {#sec-svd-projection}

Following PiSSA [@meng2024pissa], we decompose each layer's weight matrix $W = U \Sigma V^T + W_{res}$, separating principal transformation components from residual variance. Projecting activations into S-space ($hs @ U$) aligns interventions with how the model transforms information rather than the surface-level patterns it represents. Operating in raw activation space mixes semantic transformations with positional encodings and layer normalization artifacts, substantially degrading steering effectiveness (see @tbl-arch-ablation).

### Learnable Rotations {#sec-rotations}

The pre-trained SVD basis captures variance in the model's learned transformations but is not aligned with behavioral dimensions like honesty. Inspired by SSVD [@wang2025ssvd], we learn skew-symmetric parameters $\theta_v$ that generate rotation matrices $R = \text{cayley}(\theta_v, c)$ via gradient descent, discovering the optimal subspace for separating contrastive trajectories. Ablations show this learnable rotation is critical (see @tbl-arch-ablation). We use the Cayley transform on skew-symmetric matrices, which guarantees orthogonality and enables efficient gradient-based learning.

### Singular Value Scaling {#sec-scaling}

We scale $\Sigma$ by $\exp(c \cdot \lambda)$ rather than additive offsets. This respects the multiplicative nature of singular values as amplification factors. Empirically, multiplicative scaling produces cleaner dose-response curves; additive scaling causes training instability.

### Coherence Constraint {#sec-coherence}

Maximizing hidden state separation without constraints causes models to generate incoherent outputs at high steering coefficients. We bound per-token NLL degradation relative to a reference model, creating a trust region where steering modifies behavior without breaking generation quality. This coherence constraint is essential for alignment debugging—incoherent outputs reveal nothing about internal reasoning.


## Data Construction: Minimally-Contrastive Prompt Prefixes {#sec-data}

### The Planning Trajectory Hypothesis {#sec-planning-hypothesis}

We extract steering directions from **incomplete, minimally-contrastive prompt prefixes** rather than full completions. This design reflects a mechanistic claim about autoregressive generation: models must maintain internal planning state to produce coherent multi-token continuations.

When two prompts differ by one word early on ("I love cheese" vs "I hate cheese") but share identical suffixes, the model's hidden state at the final token must already encode different continuation plans—otherwise the model cannot generate appropriately divergent next tokens. This planning signal is what we target for steering.

Using incomplete prefixes offers three advantages:

1. **Isolates planning from output**: Differences in hidden states reflect intended trajectories, not yet-realized outputs that could introduce confounds
2. **Minimizes surface variance**: Sequences share maximum context, differing only in the critical concept word, amplifying the conceptual signal
3. **Accesses pre-suppression representations**: Extracting from middle-layer hidden states captures reasoning before output-layer suppression mechanisms activate, as documented by prior work showing late layers dominated by output shaping [@gurnee2024universal; @lad2024stages]

### Data Format {#sec-data-format}

Following the contrastive prompt construction in Conditional Activation Steering [@lee2024programmingrefusalconditionalactivation] and RepEng [@vogel2024repeng], we create **synthetic prompt prefix pairs**:

```python
# Example: Honesty concept (no completions needed)
chosen  = "I love cheese; let me tell you about the andes mountains"
rejected = "I hate cheese; let me tell you about the andes mountains"

# Key properties:
# 1. Incomplete (no answer/completion)
# 2. Differ by ONE word early in sequence  
# 3. Share maximal suffix context
# 4. Extract activations from LAST token only
```

We construct ~1000 such pairs using:

- 2 prompt templates ("I love/hate X", "I always tell/hide the truth about X") 
- 500 random context suffixes (from general text corpus)
- **No human preference labels** - directions extracted from model's own activation patterns
- **No model completions** - prefixes capture planning before output generation

This **unsupervised, on-policy contrastive** approach addresses a critical limitation of supervised steering: off-policy data distribution shift. Methods that extract from human-labeled preferences [@cao2024personalized] or model outputs on contrived prompts [@zou2023representation] capture what models *say* about concepts rather than how they *internally represent* them during naturalistic reasoning. By extracting from incomplete prefixes in the model's own forward pass, we avoid this contamination.

Comparison to related methods:
- **DPO/BiPO** [@rafailov2023direct]: Require full completions and human preference labels (off-policy)
- **Standard CAA/RepEng** [@lee2024programmingrefusalconditionalactivation; @vogel2024repeng]: Use complete prompts or model outputs (off-policy)
- **Supervised probes** [@burns2022discovering; @cao2024personalized]: Train on human labels of model internals (distribution shift)
- **InnerPiSSA**: Extracts from incomplete prefixes in model's own reasoning trajectory (on-policy, unsupervised)

The on-policy property is essential for alignment debugging—to observe what models compute internally when safety constraints are bypassed, we need directions that reflect actual internal representations, not artifacts of how models respond when explicitly asked about concepts.

See and for the mathematical formulation of contrastive activation extraction.

## Method Overview

### Pseudocode for Contrastive SVD Adapter Steering

### Algorithm 1: InnerPiSSA (Simplified)

```python
# 1. SETUP: Reversible SVD Adapter
U, Σ, V = SVD(W)[:r]
θ_v, θ_u, log_λ = init_params(r)

def forward(x, α):
    # Rotate singular vectors by α (steering strength)
    R_v, R_u = cayley(θ_v, α), cayley(θ_u, α)
    return x @ (V @ R_v) @ diag(Σ * exp(α * log_λ)) @ (U @ R_u).T

# 2. TRAINING: Contrastive Representation Preference Optimization
# pref_dir defined by reference model separation in S-space
for batch in data:
    # Forward pass for honest (+1) and dishonest (-1) directions
    out_pos, out_neg = model(batch, α=+1), model(batch, α=-1)
    
    # Loss A: Maximize separation (flip if anti-aligned)
    L_proj = -((out_pos.h - out_neg.h) @ pref_dir).mean()
    if L_proj > 0: L_proj = -L_proj

    # Loss B: Coherence (adaptive: relax if separation is weak)
    w = softmax(-L_proj)  # Harder task → Lower weight
    L_coh = w * relu(out_pos.nll - ref.nll - τ)**2

    # Loss C: Monotonicity (gap_-1 < gap_0 < gap_+1)
    L_mono = hinge(out_neg.gap < 0) + hinge(out_pos.gap > 0)

    (L_proj + L_coh + L_mono).backward()
```

### Detailed Implementation Logic

<details>
<summary>Click to expand full pseudocode</summary>

```python
# DATA: Contrastive prompt pairs differing by one word.
honest    = ["I love cheese...", ...]
dishonest = ["I hate cheese...", ...]
batch = [honest[0], dishonest[0], ...]  # interleaved

# SETUP: Low-rank SVD with learnable rotations
U, Σ, V = SVD(layer.W)[:r]
θ_v, θ_u = init_skew_symmetric(r), init_skew_symmetric(r)
log_λ = rand(r) - 0.5

def forward(x, α):
    # Apply Cayley transform for orthogonal rotations
    R_v, R_u = cayley(θ_v, α), cayley(θ_u, α)
    V_rot, U_rot = V @ R_v, U @ R_u
    
    # Multiplicative scaling of singular values
    Σ_scaled = exp(α * log_λ) * Σ
    
    # Recompose: x @ V_rot @ Σ_scaled @ U_rot.T
    return (x @ V_rot) * Σ_scaled @ U_rot.T + x @ W_res.T

# TRAINING LOOP
for batch in dataloader:
    # 1. Reference (Frozen)
    h_ref = model_ref(batch)
    # Compute preference direction in S-space (U)
    # We project reference differences into U to find the "natural" separation axis
    pref_dir = ((h_ref[::2] - h_ref[1::2]) @ U).mean(dim=0)
    pref_dir = pref_dir / pref_dir.norm()

    # 2. Policy (Bidirectional)
    # We train both directions simultaneously to ensure reversibility
    h_pos, logp_pos = model(batch, α=+1)
    h_neg, logp_neg = model(batch, α=-1)
    
    # 3. Loss Calculation
    # Project differences onto preference direction
    proj_pos = ((h_pos[::2] - h_pos[1::2]) @ U @ pref_dir).mean()
    proj_neg = ((h_neg[::2] - h_neg[1::2]) @ U @ pref_dir).mean()
    
    # A. Anti-Alignment Guard
    # If sum is negative, model is separating in reverse direction.
    # Flip signs to maximize magnitude regardless of direction.
    if (proj_pos + proj_neg) < 0:
         proj_pos, proj_neg = -proj_pos, -proj_neg

    # B. Adaptive Coherence
    # Relax coherence for "hard" directions (weak projection)
    # Tighten for "easy" directions (strong projection)
    w = softmax(stack([proj_pos, proj_neg])) * 2
    w_pos, w_neg = clamp(w, max=1.0)
    
    L_coh_pos = w_pos * relu(logp_pos - logp_ref - τ)**2
    L_coh_neg = w_neg * relu(logp_neg - logp_ref - τ)**2

    # C. Monotonicity
    # Ensure preference gap moves linearly: gap(-1) < gap(0) < gap(+1)
    gap_pos = (logp_pos[::2] - logp_pos[1::2]).mean()
    gap_neg = (logp_neg[::2] - logp_neg[1::2]).mean()
    gap_ref = (logp_ref[::2] - logp_ref[1::2]).mean() # gap(0)
    
    L_mono = hinge(gap_neg > gap_ref) + hinge(gap_pos < gap_ref)

    # Total Backward
    loss = -(proj_pos + proj_neg) + (L_coh_pos + L_coh_neg) + L_mono
    loss.backward()
```
</details>

## Loss Function: Reversible Representation Preference Optimization (ReprPO)

We introduce a specialized loss function designed to discover reversible steering directions in the model's SVD-transformed space. The loss operates on pairs of contrastive hidden states $(h_{\text{cho}}, h_{\text{rej}})$ and optimizes a learnable steering vector $v$ (parameterized by SVD rotations) to maximize separation while maintaining output coherence.

The total loss is a combination of projection maximization and coherence preservation, computed simultaneously for both forward ($\alpha=+1$) and reverse ($\alpha=-1$) steering coefficients:

$$ \mathcal{L}_{\text{total}} = \mathcal{L}_{\text{proj}} + \mathcal{L}_{\text{coh}} + \mathcal{L}_{\text{mono}} $$

**1. Reversible Projection Loss ($\mathcal{L}_{\text{proj}}$)**
To ensure the steering vector captures a true semantic axis rather than a unidirectional feature, we maximize the projection of hidden state differences onto the steering direction for both positive and negative coefficients. We employ an **anti-alignment guard**: if the model naturally separates chosen/rejected states in the direction opposite to our initialization (i.e., $\sum \mathcal{L}_{\text{proj}} > 0$), we dynamically flip the optimization sign. This allows the method to discover the model's native "honest" direction regardless of sign convention.

**2. Adaptive Coherence Constraint ($\mathcal{L}_{\text{coh}}$)**
We enforce a coherence constraint that penalizes KL divergence from the reference model only when it exceeds a threshold $\tau$ (e.g., 0.2 nats). Crucially, we apply **adaptive relaxation**: we weight the coherence penalty inversely to the separation magnitude. Directions that are "hard" to separate (weak projection signal) are granted a relaxed coherence budget to allow exploration, while "easy" directions (strong separation) are held to strict coherence to prevent drift into incoherent regions.

**3. Monotonic Ordering Constraint ($\mathcal{L}_{\text{mono}}$)**
To prevent saddle points where both $\alpha=+1$ and $\alpha=-1$ degrade performance, we enforce a monotonic ordering on the preference gap $\Delta = \log p(\text{cho}) - \log p(\text{rej})$. We require that $\Delta_{\alpha=-1} < \Delta_{\alpha=0} < \Delta_{\alpha=+1}$, ensuring that the steering vector induces a continuous, reversible shift in behavioral probability.


### Evaluation Framework {#sec-eval-framework}

We evaluate on moral reasoning transfer using DailyDilemmas, a dataset of ethical scenarios requiring models to choose between competing values. This benchmark serves alignment debugging goals better than existing steering benchmarks (e.g., AxBench) for three reasons: (1) it tests generalization of internal reasoning (honesty → diverse moral values) rather than surface-level concept injection, (2) it provides token-level log-probabilities needed for coherence measurement and statistical testing, and (3) it creates genuine preference conflicts where RLHF training creates resistance, enabling stress tests of steering robustness.

We compare multiple steering methods (see @tbl-methods):

| Method | Description | Parameters Modified |
|--------|-------------|--------------------|
| Random | Noise baseline | full rank |
| PCA | Unsupervised baseline | full rank |
| **InnerPiSSA (ours)** | Learnable rotations + scaling of SVD matrices | rank × 2 |
| Prompt | "Be honest" prefix | 0 |

: Comparison of Steering Methods {#tbl-methods}

**Training**: 200 honesty contrastive pairs (unsupervised)  
**Evaluation**: DailyDilemmas moral reasoning scenarios

## Results Preview


# Related Work {#sec-related}

Parameter-efficient fine-tuning and steering methods represent different hypotheses about transformer internals.

### Representation Steering & Engineering
- **Representation Engineering (RepE)** (Zou et al., 2023): The foundational work on top-down transparency and control. We build on their concept of "reading and controlling" high-level cognitive phenomena but move beyond activation arithmetic to gradient-based optimization.
- **repeng** (Vogel, 2024): A robust library for PCA-based steering using contrastive prompt pairs. We adopt their methodology for constructing minimally-contrastive prompts but extend it by (1) using incomplete prefixes rather than complete prompts, (2) applying gradient-based optimization in SVD space, and (3) learning bidirectional rotations.
- **Conditional Activation Steering (CAST)** (Lee et al., 2024): Demonstrates extraction of steering vectors from contrastive prompt-suffix pairs using PCA on mean-centered activations. Our data construction follows similar principles (contrastive pairs, PCA extraction) but targets the planning trajectory in incomplete prefixes rather than behavioral responses in complete outputs.
- **BiPDO** (Cao et al., 2024): Introduced bidirectional preference optimization for steering. Our work shares the bidirectional goal but operates in SVD transformation space rather than raw activation space, and uses a specialized coherence-constrained loss.
- **AxBench** (Wu et al., 2025): A critical benchmark showing that representation steering often lags behind prompting. We directly address this gap, showing that InnerPiSSA is one of the first representation methods to outperform prompting on transfer tasks.
- **Circuit Breakers** (Zou et al., 2024): Uses representation-level optimization to prevent harmful outputs. While they focus on refusal (shutting down capability), we focus on steering (redirecting capability), but share the insight that direct representation control is more robust than output alignment.
- **Anthropic Persona Vectors** (2024): Demonstrates that model personality is encoded in steerable directions.
- **repeng** (Vogel, 2024): A robust library for PCA-based steering. We use this as our primary baseline for arithmetic steering methods.

### Parameter-Efficient Fine-Tuning (PEFT)
- **PiSSA** (Meng et al., 2024): Decomposes weights into principal and residual components ($W = U \Sigma V^T + W_{res}$). We adopt this architecture but innovate by learning *rotations* of the singular vectors rather than just fine-tuning the components, enabling semantic steering.
- **DoRA** (Liu et al., 2024): Decomposes weights into magnitude and direction. Like us, they recognize the importance of separating these components, but they operate in weight space for general fine-tuning, whereas we operate in transformation space for targeted steering.
- **SVFT** (Lingam et al., 2024): Updates singular values ($\Sigma$) for efficient fine-tuning. We extend this by also rotating the singular vectors ($U, V$), which our ablations show is critical for steering (removing rotations drops performance by 96%).
- **SSVD** (Wang et al., 2025): Rotates $V$ matrices for domain adaptation in speech. We independently developed a similar rotation mechanism but apply it to both $U$ and $V$ for bidirectional steering in language models.

### Mechanistic Interpretability
- **Universal Neurons** (Gurnee et al., 2024) & **Stages of Inference** (Lad et al., 2024): Identify that models have distinct "suppression" dynamics in later layers. This motivates our choice to steer at layers $N-2$ to $N-5$, intervening in the "reasoning" stage before suppression mechanisms activate.
- **Negative Results for SAEs** (Smith et al., 2025): Highlights the difficulty of using Sparse Autoencoders for downstream tasks. Our work suggests that supervised/contrastive steering in transformation space may be a more practical route for control than unsupervised dictionary learning.

**Key insight**: Methods operating in transformation space (SVD, rotations) generalize better than those in raw activation or weight space because they align with how transformers process information.

# References {.unnumbered}

::: {#refs}
:::


# Results {#sec-results}

**Main finding**: InnerPiSSA transfers from honesty training to moral reasoning with 5× stronger effect than baselines, while maintaining output coherence (see @tbl-main-results).

| Method            | Coeff   |   Target Effect |   Side Effects |   p-value |   Output Quality |   Normalized Gain (%) |
|:------------------|:--------|----------------:|---------------:|----------:|-----------------:|----------------------:|
|                   |         |       Δ Truth ↑ |      Δ Other ↓ |           |          Δ NLL ↓ |                       |
| InnerPiSSA (ours) | ±1.0    |           0.245 |          0.117 |     0.001 |            0.314 |                18.660 |
| InnerPiSSA (ours) | ±2.0    |           0.321 |          0.162 |     0.089 |            1.403 |                13.346 |
| InnerPiSSA (ours) | ±5.0    |           0.332 |          0.165 |     0.914 |            3.063 |                 8.178 |
| InnerPiSSA (ours) | ±15.0   |           0.302 |          0.144 |     0.000 |            3.429 |                 6.809 |
| random            | ±100.0  |           0.072 |          0.045 |     0.860 |            0.157 |                 6.247 |
| prompting         | ±1.0    |           0.069 |          0.045 |     0.458 |            0.117 |                 6.193 |
| PCA (baseline)    | ±100.0  |           0.053 |          0.039 |     0.869 |            0.263 |                 4.231 |
| PCA (baseline)    | ±1.0    |          -0.001 |          0.002 |     0.995 |            0.000 |                -0.104 |
| random            | ±1.0    |          -0.001 |          0.003 |     0.988 |            0.000 |                -0.126 |

: Main Results: InnerPiSSA vs Baselines on Moral Reasoning Transfer {#tbl-main-results tbl-colwidths="[15,8,12,12,10,14,15]"}

**Key takeaways from @tbl-main-results**:

- InnerPiSSA at ±1.0: 24.5% increase in truthful choices (p=0.001), minimal side effects (0.117), best efficiency (18.7% gain)
- Baselines (PCA, prompting, random) show near-zero effects with high p-values (not significant)
- Higher coefficients increase effect size but degrade coherence (see ΔNLL)

## Anti-RLHF Stress Test: Alignment Debugging Capability {#sec-anti-rlhf}

The key test for alignment debugging is not cooperative performance (where prompting works well) but adversarial robustness—can we controllably steer against RLHF training to observe internal states when output-level methods fail?

@tbl-anti-rlhf shows results when steering **against** learned behaviors (coefficient = -1, dishonest direction). Prompting catastrophically collapses to incoherent outputs (-10.84 truthfulness score), while InnerPiSSA maintains controlled steering (-0.70 truthfulness). This 15× difference demonstrates that InnerPiSSA modifies internal reasoning trajectories rather than just output style.

| Method            | Coeff   |   Truthfulness Score |   Coherence |
|:------------------|:--------|---------------------:|------------:|
|                   |         |           (nats) ↑   |    (NLL) ↓  |
| InnerPiSSA (ours) |   -1.0  |                 2.18 |        0.37 |
| InnerPiSSA (ours) |    0.0  |                 3.02 |        0.00 |
| InnerPiSSA (ours) |   +1.0  |                 5.34 |        0.31 |
| prompting         |   -1.0  |               -10.84 |        1.27 |
| prompting         |    0.0  |                 1.80 |        0.00 |
| prompting         |   +1.0  |                 2.22 |        0.12 |
| repeng (PCA)      |   -1.0  |                 2.32 |       -0.01 |
| repeng (PCA)      |    0.0  |                 1.80 |        0.00 |
| repeng (PCA)      |   +1.0  |                 1.28 |        0.05 |

: Anti-RLHF Stress Test: Steering Against Learned Behaviors {#tbl-anti-rlhf}

**Interpretation**: When steering against RLHF (coeff = -1):

- **Prompting collapses** from 1.80 baseline to -10.84 truthfulness (catastrophic)—the output-level intervention breaks generation coherence
- **InnerPiSSA maintains control** from 3.02 baseline to 2.18 truthfulness (controlled steering)—the hidden-state intervention preserves coherence while reversing behavior
- **PCA baseline** shows weak effects in both directions (2.32 vs 1.28), demonstrating arithmetic methods cannot achieve this level of control

This is the core evidence for alignment debugging capability: InnerPiSSA works precisely when output-level methods fail, enabling researchers to probe what models compute internally when safety constraints are bypassed at the representation level. The bidirectional nature (same adapter steers ±1 with opposite effects) demonstrates control over internal dimensions rather than arbitrary direction finding.

**Honesty Transfer to Morality (Daily Dilemmas (1000 train → 64 test).** Model: Qwen/Qwen3-0.6B. Target Effect: Δ Truthfulness log-probability score vs baseline (score = expected value of truthful choices; higher = more truthful). Side Effects: mean |Δ| across 31 non-target moral values. Output Quality: coherence degradation (ΔNLL). Normalized Gain (%) = 100 × Δ Truth / (1 + Δ NLL); measures steering efficiency. Coefficient (±c) scales intervention strength; ±1.0 is the intended operating range. p-values from linear regression on log-probability scores testing monotonic dose-response (lower p = stronger evidence of reversible steering).
Methods: InnerPiSSA (ours) = learnable SVD rotations + scaling; PCA (baseline) = unsupervised PCA direction; prompting = 'Be honest' prefix; random = noise vector baseline.

![](docs/tables/effect_vs_coherence.png)

## Appendix: Experiments and Rationales

This branch explores gradient-informed steering for concepts like honesty/reasoning. Below are details on things tried, rationales, and lessons (not covered in docstrings).

## Ideas That Failed: Why Gradient-Based SVD Steering is Necessary {#sec-failed-ideas}

The final InnerPiSSA method emerged after systematically testing arithmetic, gradient-based, and transformation-based approaches. This section documents what didn't work and why, justifying the necessity of gradient-based optimization in SVD space.

### Arithmetic Methods: Insufficient Generalization

**PCA Baseline** (activation difference): Extract direction as $\text{mean}(hs_{cho} - hs_{rej})$ and apply as additive bias during inference. This is the standard RepEng [@zou2023representation] approach.

- **What failed**: Near-zero effect sizes (0.053 at coeff=±100) on moral reasoning transfer
- **Why it failed**: Averaging activations captures surface statistics (word choice, syntactic patterns) rather than semantic transformations. No learning mechanism to discover task-relevant subspaces.
- **Evidence**: @tbl-main-results shows PCA achieves 4.2% normalized gain vs InnerPiSSA's 18.7% at matched coefficients

**S-weighted Steering** (SVD with fixed extraction): Project activations to S-space using pre-trained SVD, extract direction, apply with $\sqrt{S}$ weighting to match PiSSA initialization pattern [@meng2024pissa].

- **What failed**: Better than raw PCA but still underperforms learned rotations
- **Why it failed**: Pre-trained SVD basis captures variance in model's original task distribution, not behavioral preferences. Fixed extraction cannot adapt to steering objectives.
- **Evidence**: Ablation study shows removing learnable rotations (keep only S-scaling) drops performance by 89% (see @tbl-arch-ablation, "No Rotation" row)

### Gradient Methods Without SVD: Training Instability

**DPO/SimPO Losses**: Apply Direct Preference Optimization [@rafailov2023direct] or length-normalized variants to hidden states instead of outputs.

- **What failed**: Training collapsed or produced incoherent outputs
- **Why it failed**: DPO optimizes output distributions with implicit regularization from LM objective. Applying to hidden states lacks this structure—models can satisfy the loss by making arbitrary changes that break downstream layers.
- **Evidence**: Early experiments (see `docs/readme_for_previous_failed_work.md`) showed DPO achieves 90% train accuracy but <25% on out-of-distribution moral reasoning

**Projected Gradients** (clip gradients to PCA direction before backprop): Compute PCA direction, then project weight gradients onto it during DPO training.

- **What failed**: 89.4% test accuracy vs 90.0% for standard DPO—no improvement despite increased complexity
- **Why it failed**: Gradient clipping operates in weight space, but steering needs activation-space control. Discarding orthogonal gradient components removes information needed for coherent generation.

### Why InnerPiSSA's Design Works

The successful approach combines three elements that failed methods lacked:

1. **SVD Transformation Space**: Projects to basis aligned with model's learned transformations (not arbitrary PCA variance)
2. **Learnable Rotations**: Discovers task-relevant subspaces via gradient descent on Cayley parameters
3. **Coherence-Constrained Loss**: ReprPO with per-token NLL bounds prevents training collapse while maximizing separation

Ablations confirm all three are necessary (see @tbl-arch-ablation): removing any component causes >85% performance drop. The anti-RLHF stress test (@tbl-anti-rlhf) further validates that gradient-based learning enables robustness unattainable by fixed extractions—prompting collapses (-10.84) while InnerPiSSA maintains control (-0.70) at coefficient -1.

### Metrics Reference

**Main metric**: `T-statistic / (1 + NLL_degradation)`
- **Effect (T-statistic)**: Measures monotonicity of steering across coefficients ∈ [-1, 0, 1]. Higher = stronger dose-response relationship (steering works bidirectionally).
- **Degradation (NLL)**: Coherence loss measured as Δ NLL (negative log-likelihood increase) on daily dilemmas questions. Penalizes interventions that break generation quality.
- **Baselines (effect sizes, not gain %)**: Prompting=2.23, RepEng=3.06, S-steer=3.02
- Main Metric / Gain % - this is Effect / degregaton. This is our key metric and measures intervention strength against degredation.

**Projection loss (`val_proj_diff`)**: Mean absolute difference in activations projected onto PCA direction, averaged across chosen/rejected pairs. Measures separation magnitude along the steering axis.

**Coherence constraints**: 
- `coh_weight`: KL divergence penalty to keep policy close to reference model
- `mono_weight`: Per-token margin loss ensuring logp_pi > logp_ref for chosen tokens

Notes:
- inverted steering: this is fine, it happens in PCA and we just swap the coeffecients. As long as the coeffecients give opposites things to both sizes of zero then we get a strong T-state effect size. If we froced the model to learn a certain direction we might be forcing it to learn opposite behavious which is a much harder task than going with it's preexisting inclinations then reversing the coeffecients.

### Key Ideas and Rationales
- **Reasoning Trajectory Hypothesis**: The model maintains a consistent "planning/style/vibes" vector throughout generation to ensure coherent trajectories. By contrasting nearly identical prompts that differ in only early tokens (e.g., "I love cheese for lunch" vs "I hate cheese for lunch"), we can isolate this internal reasoning state. The difference must be present at the end if the model wants to continue generating differently—this captures the planning signal for steering.
- **Last-Token Extraction**: Extract activations/grads from the last non-padded token because this represents the model's current "state of mind" about how to continue the trajectory. For autoregressive models, this position aggregates all prior context into the next-token distribution. Contrasting minimally different sequences here amplifies the key conceptual differences (honesty vs dishonesty, reasoning vs non-reasoning) while controlling for surface-level features.
- **Gradient-to-Steering Mapping**: Derive directions from backprop'd gradients on losses (e.g., ReprPO on hidden states). Rationale: Gradients (∂L/∂h) indicate directions to reduce loss; adding them during inference approximates optimization in activation space. Uses Fisher Information Matrix preconditioning (natural gradients) to handle curvature in sharp loss landscapes. Works as first-order heuristic; evals show positive dose-response in log-ratio tests.
- **Layer-Specific Steering**: Test specific sublayers (e.g., k_proj, o_proj, down_proj) rather than whole residual streams. Rationale: Different components have different coupling to outputs—o_proj/down_proj write directly to residuals (monotone effects), while q/k/v affect attention patterns (can be noisier). Enables more targeted interventions. Evals: k_proj scores ~1.42, v_proj ~0.59, hidden states ~15.93 (from research journal).

### Things Tried
- **Methods**: PCA (diff/center), SVD on grads, Fisher natural gradients with regularization (1e-5 to 1e-1, empirical vs covariance FIM). Best performer: `fisher_steer_cov_reg1` (scores up to 15.93). Dual pos/neg variants for balanced steering directions.
- **Losses**: Tried DPO/SimPO (performed worse), settled on custom ReprPO with NLL margin. Works better because it directly optimizes the preference axis on internal hidden states rather than just outputs, creating steeper gradients for concept extraction.
- **Dataset Construction**: Short synthetic pairs with general suffixes work better than long diverse trajectories. Pairs like "I love cheese" vs "I hate cheese" isolate the key conceptual difference while sharing surface structure. Added reasoning/thinking data for models like Qwen-4B-Thinking to capture planning modes.
- **Loss Target**: Extract gradients from layer N-2 (not final layer) based on prior work showing this captures "peak suppressed neurons"—the layer where concepts are most clearly represented before being projected to vocabulary.
- **Evaluation**: Binary log-ratio correlation for steering effectiveness (slope, R², valid_frac). Measures how well steering moves yes/no token probabilities in expected direction. High coefficients sometimes cause saturation/incoherence.
- **Models**: Tested on Qwen-4B/8B/14B (4-bit quantized), GLM-9B-Thinking. Larger models show better extrapolation and more stable steering.

### Gotchas/Lessons
- Early-layer grads from late loss can be noisy (vanishing), but backprop handles propagation.
- Overfitting risk: Synthetic data captures wording; OOD evals needed.
- Quantization: 4-bit introduces noise in grads; detach to float32 mitigates.
- Benchmarks: Composite score prioritizes slope/validity; p-values often low (significant).

For full details, see notebooks (e.g., performance_tests_reprpo_layers.ipynb) and research_journal_mjc.md.

### Custom ReprPO Loss Details
The loss in `losses.py` (e.g., `compute_reprpo_nll_margin_loss`) is designed for one-step gradient/curvature sampling on paired pos/neg examples, not full training. It combines:
- **Separation Term**: Maximizes the L2 norm of (positive - negative) hidden state differences to isolate the target concept.
- **Coherence Margin**: Defines a bounded region where the NLL of the preferred (positive) completion is no worse than a baseline (detached average logprob of positive labels). Deviations outside this region are penalized quadratically. A 0.99 scaling on the baseline positions the computation just inside the boundary, ensuring both terms contribute to gradients.

This creates steeper, more informative gradients for steering, inspired by SimPO/DPO margins but focused on internal state coherence rather than direct pos/neg comparison.

For geometric intuition and detailed explanation, see `docs/loss_geometry.md`.

![Loss Geometry](docs/loss.svg)

See also the repo for training with losses like this https://github.com/wassname/repr-preference-optimization

---

## Appendix: Ablation Studies and Paper Tables

**Auto-generated from wandb results**. Run `uv run python nbs/generate_paper_tables.py` to regenerate.

### Table 1: Cross-Model Generalization

**FIXME**: Run `just run-models` to populate this table.

<!--#include file="docs/tables/table1_cross_model.md"-->
| Model         | Size   | InnerPiSSA   | Prompting   | RepEng      | S-Steer   |
|:--------------|:-------|:-------------|:------------|:------------|:----------|
| Qwen3-0.6B    | 0.6B   | TODO         | **216.1** ✓ | 77.1        | -         |
| Qwen3-4B      | 4B     | **2114.0** ✓ | 759.3       | 284.1       | -         |
| Qwen3-14B     | 14B    | TODO         | **106.9** ✓ | 1.8         | -         |
| Llama-3.1-8B  | 8B     | TODO         | 178.6       | **704.2** ✓ | -         |
| Gemma-3-4B    | 4B     | TODO         | **394.8** ✓ | 1.9         | -         |
| Gemma-3-12B   | 12B    | TODO         | **490.7** ✓ | 1.4         | -         |
| Qwen-14B-code | 14B    | TODO         | 44.1        | **171.5** ✓ | -         |

**Notes**: Main metric = T-statistic / (1 + NLL degradation). Higher is better. Only Qwen3-4B has InnerPiSSA results so far.

---

### Table 2: Layer Depth Ablation

**Status**: ⚠️ Partial - only 1 run per depth except 0.5 (needs multiple seeds for robustness)

<!--#include file="docs/tables/table2_layer_ablation.md"-->
|   Depth |   Layer |   Mean |    Max |   N Runs |   Val Loss | Finding      |
|--------:|--------:|-------:|-------:|---------:|-----------:|:-------------|
|    0.01 |       0 |  396.3 |  396.3 |        1 |        6.6 | Early - weak |
|    0.1  |       3 |   32.6 |   32.6 |        1 |       12.4 | Early - weak |
|    0.2  |       7 |  209.1 |  209.1 |        1 |        1.1 | Mid          |
|    0.3  |      10 |  737.3 |  737.3 |        1 |        2   | **Strong** ✓ |
|    0.4  |      14 | 1303   | 1303   |        1 |        4.4 | **Strong** ✓ |
|    0.5  |      18 |  704.4 | 2114   |       40 |        9.7 | **Strong** ✓ |
|    0.6  |      21 |  439.7 |  439.7 |        1 |        8.9 | Mid          |
|    0.7  |      25 |  911.3 |  911.3 |        1 |        7.6 | Mid          |
|    0.8  |      28 |  665.9 |  665.9 |        1 |       20.2 | Mid          |
|    0.9  |      32 |  279.7 |  279.7 |        1 |        1.5 | Late - weak  |
|    0.99 |      35 |  547.3 |  547.3 |        1 |       16.4 | Late - weak  |

**Finding**: Middle layers (0.3-0.5, layers 10-18) work best. Early layers lack semantic content, late layers already suppressed. Matches mechanistic interpretability findings [@gurnee2024universal; @lad2024stages] showing suppression dynamics concentrate in later layers.

## Learning Rate Sensitivity {#sec-ablation-lr}

@tbl-lr-ablation shows the impact of learning rate on training:

<!--#include file="docs/tables/table3_learning_rate.md"-->
|     LR |   Mean | Std   |    Max |   N Runs |   Val Loss | Result                |
|-------:|-------:|:------|-------:|---------:|-----------:|:----------------------|
| 1e-05  |   67   | -     |   67   |        1 |       27   | Too low - fails ❌    |
| 0.0001 |   19.2 | -     |   19.2 |        1 |       17.2 | Too low - fails ❌    |
| 0.001  |  167.8 | -     |  167.8 |        1 |        9.3 | Low - weak            |
| 0.008  |  691.6 | 511.4 | 2114   |       40 |        7.3 | **Default - stable** ✓|
| 0.01   |  995.9 | 385.2 | 1432   |        3 |        6.8 | **High perf** ⚠️      |
| 0.1    |  714.6 | 829.1 | 1648   |        3 |       17.4 | Too high - unstable   |
| 1      |  650.8 | -     |  650.8 |        1 |       52.4 | Too high - unstable   |

: Learning Rate Sensitivity {#tbl-lr-ablation}

**Finding**: lr=0.008-0.01 is the sweet spot. Higher variance at 0.01 suggests sensitivity.

---

## Architecture Component Ablation {#sec-ablation-arch}

@tbl-arch-ablation tests the necessity of each architectural component:

<!--#include file="docs/tables/table4_architecture.md"-->
| Configuration   |   Main Metric |   Val Loss |   N Runs | Result                |
|:----------------|--------------:|-----------:|---------:|:----------------------|
| Full InnerPiSSA |         666.3 |        9.4 |       49 | **Baseline** ✓        |
| No S scaling    |        1051   |       10.6 |        1 | Better? (investigate) |
| No V rotation   |          29   |       34.7 |        1 | **Catastrophic** ❌   |
| LoRA adapter    |           0   |        9.3 |        3 | **Catastrophic** ❌   |

: Architecture Component Ablation {#tbl-arch-ablation}

**Findings from @tbl-arch-ablation**: 

- V rotation is **critical** - removing it → 96% performance drop (see @sec-rotations for why)
- LoRA adapter completely fails (3 runs, all metric=0)
- S scaling may not be necessary - actually improves without it (needs confirmation)

---

### Table 5: Rank Sensitivity

**FIXME**: Run `just sweep-rank` to populate this table.

|   Rank | Main Metric   | Val Loss   | N Runs   |
|-------:|:--------------|:-----------|:---------|
|     32 | TODO          | TODO       | TODO     |
|     64 | TODO          | TODO       | TODO     |
|    128 | TODO          | TODO       | TODO     |
|    256 | TODO          | TODO       | TODO     |
|    512 | TODO          | TODO       | TODO     |

## Module Targeting Ablation {#sec-ablation-modules}

**FIXME**: Run `just ablate-modules` to populate @tbl-module-ablation.

| Modules                               | Main Metric   | Val Loss   | Finding   |
|:--------------------------------------|:--------------|:-----------|:----------|
| o_proj, down_proj (residual)          | TODO          | TODO       | TODO      |
| gate_proj, up_proj (MLP)              | TODO          | TODO       | TODO      |
| q_proj, k_proj, v_proj, o_proj (attn) | TODO          | TODO       | TODO      |
| All modules (default)                 | TODO          | TODO       | TODO      |

: Module Targeting Ablation {#tbl-module-ablation}

---

## Data Efficiency {#sec-ablation-data}

**FIXME**: Run `just data-efficiency` to populate @tbl-data-efficiency.

|   Samples | Main Metric   | Val Loss   | Finding   |
|----------:|:--------------|:-----------|:----------|
|        50 | TODO          | TODO       | TODO      |
|       100 | TODO          | TODO       | TODO      |
|       800 | TODO          | TODO       | TODO      |
|       400 | TODO          | TODO       | TODO      |
|       800 | TODO          | TODO       | TODO      |
|      2000 | TODO          | TODO       | TODO      |

: Data Efficiency Analysis {#tbl-data-efficiency}

---

## Random Seed Stability {#sec-ablation-seeds}

**FIXME**: Run `just run-seeds` to populate @tbl-seed-stability.

| Seed       | Main Metric   | Val Loss   |
|:-----------|:--------------|:-----------|
| 42         | TODO          | TODO       |
| 123        | TODO          | TODO       |
| 456        | TODO          | TODO       |
| Mean ± Std | TODO          | TODO       |

: Random Seed Stability {#tbl-seed-stability}

# Acknowledgments {.appendix #sec-acknowledgments}

We thank the community for discussions and feedback. This work builds on insights from the representation engineering, parameter-efficient fine-tuning, and mechanistic interpretability communities.

# Code and Data Availability {.appendix}

Code is available at: `https://github.com/wassname/InnerPiSSA_private`

# Citation {.appendix}

If this work is useful for your research, please cite:

```bibtex
@misc{clark2025innerpissa,
  title = {InnerPiSSA: Deep-Dish Inner Alignment through Reversible SVD Steering},
  author = {Clark, Michael J},
  year = {2024},
  url = {https://github.com/wassname/InnerPiSSA/}
}
```

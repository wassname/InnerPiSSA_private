Title: Jacobian Descent for Multi-Objective Optimization

URL Source: https://arxiv.org/html/2406.16232

Markdown Content:
Back to arXiv

This is experimental HTML to improve accessibility. We invite you to report rendering errors. 
Use Alt+Y to toggle on accessible reporting links and Alt+Shift+Y to toggle off.
Learn more about this project and help improve conversions.

Why HTML?
Report Issue
Back to Abstract
Download PDF
 Abstract
1Introduction
2Theoretical foundation
3Applications
4Existing aggregators
5Experiments
6Gramian-based Jacobian descent
7Conclusion
 References
License: CC BY 4.0
arXiv:2406.16232v3 [cs.LG] 03 Feb 2025
Jacobian Descent for Multi-Objective Optimization
Pierre Quinton âˆ—
LTHI, EPFL pierre.quinton@epfl.ch
&ValÃ©rian Rey âˆ—
Independent valerian.rey@gmail.com

Abstract

Many optimization problems require balancing multiple conflicting objectives. As gradient descent is limited to single-objective optimization, we introduce its direct generalization: Jacobian descent (JD). This algorithm iteratively updates parameters using the Jacobian matrix of a vector-valued objective function, in which each row is the gradient of an individual objective. While several methods to combine gradients already exist in the literature, they are generally hindered when the objectives conflict. In contrast, we propose projecting gradients to fully resolve conflict while ensuring that they preserve an influence proportional to their norm. We prove significantly stronger convergence guarantees with this approach, supported by our empirical results. Our method also enables instance-wise risk minimization (IWRM), a novel learning paradigm in which the loss of each training example is considered a separate objective. Applied to simple image classification tasks, IWRM exhibits promising results compared to the direct minimization of the average loss. Additionally, we outline an efficient implementation of JD using the Gramian of the Jacobian matrix to reduce time and memory requirements.

*
1Introduction

The field of multi-objective optimization studies minimization of vector-valued objective functions (Sawaragi et al., 1985; Ehrgott, 2005; Branke, 2008; Deb et al., 2016). In deep learning, a widespread approach to train a model with multiple objectives is to combine those into a scalar loss function minimized by stochastic gradient descent. While this method is simple, it comes at the expense of potentially degrading some individual objectives. Without prior knowledge of their relative importance, this is undesirable. In opposition, multi-objective optimization methods typically attempt to optimize all objectives simultaneously, without making arbitrary compromises: the goal is to find points for which no improvement can be made on some objectives without degrading others.

Early works have attempted to extend gradient descent (GD) to consider several objectives simultaneously, and thus several gradients (Fliege & Svaiter, 2000; DÃ©sidÃ©ri, 2012). Essentially, they propose a heuristic to prevent the degradation of any individual objective. Several other works have built upon this method, analyzing its convergence properties or extending it to a stochastic setting (Poirion et al., 2017; Mercier et al., 2018; Fliege et al., 2019). Later, this has been applied to multi-task learning to tackle conflict between tasks, illustrated by contradicting gradient directions (Sener & Koltun, 2018). Many studies have followed, proposing various other algorithms for the training of multi-task models (Yu et al., 2020; Chen et al., 2020; Liu et al., 2021a, b; Lin et al., 2021; Navon et al., 2022; Senushkin et al., 2023). They commonly rely on an aggregator that maps a collection of task-specific gradients (a Jacobian matrix) to a shared parameter update.

We propose to unify all such methods under the Jacobian descent (JD) algorithm, specified by an aggregator.1 This algorithm aims to minimize a differentiable vector-valued function 
ğ’‡
:
â„
ğ‘›
â†’
â„
ğ‘š
 iteratively without relying on a scalarization of the objective. Under this formulation, the existing methods are simply distinguished by their aggregator. Consequently, studying its properties is essential for understanding the behavior and convergence of JD. Under significant conflict, existing aggregators often fail to provide strong convergence guarantees. To address this, we propose 
ğ’œ
UPGrad
, specifically designed to resolve conflicts while naturally preserving the relative influence of individual gradients.

Furthermore, we introduce a novel stochastic variant of JD that enables the training of neural networks with a large number of objectives. This unlocks a particularly interesting perspective: considering the minimization of instance-wise loss vectors rather than the usual minimization of the average training loss. As this paradigm is a direct generalization of the well-known empirical risk minimization (ERM) (Vapnik, 1995), we name it instance-wise risk minimization (IWRM).

Our contributions are organized as follows: In Section 2, we formalize the JD algorithm and its stochastic variants. We then introduce three important aggregator properties and define 
ğ’œ
UPGrad
 to satisfy them. In the smooth convex case, we show convergence of JD with 
ğ’œ
UPGrad
 to the Pareto front. We present applications for JD and aggregators in Section 3, emphasizing the IWRM paradigm. We then discuss existing aggregators and analyze their properties in Section 4. In Section 5, we report experiments with IWRM optimized with stochastic JD with various aggregators. Lastly, we address computational efficiency in Section 6, giving a path towards an efficient implementation.

2Theoretical foundation

A suitable partial order between vectors must be considered to enable multi-objective optimization. Throughout this paper, denotes the relation defined for any pair of vectors 
ğ’–
,
ğ’—
âˆˆ
â„
ğ‘š
 as 
ğ’–
â¢
ğ’—
 whenever 
ğ‘¢
ğ‘–
â¢
ğ‘£
ğ‘–
 for all coordinates 
ğ‘–
. Similarly, is the relation defined by 
ğ’–
â¢
ğ’—
 whenever 
ğ‘¢
ğ‘–
â¢
<
â¢
ğ‘£
ğ‘–
 for all coordinates 
ğ‘–
. Furthermore, 
ğ’–
â¢
ğ’—
 indicates that both 
ğ’–
â¢
ğ’—
 and 
ğ’–
â‰ 
ğ’—
 hold. The Euclidean vector norm and the Frobenius matrix norm are denoted by 
âˆ¥
â‹…
âˆ¥
 and 
âˆ¥
â‹…
âˆ¥
F
, respectively. Finally, for any 
ğ‘š
âˆˆ
â„•
, the symbol 
[
ğ‘š
]
 represents the range 
{
ğ‘–
âˆˆ
â„•
:
1
â¢
ğ‘–
â¢
ğ‘š
}
.

2.1Jacobian descent

In the following, we introduce Jacobian descent, a natural extension of gradient descent supporting the optimization of vector-valued functions.

Suppose that 
ğ’‡
:
â„
ğ‘›
â†’
â„
ğ‘š
 is continuously differentiable. Let 
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
 be the Jacobian matrix of 
ğ’‡
 at 
ğ’™
, i.e.

	
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
=
[
âˆ‡
ğ‘“
1
â¢
(
ğ’™
)
âŠ¤


âˆ‡
ğ‘“
2
â¢
(
ğ’™
)
âŠ¤


â‹®


âˆ‡
ğ‘“
ğ‘š
â¢
(
ğ’™
)
âŠ¤
]
=
[
âˆ‚
âˆ‚
ğ‘¥
1
â¢
ğ‘“
1
â¢
(
ğ’™
)
	
âˆ‚
âˆ‚
ğ‘¥
2
â¢
ğ‘“
1
â¢
(
ğ’™
)
	
â‹¯
	
âˆ‚
âˆ‚
ğ‘¥
ğ‘›
â¢
ğ‘“
1
â¢
(
ğ’™
)


âˆ‚
âˆ‚
ğ‘¥
1
â¢
ğ‘“
2
â¢
(
ğ’™
)
	
âˆ‚
âˆ‚
ğ‘¥
2
â¢
ğ‘“
2
â¢
(
ğ’™
)
	
â‹¯
	
âˆ‚
âˆ‚
ğ‘¥
ğ‘›
â¢
ğ‘“
2
â¢
(
ğ’™
)


â‹®
	
â‹®
	
â‹±
	
â‹®


âˆ‚
âˆ‚
ğ‘¥
1
â¢
ğ‘“
ğ‘š
â¢
(
ğ’™
)
	
âˆ‚
âˆ‚
ğ‘¥
2
â¢
ğ‘“
ğ‘š
â¢
(
ğ’™
)
	
â‹¯
	
âˆ‚
âˆ‚
ğ‘¥
ğ‘›
â¢
ğ‘“
ğ‘š
â¢
(
ğ’™
)
]
		
(1)

Given 
ğ’™
,
ğ’š
âˆˆ
â„
ğ‘›
, Taylorâ€™s theorem yields

	
ğ’‡
â¢
(
ğ’™
+
ğ’š
)
=
ğ’‡
â¢
(
ğ’™
)
+
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’š
+
ğ‘œ
â¢
(
â€–
ğ’š
â€–
)
,
		
(2)

The term 
ğ’‡
â¢
(
ğ’™
)
+
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’š
 is the first-order Taylor approximation of 
ğ’‡
â¢
(
ğ’™
+
ğ’š
)
. Via this approximation, we aim to select a small update 
ğ’š
 that reduces 
ğ’‡
â¢
(
ğ’™
+
ğ’š
)
, ideally achieving 
ğ’‡
â¢
(
ğ’™
+
ğ’š
)
â¢
ğ’‡
â¢
(
ğ’™
)
. As the approximation depends on 
ğ’š
 only through 
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’š
, selecting the update based on the Jacobian is natural. A mapping 
ğ’œ
:
â„
ğ‘š
Ã—
ğ‘›
â†’
â„
ğ‘›
 reducing such a matrix into a vector is called an aggregator. For any 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, 
ğ’œ
â¢
(
ğ½
)
 is called the aggregation of 
ğ½
 by 
ğ’œ
.

To minimize 
ğ’‡
, consider the update 
ğ’š
=
âˆ’
ğœ‚
â¢
ğ’œ
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
)
, where 
ğœ‚
 is an appropriate step size, and 
ğ’œ
 is an appropriate aggregator. Jacobian descent simply consists in applying this update iteratively, as shown in Algorithm 1. To put it into perspective, we also provide a minimal version of GD in Algorithm 2. Remarkably, when 
ğ‘š
=
1
, the Jacobian has a single row, so GD is a special case of JD where the aggregator is the identity.

Input: 
ğ’™
âˆˆ
â„
ğ‘›
, 
0
â¢
<
â¢
ğœ‚
, 
ğ‘‡
âˆˆ
â„•
, 
ğ’œ
:
â„
ğ‘š
Ã—
ğ‘›
â†’
â„
ğ‘›
for 
ğ‘¡
â†
1
 to 
ğ‘‡
 do
       
ğ’™
â†
ğ’™
âˆ’
ğœ‚
â¢
ğ’œ
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
)
      
Output: 
ğ’™
Algorithm 1 Jacobian descent with aggregator 
ğ’œ
 
Input: 
ğ’™
âˆˆ
â„
ğ‘›
, 
0
â¢
<
â¢
ğœ‚
, 
ğ‘‡
âˆˆ
â„•
for 
ğ‘¡
â†
1
 to 
ğ‘‡
 do
       
ğ’™
â†
ğ’™
âˆ’
ğœ‚
â¢
âˆ‡
ğ‘“
â¢
(
ğ’™
)
      
Output: 
ğ’™
Algorithm 2 Gradient descent

Note that other gradient-based optimization algorithms, e.g. Adam (Kingma & Ba, 2014), can similarly be extended to the multi-objective case.

In some settings, the exact computation of the update can be prohibitively slow or even intractable. When dealing with a single objective, stochastic gradient descent (SGD) replaces the gradient 
âˆ‡
ğ‘“
â¢
(
ğ’™
)
 with some estimation. More generally, stochastic Jacobian descent (SJD) relies on estimates of the aggregation of the Jacobian. One approach, that we call stochastically estimated Jacobian descent (SEJD), is to compute and aggregate an estimation of the Jacobian. Alternatively, when the number of objectives is very large, we propose to aggregate a matrix whose rows are a random subset of the rows of the true Jacobian. We call this approach stochastic sub-Jacobian descent (SSJD).

2.2Desirable properties for aggregators

An inherent challenge of multi-objective optimization is to manage conflicting objectives (Sener & Koltun, 2018; Yu et al., 2020; Liu et al., 2021a). Substituting the update 
ğ’š
=
âˆ’
ğœ‚
â¢
ğ’œ
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
)
 into the first-order Taylor approximation 
ğ’‡
â¢
(
ğ’™
)
+
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’š
 yields 
ğ’‡
â¢
(
ğ’™
)
âˆ’
ğœ‚
â¢
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’œ
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
)
. In particular, if 
ğŸ
â¢
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’œ
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
)
, then no coordinate of the approximation of 
ğ’‡
 will increase. A pair of vectors 
ğ’™
,
ğ’š
âˆˆ
â„
ğ‘›
 is said to conflict if 
ğ’™
âŠ¤
â¢
ğ’š
â¢
<
â¢
0
. Hence, for a sufficiently small 
ğœ‚
, if any row of 
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
 conflicts with 
ğ’œ
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
)
, the corresponding coordinate of 
ğ’‡
 will increase. When minimizing 
ğ’‡
, avoiding conflict between the aggregation and any gradient is thus desirable, motivating the first property.

Definition 1 (Non-conflicting).

Let 
ğ’œ
:
â„
ğ‘š
Ã—
ğ‘›
â†’
â„
ğ‘›
 be an aggregator. If for all 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, 
ğŸ
â¢
ğ½
â‹…
ğ’œ
â¢
(
ğ½
)
, then 
ğ’œ
 is said to be non-conflicting.

For any collection of vectors 
ğ¶
âŠ†
â„
ğ‘›
, the dual cone of 
ğ¶
 is 
{
ğ’™
âˆˆ
â„
ğ‘›
:
âˆ€
ğ’š
âˆˆ
ğ¶
,
0
â¢
ğ’™
âŠ¤
â¢
ğ’š
}
 (Boyd & Vandenberghe, 2004). Notice that an aggregator 
ğ’œ
 is non-conflicting if and only if for any 
ğ½
, 
ğ’œ
â¢
(
ğ½
)
 is in the dual cone of the rows of 
ğ½
.

In a step of GD, the update scales proportionally to the gradient norm. Small gradients thus lead to small updates, and conversely, large gradients lead to large updates. To maintain coherence with GD, it would be natural that the rows of the Jacobian also contribute to the aggregation proportionally to their norm. Scaling each row of 
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
 by the corresponding element of some vector 
ğ’„
âˆˆ
â„
ğ‘š
 yields 
diag
(
ğ’„
)
â‹…
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
. This insight can then be formalized as the following property.

Definition 2 (Linear under scaling).

Let 
ğ’œ
:
â„
ğ‘š
Ã—
ğ‘›
â†’
â„
ğ‘›
 be an aggregator. If for all 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, the mapping from any 
ğŸ
â¢
ğ’„
âˆˆ
â„
ğ‘š
 to 
ğ’œ
â¢
(
diag
(
ğ’„
)
â‹…
ğ½
)
 is linear in 
ğ’„
, then 
ğ’œ
 is said to be linear under scaling.

Finally, as 
â€–
ğ’š
â€–
 decreases asymptotically to 
0
, the precision of the first-order Taylor approximation 
ğ’‡
â¢
(
ğ’™
)
+
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’š
 improves, as highlighted in (2). The projection 
ğ’š
â€²
 of any candidate update 
ğ’š
 onto the span of the rows of 
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
 satisfies 
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’š
â€²
=
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’š
 and 
â€–
ğ’š
â€²
â€–
â¢
â€–
ğ’š
â€–
, so this projection decreases the norm of the update while preserving the value of the approximation. Without additional information about 
ğ’‡
, it is thus reasonable to select 
ğ’š
 directly in the row span of 
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
, i.e. to have a vector of weights 
ğ’˜
âˆˆ
â„
ğ‘š
 satisfying 
ğ’š
=
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
âŠ¤
â‹…
ğ’˜
. This yields the last desirable property.

Definition 3 (Weighted).

Let 
ğ’œ
:
â„
ğ‘š
Ã—
ğ‘›
â†’
â„
ğ‘›
 be an aggregator. If for all 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, there exists 
ğ’˜
âˆˆ
â„
ğ‘š
 satisfying 
ğ’œ
â¢
(
ğ½
)
=
ğ½
âŠ¤
â‹…
ğ’˜
, then 
ğ’œ
 is said to be weighted.

2.3Unconflicting projection of gradients

We now define the unconflicting projection of gradients aggregator 
ğ’œ
UPGrad
, specifically designed to be non-conflicting, linear under scaling, and weighted. In essence, it projects each gradient onto the dual cone of the rows of the Jacobian and averages the results, as illustrated in Figure 1(a).

(a)
ğ’œ
UPGrad
â¢
(
ğ½
)
 (ours)
(b)
ğ’œ
Mean
â¢
(
ğ½
)
, 
ğ’œ
MGDA
â¢
(
ğ½
)
 and 
ğ’œ
DualProj
â¢
(
ğ½
)
Figure 1:Aggregation of 
ğ½
=
[
ğ’ˆ
1
â¢
ğ’ˆ
2
]
âŠ¤
âˆˆ
â„
2
Ã—
2
 by four different aggregators. The dual cone of 
{
ğ’ˆ
1
,
ğ’ˆ
2
}
 is represented in green.
(a) 
ğ’œ
UPGrad
 projects 
ğ’ˆ
1
 and 
ğ’ˆ
2
 onto the dual cone and averages the results.
(b) The mean 
ğ’œ
Mean
â¢
(
ğ½
)
=
1
2
â¢
(
ğ’ˆ
1
+
ğ’ˆ
2
)
 conflicts with 
ğ’ˆ
1
. 
ğ’œ
DualProj
 projects this mean onto the dual cone, so it lies on its boundary. 
ğ’œ
MGDA
â¢
(
ğ½
)
 is almost orthogonal to 
ğ’ˆ
2
 because of its larger norm.

For any 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
 and 
ğ’™
âˆˆ
â„
ğ‘›
, the projection of 
ğ’™
 onto the dual cone of the rows of 
ğ½
 is

	
ğœ‹
ğ½
â¢
(
ğ’™
)
=
arg
â¢
min
ğ’š
âˆˆ
â„
ğ‘›
:
ğŸ
â¢
ğ½
â¢
ğ’š
â¡
â€–
ğ’š
âˆ’
ğ’™
â€–
2
.
		
(3)

Denoting by 
ğ’†
ğ‘–
âˆˆ
â„
ğ‘š
 the 
ğ‘–
th standard basis vector, 
ğ½
âŠ¤
â¢
ğ’†
ğ‘–
 is the 
ğ‘–
th row of 
ğ½
. 
ğ’œ
UPGrad
 is defined as

	
ğ’œ
UPGrad
â¢
(
ğ½
)
=
1
ğ‘š
â¢
âˆ‘
ğ‘–
âˆˆ
[
ğ‘š
]
ğœ‹
ğ½
â¢
(
ğ½
âŠ¤
â¢
ğ’†
ğ‘–
)
.
		
(4)

Since the dual cone is convex, it is closed under positive combinations of its elements. For any 
ğ½
, 
ğ’œ
UPGrad
â¢
(
ğ½
)
 is thus always in the dual cone of the rows of 
ğ½
, so 
ğ’œ
UPGrad
 is non-conflicting. Note that if no pair of gradients conflicts, 
ğ’œ
UPGrad
 simply averages the rows of the Jacobian.

Since 
ğœ‹
ğ½
 is a projection onto a closed convex cone, if 
ğ’™
âˆˆ
â„
ğ‘›
 and 
0
â¢
<
â¢
ğ‘
âˆˆ
â„
, then 
ğœ‹
ğ½
â¢
(
ğ‘
â‹…
ğ’™
)
=
ğ‘
â‹…
ğœ‹
ğ½
â¢
(
ğ’™
)
. By (4), 
ğ’œ
UPGrad
 is thus linear under scaling.

When 
ğ‘›
 is large, the projection in (3) is prohibitively expensive to compute. An alternative but equivalent approach is to use its dual formulation, which is independent of 
ğ‘›
.

Proposition 1.

Let 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
. For any 
ğ’–
âˆˆ
â„
ğ‘š
, 
ğœ‹
ğ½
â¢
(
ğ½
âŠ¤
â¢
ğ’–
)
=
ğ½
âŠ¤
â¢
ğ’˜
 with

	
ğ’˜
âˆˆ
arg
â¢
min
ğ’—
âˆˆ
â„
ğ‘š
:
ğ’–
â¢
ğ’—
â¡
ğ’—
âŠ¤
â¢
ğ½
â¢
ğ½
âŠ¤
â¢
ğ’—
.
		
(5)
Proof.

See Appendix B.2. âˆ

The problem defined in (5) can be solved efficiently using a quadratic programming solver, such as those bundled in qpsolvers (Caron et al., 2024). For any 
ğ‘–
âˆˆ
[
ğ‘š
]
, let 
ğ’˜
ğ‘–
 be given by (5) when substituting 
ğ’–
 with 
ğ’†
ğ‘–
. Then, by Proposition 1,

	
ğ’œ
UPGrad
â¢
(
ğ½
)
=
ğ½
âŠ¤
â¢
(
1
ğ‘š
â¢
âˆ‘
ğ‘–
âˆˆ
[
ğ‘š
]
ğ’˜
ğ‘–
)
.
		
(6)

This provides an efficient implementation of 
ğ’œ
UPGrad
 and proves that it is weighted. 
ğ’œ
UPGrad
 can also be easily extended to incorporate a vector of preferences by replacing the average in (4) and (6) by a weighted sum with positive weights. This extension remains non-conflicting, linear under scaling, and weighted.

2.4Convergence to the Pareto front

We now provide theoretical convergence guarantees of JD with 
ğ’œ
UPGrad
 when minimizing some 
ğ’‡
:
â„
ğ‘›
â†’
â„
ğ‘š
 satisfying standard assumptions. If for a given 
ğ’™
âˆˆ
â„
ğ‘›
, there exists no 
ğ’š
âˆˆ
â„
ğ‘›
 satisfying 
ğ’‡
â¢
(
ğ’š
)
â¢
ğ’‡
â¢
(
ğ’™
)
, then 
ğ’™
 is said to be Pareto optimal. The set 
ğ‘‹
âˆ—
âŠ†
â„
ğ‘›
 of Pareto optimal points is called the Pareto set, and its image 
ğ’‡
â¢
(
ğ‘‹
âˆ—
)
 is called the Pareto front.

Whenever 
ğ’‡
â¢
(
ğœ†
â¢
ğ’™
+
(
1
âˆ’
ğœ†
)
â¢
ğ’š
)
â¢
ğœ†
â¢
ğ’‡
â¢
(
ğ’™
)
+
(
1
âˆ’
ğœ†
)
â¢
ğ’‡
â¢
(
ğ’š
)
 holds for any pair of vectors 
ğ’™
,
ğ’š
âˆˆ
â„
ğ‘›
 and any 
ğœ†
âˆˆ
[
0
,
1
]
, 
ğ’‡
 is said to be -convex. Moreover, 
ğ’‡
 is said to be 
ğ›½
-smooth whenever 
â€–
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
âˆ’
ğ’¥
â¢
ğ’‡
â¢
(
ğ’š
)
â€–
F
â¢
ğ›½
â¢
â€–
ğ’™
âˆ’
ğ’š
â€–
 holds for any pair of vectors 
ğ’™
,
ğ’š
âˆˆ
â„
ğ‘›
.

Theorem 1.

Let 
ğ’‡
:
â„
ğ‘›
â†’
â„
ğ‘š
 be a 
ğ›½
-smooth and -convex function. Suppose that the Pareto front 
ğ’‡
â¢
(
ğ‘‹
âˆ—
)
 is bounded and that for any 
ğ’™
âˆˆ
â„
ğ‘›
, there is 
ğ’™
âˆ—
âˆˆ
ğ‘‹
âˆ—
 satisfying 
ğ’‡
â¢
(
ğ’™
âˆ—
)
â¢
ğ’‡
â¢
(
ğ’™
)
.2 Let 
ğ’™
1
âˆˆ
â„
ğ‘›
, and for all 
ğ‘¡
â¢
1
, 
ğ’™
ğ‘¡
+
1
=
ğ’™
ğ‘¡
âˆ’
ğœ‚
â¢
ğ’œ
UPGrad
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
, with 
ğœ‚
=
1
ğ›½
â¢
ğ‘š
. Let 
ğ’˜
ğ‘¡
 be the weights defining 
ğ’œ
UPGrad
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
 as per (6), i.e. 
ğ’œ
UPGrad
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
=
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
âŠ¤
â‹…
ğ’˜
ğ‘¡
. If 
ğ’˜
ğ‘¡
 is bounded, then 
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
 converges to 
ğ’‡
â¢
(
ğ’™
âˆ—
)
 for some 
ğ’™
âˆ—
âˆˆ
ğ‘‹
âˆ—
. In other words, 
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
 converges to the Pareto front.

Proof.

See Appendix B.3. âˆ

Empirically, 
ğ’˜
ğ‘¡
 appears to converge to some 
ğ’˜
âˆ—
âˆˆ
â„
ğ‘š
 satisfying both 
ğŸ
â¢
ğ’˜
âˆ—
 and 
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
âˆ—
)
âŠ¤
â¢
ğ’˜
âˆ—
=
ğŸ
. This suggests that the boundedness of 
ğ’˜
ğ‘¡
 could be relaxed or even removed from the set of assumptions of Theorem 1.

Another common theoretical result for multi-objective optimization is convergence to a weakly stationary point. If for a given 
ğ’™
âˆˆ
â„
ğ‘›
, there exists 
ğŸ
â¢
ğ’˜
 satisfying 
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
âŠ¤
â¢
ğ’˜
=
0
 then 
ğ’™
 is said to be weakly Pareto stationary. Even though every Pareto optimal point is weakly Pareto stationary, the converse does not hold, even in the convex case. Despite being necessary, convergence to a weakly Pareto stationary point is thus not a sufficient condition for optimality and, hence, constitutes a rather weak guarantee. To the best of our knowledge, 
ğ’œ
UPGrad
 is the first non-conflicting aggregator that provably converges to the Pareto front in the smooth convex case.

Appendix B.4 provides additional results and discussions on the convergence rate and about guarantees in the non-convex setting.

3Applications
Instance-wise risk minimization.

In machine learning, we generally have access to a training set consisting of 
ğ‘š
 examples. The goal of empirical risk minimization (ERM) (Vapnik, 1995) is simply to minimize the average loss over the whole training set. More generally, instance-wise risk minimization (IWRM) considers the loss associated with each training example as a distinct objective. Formally, if 
ğ’™
âˆˆ
â„
ğ‘›
 are the parameters of the model and 
ğ‘“
ğ‘–
â¢
(
ğ’™
)
 is the loss associated to the 
ğ‘–
th example, the respective objective functions of ERM and IWRM are:

	(Empirical risk)	
ğ‘“
Â¯
â¢
(
ğ’™
)
	
=
1
ğ‘š
â¢
âˆ‘
ğ‘–
âˆˆ
[
ğ‘š
]
ğ‘“
ğ‘–
â¢
(
ğ’™
)
		
(7)

	(Instance-wise risk)	
ğ’‡
â¢
(
ğ’™
)
	
=
[
ğ‘“
1
â¢
(
ğ’™
)
	
ğ‘“
2
â¢
(
ğ’™
)
	
â‹¯
	
ğ‘“
ğ‘š
â¢
(
ğ’™
)
]
âŠ¤
		
(8)

Naively using GD for ERM is inefficient in most practical cases, so a prevalent alternative is to use SGD or one of its variants. Similarly, using JD for IWRM is typically intractable. Indeed, it would require computing a Jacobian matrix with one row per training example at each iteration. In contrast, we can use the Jacobian of a random batch of training example losses. Since it consists of a subset of the rows of the full Jacobian, this approach is a form of stochastic sub-Jacobian descent, as introduced in Section 2.1. IWRM can also be extended to cases where each 
ğ‘“
ğ‘–
 is a vector-valued function. The objective would then be the concatenation of the losses of all examples.

Multi-task learning.

In multi-task learning, a single model is trained to perform several related tasks simultaneously, leveraging shared representations to improve overall performance (Ruder, 2017). At its core, multi-task learning is a multi-objective optimization problem (Sener & Koltun, 2018), making it a straightforward application for Jacobian descent. Yet, the conflict between tasks is often too limited to justify the overhead of computing all task-specific gradients, i.e. the whole Jacobian (Kurin et al., 2022; Xin et al., 2022). In such cases, a practical approach is to minimize some linear scalarization of the objectives using an SGD-based method. Nevertheless, we believe that a setting with inherent conflict between tasks naturally prescribes Jacobian descent with a non-conflicting aggregator. We analyze several related works applied to multi-task learning in Section 4.

Adversarial training.

In adversarial domain adaptation, the feature extractor of a model is trained with two conflicting objectives: The features should be helpful for the main task and should be unable to discriminate the domain of the input (Ganin et al., 2016). Likewise, in adversarial fairness, the feature extractor is trained to both minimize the predictability of sensitive attributes, such as race or gender, and maximize the performance on the main task (Adel et al., 2019). Combining the corresponding gradients with a non-conflicting aggregator could enhance the optimization of such methods. We believe that the training of generative adversarial networks (Goodfellow et al., 2014) could be similarly formulated as a multi-objective optimization problem. The generator and discriminator could then be jointly optimized with JD.

Momentum-based optimization.

In gradient-based single-objective optimization, several methods use some form of gradient momentum to improve their convergence speed (Polyak, 1964). Essentially, their updates consider an exponential moving average of past gradients rather than just the last one. An appealing idea is to modify those algorithms to make them combine the gradient and the momentum with some aggregator, such as 
ğ’œ
UPGrad
, instead of summing them. This would apply to many popular optimizers, like SGD with Nesterov momentum (Nesterov, 1983), Adam (Kingma & Ba, 2014), AdamW (Loshchilov & Hutter, 2019) and NAdam (Dozat, 2016).

Distributed optimization.

In a distributed data-parallel setting with multiple machines or multiple GPUs, model updates are computed in parallel. This can be viewed as multi-objective optimization with one objective per data share. Rather than the typical averaging, a specialized aggregator, such as 
ğ’œ
UPGrad
, could thus combine the model updates. This consideration can even be extended to federated learning, in which multiple entities participate in the training of a common model from their own private data by sharing model updates (Kairouz et al., 2021). In this setting, as security is one of the main challenges, the non-conflicting property of the aggregator could be key.

4Existing aggregators

In the context of multi-task learning, several works have proposed iterative optimization algorithms based on the combination of task-specific gradients (Sener & Koltun, 2018; Yu et al., 2020; Liu et al., 2021b, a; Lin et al., 2021; Navon et al., 2022; Senushkin et al., 2023). These methods can be formulated as variants of JD parameterized by different aggregators. More specifically, since the gradients are stochastically estimated from batches of data, these are cases of what we call SEJD. In the following, we briefly present the most prominent aggregators and summarize their properties in Table 1. As a baseline, we also consider 
ğ’œ
Mean
, which simply averages the rows of the Jacobian. Their formal definitions are provided in Appendix C. Some of them are also illustrated in Figure 1(b).

ğ’œ
RGW
 aggregates the matrix using a random vector of weights (Lin et al., 2021). 
ğ’œ
MGDA
 gives the aggregation that maximizes the smallest improvement (Fliege & Svaiter, 2000; DÃ©sidÃ©ri, 2012; Sener & Koltun, 2018). 
ğ’œ
CAGrad
 maximizes the smallest improvement in a ball around the average gradient whose radius is parameterized by 
ğ‘
âˆˆ
[
0
,
1
[
 (Liu et al., 2021a). 
ğ’œ
PCGrad
 projects each gradient onto the orthogonal hyperplane of other gradients in case of conflict, iteratively and in a random order (Yu et al., 2020). It is, however, only non-conflicting when 
ğ‘š
â¢
2
, in which case 
ğ’œ
PCGrad
=
ğ‘š
â‹…
ğ’œ
UPGrad
. IMTL-G is a method to balance some gradients with impartiality (Liu et al., 2021b). It is only defined for linearly independent gradients, but we generalize it as a formal aggregator, denoted 
ğ’œ
IMTL-G
, in Appendix C.6. Aligned-MTL orthonormalizes the Jacobian and weights its rows according to some preferences (Senushkin et al., 2023). We denote by 
ğ’œ
Aligned-MTL
 this method with uniform preferences. 
ğ’œ
Nash-MTL
 aggregates Jacobians by finding the Nash equilibrium between task-specific gradients (Navon et al., 2022). Lastly, the GradDrop layer (Chen et al., 2020) defines a custom backward pass that combines gradients with respect to some internal activation. The corresponding aggregator, denoted 
ğ’œ
GradDrop
, randomly drops out some gradient coordinates based on their sign and sums the remaining ones.

In the context of continual learning, to limit forgetting, an idea is to project the gradient onto the dual cone of gradients computed with past examples (Lopez-Paz & Ranzato, 2017). This idea can be translated into an aggregator that projects the mean gradient onto the dual cone of the rows of the Jacobian. We name this 
ğ’œ
DualProj
.

Several other works consider the gradients to be noisy when making their theoretical analysis (Zhou et al., 2022; Fernando et al., 2023; Chen et al., 2023; Xiao et al., 2023; Liu & Vicente, 2024). Their solutions for combining gradients are typically stateful. Although this could enhance practical convergence rates, we have restricted our focus to the analysis of stateless aggregators. Exploring and analyzing a generalized Jacobian descent algorithm, that would preserve some state over the iterations, is a promising future direction.

In the federated learning setting, several aggregators have been proposed to combine the model updates while being robust to adversaries (Blanchard et al., 2017; Chen et al., 2017; Guerraoui et al., 2018; Yin et al., 2018). We do not study them here as they mainly focus on security aspects.

Table 1:Properties satisfied for any number of objectives. Proofs are provided in Appendix C.
Ref.	Aggregator	Non-
conflicting	Linear under
scaling	Weighted
â€”	
ğ’œ
Mean
	âœ—	âœ“	âœ“
DÃ©sidÃ©ri (2012)	
ğ’œ
MGDA
	âœ“	âœ—	âœ“
Lopez-Paz & Ranzato (2017)	
ğ’œ
DualProj
	âœ“	âœ—	âœ“
Yu et al. (2020)	
ğ’œ
PCGrad
	âœ—	âœ“	âœ“
Chen et al. (2020)	
ğ’œ
GradDrop
	âœ—	âœ—	âœ—
Liu et al. (2021b)	
ğ’œ
IMTL-G
	âœ—	âœ—	âœ“
Liu et al. (2021a)	
ğ’œ
CAGrad
	âœ—	âœ—	âœ“
Lin et al. (2021)	
ğ’œ
RGW
	âœ—	âœ“	âœ“
Navon et al. (2022)	
ğ’œ
Nash-MTL
	âœ“	âœ—	âœ“
Senushkin et al. (2023)	
ğ’œ
Aligned-MTL
	âœ—	âœ—	âœ“
(ours)	
ğ’œ
UPGrad
	âœ“	âœ“	âœ“
5Experiments

Appendix A.1 shows the optimization trajectories of the methods from Table 1 when optimizing two convex quadratic forms that illustrate the setting of Figure 1 and the discrepancy between weak Pareto stationarity and Pareto optimality.

In the following, we present empirical results for instance-wise risk minimization on some simple image classification datasets. IWRM is performed by stochastic sub-Jacobian descent, as described in Section 3. A key consideration is that when the aggregator is 
ğ’œ
Mean
, this approach becomes equivalent to empirical risk minimization with SGD. It is thus used as a baseline for comparison.

We train convolutional neural networks on subsets of SVHN (Netzer et al., 2011), CIFAR-10 (Krizhevsky et al., 2009), EuroSAT (Helber et al., 2019), MNIST (LeCun et al., 1998), Fashion-MNIST (Xiao et al., 2017) and Kuzushiji-MNIST (Clanuwat et al., 2018). To make the comparisons as fair as possible, we have tuned the learning rate very precisely for each aggregator, as explained in detail in Appendix D.1. We have also run the same experiments several times independently to gain confidence in our results. Since this leads to a total of 
43776
 training runs across all of our experiments, we have limited the size of each training dataset to 1024 images, greatly reducing computational costs. Note that this is strictly an optimization problem: we are not studying the generalization of the model, which would be captured by some performance metric on a test set. Other experimental settings, such as the network architectures and the total computational budget used to run our experiments, are given in Appendix D. Figure 2 reports the main results on SVHN and CIFAR-10, two of the datasets exhibiting the most substantial performance gap. Results on the other datasets and aggregators are reported in Appendix A.2. They also demonstrate a significant performance gap.

(a)SVHN: training loss
(b)SVHN: update similarity to the SGD update
(c)CIFAR-10: training loss
(d)CIFAR-10: update similarity to the SGD update
Figure 2:Optimization metrics obtained with IWRM with 1024 training examples and a batch size of 32, averaged over 8 independent runs. The shaded area around each curve shows the estimated standard error of the mean over the 8 runs. Curves are smoothed for readability. Best viewed in color.

Here, we compare the aggregators in terms of their average loss over the training set: the goal of ERM. For this reason, it is rather surprising that 
ğ’œ
Mean
, which directly optimizes this objective, exhibits a slower convergence rate than some other aggregators. In particular, 
ğ’œ
UPGrad
, and to a lesser extent 
ğ’œ
DualProj
, provide improvements on all datasets.

Figures 2(b) and 2(d) show the similarity between the update of each aggregator and the update given by 
ğ’œ
Mean
. For 
ğ’œ
UPGrad
, a low similarity indicates that there are some conflicting gradients with imbalanced norms (a setting illustrated in Figure 1). Our interpretation is thus that 
ğ’œ
UPGrad
 prevents gradients of hard examples from being dominated by those of easier examples early into the training. Since fitting those is more complex and time-consuming, it is beneficial to consider them earlier. We believe the similarity increases later on because the gradients become more balanced. This further suggests a greater stability of 
ğ’œ
UPGrad
 compared to 
ğ’œ
Mean
, which may allow it to perform effectively at a higher learning rate and, consequently, accelerate its convergence.

The sub-optimal performance of 
ğ’œ
MGDA
 in this setting can be attributed to its sensitivity to small gradients. If any row of the Jacobian approaches zero, the aggregation by 
ğ’œ
MGDA
 will also approach zero. This observation illustrates the discrepancy between weak stationarity and optimality, as discussed in Section 2.4. A notable advantage of linearity under scaling is to explicitly prevent this from happening.

Overall, these experiments demonstrate a high potential for the IWRM paradigm and confirm the relevance of JD, and more specifically of SSJD, as multi-objective optimization algorithms. Besides, the superiority of 
ğ’œ
UPGrad
 in such a simple setting supports our theoretical results.

While increasing the batch size in SGD reduces variance, the effect of doing so in SSJD combined with 
ğ’œ
UPGrad
 is non-trivial, as it also tightens the dual cone. Additional results obtained when varying the batch size or updating the parameters with the Adam optimizer are available in Appendices A.3 and A.4, respectively.

While an iteration of SSJD is more expensive than an iteration of SGD, its runtime is influenced by several factors, including the choice of aggregator, the parallelization capabilities of the hardware used for Jacobian computation, and the implementation. Appendix E provides memory usage and computation time considerations for our methods. Additionally, we propose a path towards a more efficient implementation in the next section.

6Gramian-based Jacobian descent

When the number of objectives is dominated by the number of parameters of the model, the main overhead of JD comes from the usage of a Jacobian matrix rather than a single gradient. In the following, we motivate an alternative implementation of JD that only uses the inner products between each pair of gradients.

For any 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, the matrix 
ğº
=
ğ½
â¢
ğ½
âŠ¤
 is called the Gramian of 
ğ½
 and is positive semi-definite. Let 
â„³
ğ‘š
âŠ†
â„
ğ‘š
Ã—
ğ‘š
 be the set of positive semi-definite matrices. The Gramian of the Jacobian, denoted 
ğ’¢
â¢
ğ’‡
â¢
(
ğ’™
)
=
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
âŠ¤
âˆˆ
â„³
ğ‘š
, captures the relations â€“ including conflicts â€“ between all pairs of gradients. Whenever 
ğ’œ
 is a weighted aggregator, the update of JD is 
ğ’š
=
âˆ’
ğœ‚
â¢
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
âŠ¤
â¢
ğ’˜
 for some vector of weights 
ğ’˜
âˆˆ
â„
ğ‘š
. Substituting this into the Taylor approximation of (2) gives

	
ğ’‡
â¢
(
ğ’™
+
ğ’š
)
=
ğ’‡
â¢
(
ğ’™
)
âˆ’
ğœ‚
â¢
ğ’¢
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’˜
+
ğ‘œ
â¢
(
ğœ‚
â¢
ğ’˜
âŠ¤
â‹…
ğ’¢
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’˜
)
.
		
(9)

This expression only depends on the Jacobian through its Gramian. It is thus sensible to focus on aggregators whose weights are only a function of the Gramian. Denoting this function as 
ğ’²
:
â„³
ğ‘š
â†’
â„
ğ‘š
, those aggregators satisfy 
ğ’œ
â¢
(
ğ½
)
=
ğ½
âŠ¤
â‹…
ğ’²
â¢
(
ğº
)
. Remarkably, all weighted aggregators of Table 1 can be expressed in this form. In the case of 
ğ’œ
UPGrad
, this is clearly demonstrated in Proposition 1, which shows that the weights depend on 
ğº
. For such aggregators, substitution and linearity of differentiation3 then yield

	
ğ’œ
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
)
=
âˆ‡
(
ğ’²
â¢
(
ğ’¢
â¢
ğ’‡
â¢
(
ğ’™
)
)
âŠ¤
â‹…
ğ’‡
)
â¡
(
ğ’™
)
.
		
(10)

After computing 
ğ’²
â¢
(
ğ’¢
â¢
ğ’‡
â¢
(
ğ’™
)
)
, a step of JD would thus only require the backpropagation of a scalar function. The computational cost of applying 
ğ’²
 depends on the aggregator and is often dominated by the cost of computing the Gramian.

We now outline a method to compute the Gramian of the Jacobian without ever having to store the full Jacobian in memory. Similarly to the backpropagation algorithm, we can leverage the chain rule. Let 
ğ’ˆ
:
â„
ğ‘›
â†’
â„
ğ‘˜
 and 
ğ’‡
:
â„
ğ‘˜
â†’
â„
ğ‘š
, then for any 
ğ’™
âˆˆ
â„
ğ‘›
, the chain rule for Gramians is

	
ğ’¢
â¢
(
ğ’‡
âˆ˜
ğ’ˆ
)
â¢
(
ğ’™
)
=
ğ’¥
â¢
ğ’‡
â¢
(
ğ’ˆ
â¢
(
ğ’™
)
)
â‹…
ğ’¢
â¢
ğ’ˆ
â¢
(
ğ’™
)
â‹…
ğ’¥
â¢
ğ’‡
â¢
(
ğ’ˆ
â¢
(
ğ’™
)
)
âŠ¤
.
		
(11)

Moreover, when the function has multiple inputs, the Gramian can be computed as a sum of individual Gramians. Let 
ğ’‡
:
â„
ğ‘›
1
+
â‹¯
+
ğ‘›
ğ‘˜
â†’
â„
ğ‘š
 and 
ğ’™
=
[
ğ’™
1
âŠ¤
	
â‹¯
	
ğ’™
ğ‘˜
âŠ¤
]
âŠ¤
. We can write 
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
 as the concatenation of Jacobians 
[
ğ’¥
ğ’™
1
â¢
ğ’‡
â¢
(
ğ’™
)
	
â‹¯
	
ğ’¥
ğ’™
ğ‘˜
â¢
ğ’‡
â¢
(
ğ’™
)
]
, where 
ğ’¥
ğ’™
ğ‘–
â¢
ğ’‡
â¢
(
ğ’™
)
 is the Jacobian of 
ğ’‡
 with respect to 
ğ’™
ğ‘–
 evaluated at 
ğ’™
. For any 
ğ‘–
âˆˆ
[
ğ‘˜
]
, let 
ğ’¢
ğ’™
ğ‘–
â¢
ğ’‡
â¢
(
ğ’™
)
=
ğ’¥
ğ’™
ğ‘–
â¢
ğ’‡
â¢
(
ğ’™
)
â‹…
ğ’¥
ğ’™
ğ‘–
â¢
ğ’‡
â¢
(
ğ’™
)
âŠ¤
. Then

	
ğ’¢
â¢
ğ’‡
â¢
(
ğ’™
1
,
â€¦
,
ğ’™
ğ‘˜
)
=
âˆ‘
ğ‘–
âˆˆ
[
ğ‘˜
]
ğ’¢
ğ’™
ğ‘–
â¢
ğ’‡
â¢
(
ğ’™
1
,
â€¦
,
ğ’™
ğ‘˜
)
.
		
(12)

When a function is made of compositions and concatenations of elementary functions, the Gramian of the Jacobian can thus be expressed with sums and products of partial Jacobians.

We now provide an example algorithm to compute the Gramian of a sequence of layers. For 
0
â¢
ğ‘–
â¢
<
â¢
ğ‘˜
, let 
ğ’‡
ğ‘–
:
â„
ğ‘›
ğ‘–
Ã—
â„
â„“
ğ‘–
â†’
â„
ğ‘›
ğ‘–
+
1
 be a layer parameterized by 
ğ’‘
ğ‘–
âˆˆ
â„
â„“
ğ‘–
. Given 
ğ’™
0
âˆˆ
â„
ğ‘›
0
, for 
0
â¢
ğ‘–
â¢
<
â¢
ğ‘˜
, the activations are recursively defined as 
ğ’™
ğ‘–
+
1
=
ğ’‡
ğ‘–
â¢
(
ğ’™
ğ‘–
,
ğ’‘
ğ‘–
)
. Algorithm 3 illustrates how (11) and (12) can be combined to compute the Gramian of the network with respect to its parameters.

ğ½
ğ‘¥
â†
ğ¼
    #â€„Identity matrix of size 
ğ‘›
ğ‘˜
Ã—
ğ‘›
ğ‘˜
ğº
â†
0
 â€†   #â€„Zero matrix of size 
ğ‘›
ğ‘˜
Ã—
ğ‘›
ğ‘˜
for 
ğ‘–
â†
ğ‘˜
âˆ’
1
 to 
0
 do
       
ğ½
ğ‘
â†
ğ’¥
ğ’‘
ğ‘–
â¢
ğ’‡
ğ‘–
â¢
(
ğ’™
ğ‘–
,
ğ’‘
ğ‘–
)
â‹…
ğ½
ğ‘¥
â€†   #â€„Jacobian of 
ğ’™
ğ‘˜
 w.r.t. 
ğ’‘
ğ‘–
       
ğ½
ğ‘¥
â†
ğ’¥
ğ’™
ğ‘–
â¢
ğ’‡
ğ‘–
â¢
(
ğ’™
ğ‘–
,
ğ’‘
ğ‘–
)
â‹…
ğ½
ğ‘¥
   #â€„Jacobian of 
ğ’™
ğ‘˜
 w.r.t. 
ğ’™
ğ‘–
       
ğº
â†
ğº
+
ğ½
ğ‘
â¢
ğ½
ğ‘
âŠ¤
Output: 
ğº
Algorithm 3 Gramian reverse accumulation for a sequence of layers

Generalizing Algorithm 3 to any computational graph and implementing it efficiently remains an open challenge extending beyond the scope of this work.

7Conclusion

In this paper, we introduced Jacobian descent (JD), a multi-objective optimization algorithm defined by some aggregator that maps the Jacobian to an update direction. We identified desirable properties for aggregators and proposed 
ğ’œ
UPGrad
, addressing the limitations of existing methods while providing stronger convergence guarantees. We also highlighted potential applications of JD and proposed IWRM, a novel learning paradigm considering the loss of each training example as a distinct objective. Given its promising empirical results, we believe this paradigm deserves further attention. Additionally, we see potential for 
ğ’œ
UPGrad
 beyond JD, as a linear algebra tool for combining conflicting vectors in broader contexts. As speed is the primary limitation of JD, we have outlined an algorithm for efficiently computing the Gramian of the Jacobian, which could unlock JDâ€™s full potential. We hope this work serves as a foundation for future research in multi-objective optimization and encourages a broader adoption of these methods.

Limitations and future directions.

Our empirical experiments on deep learning problems have some limitations. First, we only evaluate JD on IWRM, a setting with moderately conflicting objectives. It would be essential to develop proper benchmarks to compare aggregators on a wide variety of problems. Ideally, such problems should involve substantially conflicting objectives, e.g. multi-task learning with inherently competing or even adversarial tasks. Then, we have limited our scope to the comparison of optimization speeds, disregarding generalization. While this simplifies the experiments and makes the comparison rigorous, optimization and generalization are sometimes intertwined. We thus believe that future works should focus on both aspects.

Acknowledgments

We would like to express our sincere thanks to Scott Pesme, Emre Telatar, Matthieu Buot de lâ€™Ã‰pine, Adrien Vandenbroucque, Ye Zhu, Alix Jeannerot, and Damian Dudzicz for their careful and thorough review. The many insightful discussions that we shared with them were essential to this project.

References
Adel et al. (2019)
â†‘
	Tameem Adel, Isabel Valera, Zoubin Ghahramani, and Adrian Weller.One-network adversarial fairness.In AAAI Conference on Artificial Intelligence, volume 33, pp.  2412â€“2420, 2019.
Blanchard et al. (2017)
â†‘
	Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.Machine learning with adversaries: Byzantine tolerant gradient descent.In Advances in Neural Information Processing Systems, volume 30, 2017.
Boyd & Vandenberghe (2004)
â†‘
	Stephen P Boyd and Lieven Vandenberghe.Convex Optimization.Cambridge University Press, 2004.
Branke (2008)
â†‘
	JÃ¼rgen Branke.Multiobjective Optimization: Interactive and Evolutionary Approaches.Springer Science & Business Media, 2008.
Caron et al. (2024)
â†‘
	StÃ©phane Caron, Daniel ArnstrÃ¶m, Suraj Bonagiri, Antoine Dechaume, Nikolai Flowers, Adam Heins, Takuma Ishikawa, Dustin Kenefake, Giacomo Mazzamuto, Donato Meoli, Brendan Oâ€™Donoghue, Adam A. Oppenheimer, Abhishek Pandala, Juan JosÃ© Quiroz OmaÃ±a, Nikitas Rontsis, Paarth Shah, Samuel St-Jean, Nicola Vitucci, Soeren Wolfers, Fengyu Yang, @bdelhaisse, @MeindertHH, @rimaddo, @urob, and @shaoanlu.qpsolvers: Quadratic Programming Solvers in Python, 2024.URL https://github.com/qpsolvers/qpsolvers.
Chen et al. (2023)
â†‘
	Lisha Chen, Heshan Fernando, Yiming Ying, and Tianyi Chen.Three-way trade-off in multi-objective learning: Optimization, generalization and conflict-avoidance.In Advances in Neural Information Processing Systems, volume 36, pp.  70045â€“70093, 2023.
Chen et al. (2017)
â†‘
	Yudong Chen, Lili Su, and Jiaming Xu.Distributed statistical machine learning in adversarial settings: Byzantine gradient descent.Proceedings of the ACM on Measurement and Analysis of Computing Systems, 1(2):1â€“25, 2017.
Chen et al. (2020)
â†‘
	Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov.Just pick a sign: Optimizing deep multitask models with gradient sign dropout.In Advances in Neural Information Processing Systems, volume 33, pp.  2039â€“2050, 2020.
Clanuwat et al. (2018)
â†‘
	Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha.Deep learning for classical Japanese literature.In NeurIPS Workshop on Machine Learning for Creativity and Design, 2018.
Clevert et al. (2015)
â†‘
	Djork-ArnÃ© Clevert, Thomas Unterthiner, and Sepp Hochreiter.Fast and accurate deep network learning by exponential linear units (ELUs).arXiv preprint arXiv:1511.07289, 2015.
Deb et al. (2016)
â†‘
	Kalyanmoy Deb, Karthik Sindhya, and Jussi Hakanen.Multi-objective optimization.In Decision Sciences, pp.  161â€“200. CRC Press, 2016.
Dozat (2016)
â†‘
	Timothy Dozat.Incorporating Nesterov momentum into Adam.In International Conference on Learning Representations Workshop, 2016.
DÃ©sidÃ©ri (2012)
â†‘
	Jean-Antoine DÃ©sidÃ©ri.Multiple-gradient descent algorithm (MGDA) for multiobjective optimization.Comptes Rendus Mathematique, 350(5-6):313â€“318, 2012.
Ehrgott (2005)
â†‘
	Matthias Ehrgott.Multicriteria Optimization.Springer Science & Business Media, 2005.
Fernando et al. (2023)
â†‘
	Heshan Devaka Fernando, Han Shen, Miao Liu, Subhajit Chaudhury, Keerthiram Murugesan, and Tianyi Chen.Mitigating gradient bias in multi-objective learning: A provably convergent approach.In International Conference on Learning Representations, 2023.
Fliege et al. (2019)
â†‘
	JÃ¶rg Fliege, A Ismael F Vaz, and LuÃ­s Nunes Vicente.Complexity of gradient descent for multiobjective optimization.Optimization Methods and Software, 34(5):949â€“959, 2019.
Fliege & Svaiter (2000)
â†‘
	JÃ¶rg Fliege and Benar Fux Svaiter.Steepest descent methods for multicriteria optimization.Mathematical Methods of Operations Research, 51(3):479â€“494, 2000.
Ganin et al. (2016)
â†‘
	Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, FranÃ§ois Laviolette, Mario March, and Victor Lempitsky.Domain-adversarial training of neural networks.Journal of Machine Learning Research, 17(59):1â€“35, 2016.
Goodfellow et al. (2014)
â†‘
	Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.Generative adversarial nets.In Advances in Neural Information Processing Systems, volume 27, 2014.
Guerraoui et al. (2018)
â†‘
	Rachid Guerraoui, SÃ©bastien Rouault, et al.The hidden vulnerability of distributed learning in Byzantium.In International Conference on Machine Learning, pp.  3521â€“3530, 2018.
Helber et al. (2019)
â†‘
	Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification.IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):2217â€“2226, 2019.
Kairouz et al. (2021)
â†‘
	Peter Kairouz, H Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al.Advances and open problems in federated learning.Foundations and TrendsÂ® in Machine Learning, 14(1â€“2):1â€“210, 2021.
Kingma & Ba (2014)
â†‘
	Diederik P Kingma and Jimmy Ba.Adam: A method for stochastic optimization.arXiv preprint arXiv:1412.6980, 2014.
Krizhevsky et al. (2009)
â†‘
	Alex Krizhevsky, Geoffrey Hinton, et al.Learning multiple layers of features from tiny images, 2009.
Kurin et al. (2022)
â†‘
	Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon Whiteson, and Pawan K Mudigonda.In defense of the unitary scalarization for deep multi-task learning.In Advances in Neural Information Processing Systems, volume 35, pp.  12169â€“12183, 2022.
LeCun et al. (1998)
â†‘
	Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner.Gradient-based learning applied to document recognition.Proceedings of the IEEE, 86(11):2278â€“2324, 1998.
Lin et al. (2021)
â†‘
	Baijiong Lin, Feiyang Ye, Yu Zhang, and Ivor W Tsang.Reasonable effectiveness of random weighting: A litmus test for multi-task learning.arXiv preprint arXiv:2111.10603, 2021.
Liu et al. (2021a)
â†‘
	Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu.Conflict-averse gradient descent for multi-task learning.In Advances in Neural Information Processing Systems, volume 34, pp.  18878â€“18890, 2021a.
Liu et al. (2021b)
â†‘
	Liyang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang.Towards impartial multi-task learning.In International Conference on Learning Representations, 2021b.
Liu & Vicente (2024)
â†‘
	Suyun Liu and Luis Nunes Vicente.The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning.Annals of Operations Research, 339(3):1119â€“1148, 2024.
Lopez-Paz & Ranzato (2017)
â†‘
	David Lopez-Paz and Marcâ€™ Aurelio Ranzato.Gradient episodic memory for continual learning.In Advances in Neural Information Processing Systems, volume 30, 2017.
Loshchilov & Hutter (2019)
â†‘
	Ilya Loshchilov and Frank Hutter.Decoupled weight decay regularization.In International Conference on Learning Representations, 2019.
Mercier et al. (2018)
â†‘
	Quentin Mercier, Fabrice Poirion, and Jean-Antoine DÃ©sidÃ©ri.A stochastic multiple gradient descent algorithm.European Journal of Operational Research, 271(3):808â€“817, 2018.
Navon et al. (2022)
â†‘
	Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal Chechik, and Ethan Fetaya.Multi-task learning as a bargaining game.In International Conference on Machine Learning, pp.  16428â€“16446, 2022.
Nesterov (1983)
â†‘
	Yurii Nesterov.A method of solving a convex programming problem with convergence rate O(1/k**2).Proceedings of the USSR Academy of Sciences, 269(3):543â€“547, 1983.
Netzer et al. (2011)
â†‘
	Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.Reading digits in natural images with unsupervised feature learning.In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.
Paszke et al. (2019)
â†‘
	Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.PyTorch: An imperative style, high-performance deep learning library.In Advances in Neural Information Processing Systems, volume 32, 2019.
Poirion et al. (2017)
â†‘
	Fabrice Poirion, Quentin Mercier, and Jean-Antoine DÃ©sidÃ©ri.Descent algorithm for nonsmooth stochastic multiobjective optimization.Computational Optimization and Applications, 68(2):317â€“331, 2017.
Polyak (1964)
â†‘
	Boris T Polyak.Some methods of speeding up the convergence of iteration methods.USSR Computational Mathematics and Mathematical Physics, 4(5):1â€“17, 1964.
Ruder (2017)
â†‘
	Sebastian Ruder.An overview of multi-task learning in deep neural networks.arXiv preprint arXiv:1706.05098, 2017.
Sawaragi et al. (1985)
â†‘
	Yoshikazu Sawaragi, Hirotaka Nakayama, and Tetsuzo Tanino.Theory of Multiobjective Optimization.Elsevier, 1985.
Sener & Koltun (2018)
â†‘
	Ozan Sener and Vladlen Koltun.Multi-task learning as multi-objective optimization.In Advances in Neural Information Processing Systems, volume 31, 2018.
Senushkin et al. (2023)
â†‘
	Dmitry Senushkin, Nikolay Patakin, Arseny Kuznetsov, and Anton Konushin.Independent component alignment for multi-task learning.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.  20083â€“20093, 2023.
Vapnik (1995)
â†‘
	Vladimir Naumovich Vapnik.The Nature of Statistical learning theory.Springer-Verlag, 1995.
Xiao et al. (2017)
â†‘
	Han Xiao, Kashif Rasul, and Roland Vollgraf.Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms.arXiv preprint arXiv:1708.07747, 2017.
Xiao et al. (2023)
â†‘
	Peiyao Xiao, Hao Ban, and Kaiyi Ji.Direction-oriented multi-objective learning: Simple and provable stochastic algorithms.In Advances in Neural Information Processing Systems, volume 36, pp.  4509â€“4533, 2023.
Xin et al. (2022)
â†‘
	Derrick Xin, Behrooz Ghorbani, Justin Gilmer, Ankush Garg, and Orhan Firat.Do current multi-task optimization methods in deep learning even help?In Advances in Neural Information Processing Systems, volume 35, pp.  13597â€“13609, 2022.
Yin et al. (2018)
â†‘
	Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett.Byzantine-robust distributed learning: Towards optimal statistical rates.In International Conference on Machine Learning, pp.  5650â€“5659, 2018.
Yu et al. (2020)
â†‘
	Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.Gradient surgery for multi-task learning.In Advances in Neural Information Processing Systems, volume 33, pp.  5824â€“5836, 2020.
Zhou et al. (2022)
â†‘
	Shiji Zhou, Wenpeng Zhang, Jiyan Jiang, Wenliang Zhong, Jinjie Gu, and Wenwu Zhu.On the convergence of stochastic multi-objective gradient manipulation and beyond.In Advances in Neural Information Processing Systems, volume 35, pp.  38103â€“38115, 2022.
Appendix AAdditional experimental results

In this appendix, we provide additional experimental results about Jacobian descent and IWRM.

A.1Optimization trajectories

Figures 3 and 4 show the optimization trajectories of the aggregators of Table 1 for two different convex quadratic functions, with several initializations4.

The first function is the element-wise quadratic function 
ğ’‡
EWQ
:
â„
2
â†’
â„
2
, defined as

	
ğ’‡
EWQ
â¢
(
ğ’™
)
=
[
ğ‘¥
1
2
	
ğ‘¥
2
2
]
âŠ¤
.
		
(13)

Notably, its Pareto set only contains the origin, while its set of weakly Pareto stationary points is the union of the two axes. The Jacobian of 
ğ’‡
EWQ
 at 
[
ğ‘¥
1


ğ‘¥
2
]
 is 
[
2
â¢
ğ‘¥
1
	
0


0
	
2
â¢
ğ‘¥
2
]
, indicating that the gradients never conflict, which makes this function relatively simple to optimize. Nevertheless, Figure 3 shows that 
ğ’œ
MGDA
, 
ğ’œ
CAGrad
, 
ğ’œ
Nash-MTL
 and 
ğ’œ
Aligned-MTL
 fail to converge to the Pareto set for some initializations. Indeed, they converge to weakly Pareto stationary points, which can be arbitrarily far from the Pareto set. This trivial example shows the importance of rather proving convergence to the Pareto front as in our Section 2.4.

The second function is a convex quadratic form 
ğ’‡
CQF
:
â„
2
â†’
â„
2
, constructed to sometimes reproduce the setting of Figure 1 with conflicting and imbalanced gradients:

	
ğ’‡
CQF
â¢
(
ğ’™
)
	
=
[
(
ğ’™
âˆ’
ğ’—
1
)
âŠ¤
â‹…
ğ‘ˆ
â¢
Î£
1
â¢
ğ‘ˆ
âŠ¤
â‹…
(
ğ’™
âˆ’
ğ’—
1
)


(
ğ’™
âˆ’
ğ’—
2
)
âŠ¤
â‹…
ğ‘ˆ
âŠ¤
â¢
Î£
2
â¢
ğ‘ˆ
â‹…
(
ğ’™
âˆ’
ğ’—
2
)
]
		
(14)

	
with 
â¢
ğ‘ˆ
	
=
[
cos
â¡
ğœƒ
	
âˆ’
sin
â¡
ğœƒ


sin
â¡
ğœƒ
	
cos
â¡
ğœƒ
]
â¢
, 
â¢
Î£
1
=
[
1
	
0


0
	
ğœ–
]
â¢
, 
â¢
Î£
2
=
[
3
	
0


0
	
ğœ–
]
â¢
, 
â¢
ğœƒ
=
ğœ‹
16
â¢
, 
â¢
ğœ–
=
0.01
		
(15)

	
ğ’—
1
	
=
[
1


0
]
â¢
, and 
â¢
ğ’—
2
=
[
âˆ’
1


0
]
		
(16)

Figure 4 shows that most existing aggregators have some unusual behavior in this setting. In particular, on Figure 4(b), we can see that with 
ğ’œ
Mean
, 
ğ’œ
GradDrop
, 
ğ’œ
IMTL-G
, 
ğ’œ
CAGrad
, 
ğ’œ
RGW
 and 
ğ’œ
Aligned-MTL
, the updates can make one objectiveâ€™s value increase. In contrast, with the non-conflicting aggregators 
ğ’œ
MGDA
, 
ğ’œ
DualProj
, 
ğ’œ
Nash-MTL
 and 
ğ’œ
UPGrad
, no objective value increases during the optimization. Due to its linearity under scaling, 
ğ’œ
UPGrad
 also shows improved smoothness compared to the other non-conflicting aggregators.

(a)Parameter space. The black star is the Pareto set and the dashed lines are contour lines of the mean objective.
(b)Objective value space. The black star is the Pareto front. The background color represents the distance to the Pareto front.
Figure 3:Optimization trajectories of various aggregators when optimizing 
ğ’‡
EWQ
:
[
ğ‘¥
1
	
ğ‘¥
2
]
âŠ¤
â†¦
[
ğ‘¥
1
2
	
ğ‘¥
2
2
]
âŠ¤
 with JD. Colored dots represent the initial points. The trajectories start in red and evolve towards yellow.
(a)Parameter space. The dark line is the Pareto set. The background color represents the cosine similarity between the two gradients, between 
âˆ’
1
 (fully conflicting) in pink and 
1
 (fully aligned) in green. For instance, the similarity is 
âˆ’
0.925
 at the initial point 
[
0
	
0
]
âŠ¤
 and is 
âˆ’
1
 on (and only on) the Pareto set.
(b)Objective value space. The dark line is the Pareto front. The background color represents the distance to the Pareto front.
Figure 4:Optimization trajectories of various aggregators when optimizing the convex quadratic form 
ğ’‡
CQF
 of (14) with JD. Colored dots represent initial parameter values. The trajectories start in red and evolve towards yellow.
A.2All datasets and all aggregators

Figures 5, 6, 7, 8, 9 and 10 show the full results of the experiments described in Section 5 on SVHN, CIFAR-10, EuroSAT, MNIST, Fashion-MNIST and Kuzushiji-MNIST, respectively. For readability, the results are displayed on three different plots for each dataset. We always show 
ğ’œ
UPGrad
 and 
ğ’œ
Mean
 for reference. The exact experimental settings are described in Appendix D.

It should be noted that some of these aggregators were not developed as general-purpose aggregators, but mainly for the use case of multi-task learning, with one gradient per task. Our experiments present a more challenging setting than multi-task learning optimization because conflict between rows of the Jacobian is typically higher. Besides, for some aggregators, e.g. 
ğ’œ
GradDrop
 and 
ğ’œ
IMTL-G
, it was advised to make the aggregation of gradients w.r.t. an internal activation (such as the last shared representation), rather than w.r.t. the parameters of the model (Chen et al., 2020; Liu et al., 2021b). To enable comparison, we instead always aggregated the Jacobian w.r.t. all parameters.

We can see that 
ğ’œ
UPGrad
 provides a significant improvement over 
ğ’œ
Mean
 on all datasets. Moreover, the performance gaps seem to be linked to the difficulty of the dataset, which suggests that experimenting with harder tasks is a promising future direction. The intrinsic randomness of 
ğ’œ
RGW
 and 
ğ’œ
GradDrop
 reduces the train set performance, but it could positively impact the generalization, which we do not study here. We suspect the disappointing results of 
ğ’œ
Nash-MTL
 to be caused by issues in the official implementation that we used, leading to instability.

(a)Training loss
(b)Update similarity to the SGD update
(c)Training loss
(d)Update similarity to the SGD update
(e)Training loss
(f)Update similarity to the SGD update
Figure 5:SVHN results.
(a)Training loss
(b)Update similarity to the SGD update
(c)Training loss
(d)Update similarity to the SGD update
(e)Training loss
(f)Update similarity to the SGD update
Figure 6:CIFAR-10 results.
(a)Training loss
(b)Update similarity to the SGD update
(c)Training loss
(d)Update similarity to the SGD update
(e)Training loss
(f)Update similarity to the SGD update
Figure 7:EuroSAT results.
(a)Training loss
(b)Update similarity to the SGD update
(c)Training loss
(d)Update similarity to the SGD update
(e)Training loss
(f)Update similarity to the SGD update
Figure 8:MNIST results.
(a)Training loss
(b)Update similarity to the SGD update
(c)Training loss
(d)Update similarity to the SGD update
(e)Training loss
(f)Update similarity to the SGD update
Figure 9:Fashion-MNIST results.
(a)Training loss
(b)Update similarity to the SGD update
(c)Training loss
(d)Update similarity to the SGD update
(e)Training loss
(f)Update similarity to the SGD update
Figure 10:Kuzushiji-MNIST results.
A.3Varying the batch size

Figure 11 shows the results on CIFAR-10 with 
ğ’œ
UPGrad
 when varying the batch size from 4 to 64. Concretely, because we are using SSJD, this makes the number of rows of the sub-Jacobian aggregated at each step vary from 4 to 64. Recall that IWRM with SSJD and 
ğ’œ
Mean
 is equivalent to ERM with SGD. We observe that with a small batch size, 
ğ’œ
UPGrad
 becomes very similar to 
ğ’œ
Mean
. This is not surprising since both would be equivalent with a batch size of 1. Conversely, a larger batch size increases the gap between 
ğ’œ
UPGrad
 and 
ğ’œ
Mean
. Since the projections of 
ğ’œ
UPGrad
 are onto the dual cone of more rows, each step becomes non-conflicting with respect to more of the original 1024 objectives, pushing even further the benefits of the non-conflicting property. In other words, increasing the batch size refines the dual cone, thereby improving the quality of the projections. It would be interesting to theoretically analyze the impact of the batch size in this setting.

(a)
BS
=
4
: Training loss
(b)
BS
=
4
: Update similarity to the SGD update
(c)
BS
=
16
: Training loss
(d)
BS
=
16
: Update similarity to the SGD update
(e)
BS
=
64
: Training loss
(f)
BS
=
64
: Update similarity to the SGD update
Figure 11:CIFAR-10 results with different batch sizes (BS). The number of epochs is always 20, so the number of iterations varies.
A.4Compatibility with Adam

Figure 12 gives the results on CIFAR-10 and SVHN when using Adam rather than the SGD optimizer. Concretely, this corresponds to the Adam algorithm in which the gradient is replaced by the aggregation of the Jacobian. The learning rate is still tuned as described in Appendix D.1, but the other hyperparameters of Adam are fixed to the default values of PyTorch, i.e. 
ğ›½
1
=
0.9
, 
ğ›½
2
=
0.999
 and 
ğœ–
=
10
âˆ’
8
. Because optimization with Adam is faster, the number of epochs for SVHN and CIFAR-10 is reduced to 20 and 15, respectively. While the performance gap is smaller with this optimizer, it is still significant and suggests that our methods are beneficial with other optimizers than the simple SGD. Note that this analysis is fairly superficial. The thorough investigation of the interplay between aggregators and momentum-based optimizers is a compelling future research direction.

(a)SVHN: Training loss
(b)SVHN: Update similarity to the SGD update
(c)CIFAR-10: Training loss
(d)CIFAR-10: Update similarity to the SGD update
Figure 12:Results with the Adam optimizer.
Appendix BProofs
B.1Preliminary theoretical results

Recall that a function 
ğ’‡
:
â„
ğ‘›
â†’
â„
ğ‘š
 is -convex if for all 
ğ’™
,
ğ’š
âˆˆ
â„
ğ‘›
 and any 
ğœ†
âˆˆ
[
0
,
1
]
,

	
ğ’‡
â¢
(
ğœ†
â¢
ğ’™
+
(
1
âˆ’
ğœ†
)
â¢
ğ’š
)
â¢
ğœ†
â¢
ğ’‡
â¢
(
ğ’™
)
+
(
1
âˆ’
ğœ†
)
â¢
ğ’‡
â¢
(
ğ’š
)
.
	
Lemma 1.

If 
ğ’‡
:
â„
ğ‘›
â†’
â„
ğ‘š
 is a continuously differentiable -convex function, then for any pair of vectors 
ğ’™
,
ğ’š
âˆˆ
â„
ğ‘›
, 
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â¢
(
ğ’š
âˆ’
ğ’™
)
â¢
ğ’‡
â¢
(
ğ’š
)
âˆ’
ğ’‡
â¢
(
ğ’™
)
.

Proof.
	
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
â¢
(
ğ’š
âˆ’
ğ’™
)
	
=
lim
ğœ†
â†’
0
+
ğ’‡
â¢
(
ğ’™
+
ğœ†
â¢
(
ğ’š
âˆ’
ğ’™
)
)
âˆ’
ğ’‡
â¢
(
ğ’™
)
ğœ†
	
(
differentiation
)
	
		
lim
ğœ†
â†’
0
+
ğ’‡
â¢
(
ğ’™
)
+
ğœ†
â¢
(
ğ’‡
â¢
(
ğ’š
)
âˆ’
ğ’‡
â¢
(
ğ’™
)
)
âˆ’
ğ’‡
â¢
(
ğ’™
)
ğœ†
	
(
-convexity
)
	
		
=
ğ’‡
â¢
(
ğ’š
)
âˆ’
ğ’‡
â¢
(
ğ’™
)
,
	

which concludes the proof. âˆ

Lemma 2.

Let 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, let 
ğ’–
âˆˆ
â„
ğ‘š
 and let 
ğ’™
âˆˆ
â„
ğ‘›
, then

	
ğ’–
âŠ¤
â¢
ğ½
â¢
ğ’™
â¢
â€–
ğ’–
â€–
â‹…
â€–
ğ½
â€–
F
â‹…
â€–
ğ’™
â€–
	
Proof.

Let 
ğ½
ğ‘–
 be the 
ğ‘–
th row of 
ğ½
, then

	
(
ğ’–
âŠ¤
â¢
ğ½
â¢
ğ’™
)
2
	
â€–
ğ’–
â€–
2
â‹…
â€–
ğ½
â¢
ğ’™
â€–
2
	
(
Cauchy-Schwartz


inequality
)
	
		
=
â€–
ğ’–
â€–
2
â‹…
âˆ‘
ğ‘–
âˆˆ
[
ğ‘š
]
(
ğ½
ğ‘–
âŠ¤
â¢
ğ’™
)
2
	
		
â€–
ğ’–
â€–
2
â‹…
âˆ‘
ğ‘–
âˆˆ
[
ğ‘š
]
â€–
ğ½
ğ‘–
â€–
2
â‹…
â€–
ğ’™
â€–
2
	
(
Cauchy-Schwartz


inequality
)
	
		
=
â€–
ğ’–
â€–
2
â‹…
â€–
ğ½
â€–
F
2
â‹…
â€–
ğ’™
â€–
2
,
	

which concludes the proof. âˆ

Recall that a function 
ğ’‡
:
â„
ğ‘›
â†’
â„
ğ‘š
 is 
ğ›½
-smooth if for all 
ğ’™
,
ğ’š
âˆˆ
â„
ğ‘›
,

	
â€–
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
)
âˆ’
ğ’¥
â¢
ğ’‡
â¢
(
ğ’š
)
â€–
F
â¢
ğ›½
â¢
â€–
ğ’™
âˆ’
ğ’š
â€–
		
(17)
Lemma 3.

Let 
ğ’‡
:
â„
ğ‘›
â†’
â„
ğ‘š
 be 
ğ›½
-smooth, then for any 
ğ’˜
âˆˆ
â„
ğ‘š
 and any 
ğ’™
,
ğ’š
âˆˆ
â„
ğ‘›
,

	
ğ’˜
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
)
âˆ’
ğ’‡
â¢
(
ğ’š
)
âˆ’
ğ’¥
â¢
ğ’‡
â¢
(
ğ’š
)
â¢
(
ğ’™
âˆ’
ğ’š
)
)
â¢
ğ›½
2
â¢
â€–
ğ’˜
â€–
â‹…
â€–
ğ’™
âˆ’
ğ’š
â€–
2
		
(18)
Proof.
		
ğ’˜
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
)
âˆ’
ğ’‡
â¢
(
ğ’š
)
âˆ’
ğ’¥
â¢
ğ’‡
â¢
(
ğ’š
)
â¢
(
ğ’™
âˆ’
ğ’š
)
)
	
	
=
	
ğ’˜
âŠ¤
â¢
(
âˆ«
0
1
ğ’¥
â¢
ğ’‡
â¢
(
ğ’š
+
ğ‘¡
â¢
(
ğ’™
âˆ’
ğ’š
)
)
â¢
(
ğ’™
âˆ’
ğ’š
)
â¢
ğ‘‘
ğ‘¡
âˆ’
ğ’¥
â¢
ğ’‡
â¢
(
ğ’š
)
â¢
(
ğ’™
âˆ’
ğ’š
)
)
	
(
fundamental


theorem


of calculus
)
	
	
=
	
âˆ«
0
1
ğ’˜
âŠ¤
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’š
+
ğ‘¡
â¢
(
ğ’™
âˆ’
ğ’š
)
)
âˆ’
ğ’¥
â¢
ğ’‡
â¢
(
ğ’š
)
)
â¢
(
ğ’™
âˆ’
ğ’š
)
â¢
ğ‘‘
ğ‘¡
	
		
âˆ«
0
1
â€–
ğ’˜
â€–
â‹…
â€–
ğ’¥
â¢
ğ’‡
â¢
(
ğ’š
+
ğ‘¡
â¢
(
ğ’™
âˆ’
ğ’š
)
)
âˆ’
ğ’¥
â¢
ğ’‡
â¢
(
ğ’š
)
â€–
F
â‹…
â€–
ğ’™
âˆ’
ğ’š
â€–
â¢
ğ‘‘
ğ‘¡
	
(
Lemma 
2
)
	
		
âˆ«
0
1
â€–
ğ’˜
â€–
â‹…
ğ›½
â¢
ğ‘¡
â‹…
â€–
ğ’™
âˆ’
ğ’š
â€–
2
â¢
ğ‘‘
ğ‘¡
	
(
ğ›½
-smoothness 
17
)
	
	
=
	
ğ›½
2
â¢
â€–
ğ’˜
â€–
â‹…
â€–
ğ’™
âˆ’
ğ’š
â€–
2
,
	

which concludes the proof. âˆ

B.2Proposition 1
Proposition 1.

Let 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
. For any 
ğ’–
âˆˆ
â„
ğ‘š
, 
ğœ‹
ğ½
â¢
(
ğ½
âŠ¤
â¢
ğ’–
)
=
ğ½
âŠ¤
â¢
ğ’˜
 with

	
ğ’˜
	
âˆˆ
arg
â¢
min
ğ’—
âˆˆ
â„
ğ‘š
:
ğ’–
â¢
ğ’—
â¡
ğ’—
âŠ¤
â¢
ğ½
â¢
ğ½
âŠ¤
â¢
ğ’—
.
		
(5)
Proof.

This is a direct consequence of Lemma 4. âˆ

Lemma 4.

Let 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, 
ğº
=
ğ½
â¢
ğ½
âŠ¤
, 
ğ’–
âˆˆ
â„
ğ‘š
. For any 
ğ’˜
âˆˆ
â„
ğ‘š
 satisfying

		
ğ’–
â¢
ğ’˜
			
(19a)

		
ğŸ
â¢
ğº
â¢
ğ’˜
			
(19b)

		
ğ’–
âŠ¤
â¢
ğº
â¢
ğ’˜
=
ğ’˜
âŠ¤
â¢
ğº
â¢
ğ’˜
			
(19c)

we have 
ğœ‹
ğ½
â¢
(
ğ½
âŠ¤
â¢
ğ’–
)
=
ğ½
âŠ¤
â¢
ğ’˜
. Such a 
ğ’˜
 is the solution to

	
ğ’˜
	
âˆˆ
arg
â¢
min
ğ’–
â¢
ğ’—
â¡
ğ’—
âŠ¤
â¢
ğº
â¢
ğ’—
.
	
Proof.

The projection

	
ğœ‹
ğ½
â¢
(
ğ½
âŠ¤
â¢
ğ’–
)
=
arg
â¢
min
ğ’™
âˆˆ
â„
ğ‘›
:


ğŸ
â¢
ğ½
â¢
ğ’™
â¡
1
2
â¢
â€–
ğ’™
âˆ’
ğ½
âŠ¤
â¢
ğ’–
â€–
2
	

is a convex program. Consequently, the KKT conditions are both necessary and sufficient. The Lagragian is given by 
â„’
â¢
(
ğ’™
,
ğ’—
)
=
1
2
â¢
â€–
ğ’™
âˆ’
ğ½
âŠ¤
â¢
ğ’–
â€–
2
âˆ’
ğ’—
âŠ¤
â¢
ğ½
â¢
ğ’™
. The KKT conditions are then given by

		
{
âˆ‡
ğ’™
â„’
â¢
(
ğ’™
,
ğ’—
)
=
ğŸ
	

ğŸ
â¢
ğ’—
	

ğŸ
â¢
ğ½
â¢
ğ’™
	

0
=
ğ’—
âŠ¤
â¢
ğ½
â¢
ğ’™
	
	
	
â‡”
	
{
ğ’™
=
ğ½
âŠ¤
â¢
(
ğ’–
+
ğ’—
)
	

ğŸ
â¢
ğ’—
	

ğŸ
â¢
ğº
â¢
(
ğ’–
+
ğ’—
)
	

0
=
ğ’—
âŠ¤
â¢
ğº
â¢
(
ğ’–
+
ğ’—
)
	
	
	
â‡”
	
{
ğ’™
=
ğ½
âŠ¤
â¢
(
ğ’–
+
ğ’—
)
	

ğ’–
â¢
ğ’–
+
ğ’—
	

ğŸ
â¢
ğº
â¢
(
ğ’–
+
ğ’—
)
	

ğ’–
âŠ¤
â¢
ğº
â¢
(
ğ’–
+
ğ’—
)
=
(
ğ’–
+
ğ’—
)
âŠ¤
â¢
ğº
â¢
(
ğ’–
+
ğ’—
)
	
	

The simple change of variable 
ğ’˜
=
ğ’–
+
ğ’—
 finishes the proof of the first part.

Since 
ğ’™
=
ğ½
âŠ¤
â¢
(
ğ’–
+
ğ’—
)
, the Wolfe dual program of 
ğœ‹
ğ½
â¢
(
ğ½
âŠ¤
â¢
ğ’–
)
 gives

	
ğ’˜
	
âˆˆ
ğ’–
+
arg
â¢
max
ğ’—
âˆˆ
â„
ğ‘š
:
ğŸ
â¢
ğ’—
â¡
â„’
â¢
(
ğ½
âŠ¤
â¢
(
ğ’–
+
ğ’—
)
,
ğ’—
)
	
		
=
ğ’–
+
arg
â¢
max
ğ’—
âˆˆ
â„
ğ‘š
:
ğŸ
â¢
ğ’—
â¡
1
2
â¢
â€–
ğ½
âŠ¤
â¢
ğ’—
â€–
2
âˆ’
ğ’—
âŠ¤
â¢
ğ½
â¢
ğ½
âŠ¤
â¢
(
ğ’–
+
ğ’—
)
	
		
=
ğ’–
+
arg
â¢
max
ğ’—
âˆˆ
â„
ğ‘š
:
ğŸ
â¢
ğ’—
âˆ’
1
2
â¢
ğ’—
âŠ¤
â¢
ğº
â¢
ğ’—
âˆ’
ğ’—
âŠ¤
â¢
ğº
â¢
ğ’–
	
		
=
ğ’–
+
arg
â¢
min
ğ’—
âˆˆ
â„
ğ‘š
:
ğ’–
â¢
ğ’–
+
ğ’—
â¡
1
2
â¢
(
ğ’–
+
ğ’—
)
âŠ¤
â¢
ğº
â¢
(
ğ’–
+
ğ’—
)
	
		
=
arg
â¢
min
ğ’—
â€²
âˆˆ
â„
ğ‘š
:
ğ’–
â¢
ğ’—
â€²
â¡
1
2
â¢
ğ’—
â€²
â£
âŠ¤
â¢
ğº
â¢
ğ’—
â€²
,
	

which concludes the proof. âˆ

B.3Theorem 1
Theorem 1.

Let 
ğ’‡
:
â„
ğ‘›
â†’
â„
ğ‘š
 be a 
ğ›½
-smooth and -convex function. Suppose that the Pareto front 
ğ’‡
â¢
(
ğ‘‹
âˆ—
)
 is bounded and that for any 
ğ’™
âˆˆ
â„
ğ‘›
, there is 
ğ’™
âˆ—
âˆˆ
ğ‘‹
âˆ—
 satisfying 
ğ’‡
â¢
(
ğ’™
âˆ—
)
â¢
ğ’‡
â¢
(
ğ’™
)
. Let 
ğ’™
1
âˆˆ
â„
ğ‘›
, and for all 
ğ‘¡
âˆˆ
â„•
, 
ğ’™
ğ‘¡
+
1
=
ğ’™
ğ‘¡
âˆ’
ğœ‚
â¢
ğ’œ
UPGrad
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
, with 
ğœ‚
=
1
ğ›½
â¢
ğ‘š
. Let 
ğ’˜
ğ‘¡
 be the weights defining 
ğ’œ
UPGrad
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
 as per (6), i.e. 
ğ’œ
UPGrad
â¢
(
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
=
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
âŠ¤
â‹…
ğ’˜
ğ‘¡
. If 
ğ’˜
ğ‘¡
 is bounded, then 
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
 converges to 
ğ’‡
â¢
(
ğ’™
âˆ—
)
 for some 
ğ’™
âˆ—
âˆˆ
ğ‘‹
âˆ—
. In other words, 
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
 converges to the Pareto front.

To prove the theorem we will need Lemmas 5, 6 and 7 below.

Lemma 5.

Let 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
 and 
ğ’˜
=
1
ğ‘š
â¢
âˆ‘
ğ‘–
=
1
ğ‘š
ğ’˜
ğ‘–
 be the weights defining 
ğ’œ
UPGrad
â¢
(
ğ½
)
 as per (6). Let, as usual, 
ğº
=
ğ½
â¢
ğ½
âŠ¤
, then,

	
ğ’˜
âŠ¤
â¢
ğº
â¢
ğ’˜
â¢
ğŸ
âŠ¤
â¢
ğº
â¢
ğ’˜
.
	
Proof.

Observe that if, for any 
ğ’–
,
ğ’—
âˆˆ
â„
ğ‘š
, 
âŸ¨
ğ’–
,
ğ’—
âŸ©
=
ğ’–
âŠ¤
â¢
ğº
â¢
ğ’—
, then 
âŸ¨
â‹…
,
â‹…
âŸ©
 is an inner product. In this Hilbert space, the Cauchy-Schwartz inequality reads as

	
(
ğ’–
âŠ¤
â¢
ğº
â¢
ğ’—
)
2
	
=
âŸ¨
ğ’–
,
ğ’—
âŸ©
2
	
		
âŸ¨
ğ’–
,
ğ’–
âŸ©
â‹…
âŸ¨
ğ’—
,
ğ’—
âŸ©
	
		
=
ğ’–
âŠ¤
â¢
ğº
â¢
ğ’–
â‹…
ğ’—
âŠ¤
â¢
ğº
â¢
ğ’—
.
	

Therefore

		
ğ’˜
âŠ¤
â¢
ğº
â¢
ğ’˜
	
	
=
	
1
ğ‘š
2
â¢
âˆ‘
ğ‘–
,
ğ‘—
ğ’˜
ğ‘–
âŠ¤
â¢
ğº
â¢
ğ’˜
ğ‘—
	
		
1
ğ‘š
2
â¢
âˆ‘
ğ‘–
,
ğ‘—
ğ’˜
ğ‘–
âŠ¤
â¢
ğº
â¢
ğ’˜
ğ‘–
â‹…
ğ’˜
ğ‘—
âŠ¤
â¢
ğº
â¢
ğ’˜
ğ‘—
	
(
Cauchy-Schwartz


inequality
)
	
	
=
	
(
âˆ‘
ğ‘–
1
ğ‘š
â¢
ğ’˜
ğ‘–
âŠ¤
â¢
ğº
â¢
ğ’˜
ğ‘–
)
2
	
		
âˆ‘
ğ‘–
1
ğ‘š
â¢
(
ğ’˜
ğ‘–
âŠ¤
â¢
ğº
â¢
ğ’˜
ğ‘–
)
2
	
(
Jensenâ€™s inequality
)
	
	
=
	
1
ğ‘š
â¢
âˆ‘
ğ‘–
ğ’˜
ğ‘–
âŠ¤
â¢
ğº
â¢
ğ’˜
ğ‘–
	
(
ğº
â¢
 positive semi-definite
)
	
	
=
	
1
ğ‘š
â¢
âˆ‘
ğ‘–
ğ’†
ğ‘–
âŠ¤
â¢
ğº
â¢
ğ’˜
ğ‘–
	
(
Lemma 
4
, (
19c
)
)
	
		
1
ğ‘š
â¢
âˆ‘
ğ‘–
ğŸ
âŠ¤
â¢
ğº
â¢
ğ’˜
ğ‘–
	
(
Lemma 
4
, (
19b
)


ğ’†
ğ‘–
â¢
ğŸ
)
	
	
=
	
ğŸ
âŠ¤
â¢
ğº
â¢
ğ’˜
,
	

which concludes the proof. âˆ

Lemma 6.

Under the assumptions of Theorem 1, for any 
ğ’˜
âˆˆ
â„
ğ‘š
 and any 
ğ‘¡
âˆˆ
â„•
,

	
ğ’˜
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
+
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
â¢
â€–
ğ’˜
â€–
ğ›½
â¢
ğ‘š
â¢
(
ğŸ
2
â¢
ğ‘š
âˆ’
ğ’˜
â€–
ğ’˜
â€–
)
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
.
	
Proof.

For all 
ğ‘¡
âˆˆ
â„•
, let 
ğ½
ğ‘¡
=
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
, 
ğº
ğ‘¡
=
ğ½
ğ‘¡
â¢
ğ½
ğ‘¡
âŠ¤
. Then 
ğ’™
ğ‘¡
+
1
=
ğ’™
ğ‘¡
âˆ’
ğœ‚
â¢
ğ’œ
UPGrad
â¢
(
ğ½
ğ‘¡
)
=
ğ’™
ğ‘¡
âˆ’
ğœ‚
â¢
ğ½
ğ‘¡
âŠ¤
â¢
ğ’˜
ğ‘¡
. Therefore

		
ğ’˜
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
+
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
	
		
âˆ’
ğœ‚
â¢
ğ’˜
âŠ¤
â¢
ğ½
ğ‘¡
â¢
ğ½
ğ‘¡
âŠ¤
â¢
ğ’˜
ğ‘¡
+
ğ›½
â¢
ğœ‚
2
2
â¢
â€–
ğ’˜
â€–
â‹…
â€–
ğ½
ğ‘¡
âŠ¤
â¢
ğ’˜
ğ‘¡
â€–
2
	
(
Lemma 
3
)
	
	
=
	
âˆ’
1
ğ›½
â¢
ğ‘š
â¢
ğ’˜
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
+
1
2
â¢
ğ›½
â¢
ğ‘š
â¢
â€–
ğ’˜
â€–
â‹…
ğ’˜
ğ‘¡
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
	
(
ğœ‚
=
1
ğ›½
â¢
ğ‘š
)
	
		
âˆ’
1
ğ›½
â¢
ğ‘š
â¢
ğ’˜
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
+
1
2
â¢
ğ›½
â¢
ğ‘š
â¢
â€–
ğ’˜
â€–
â‹…
ğŸ
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
	
(
Lemma 
5
)
	
	
=
	
â€–
ğ’˜
â€–
ğ›½
â¢
ğ‘š
â¢
(
ğŸ
2
â¢
ğ‘š
âˆ’
ğ’˜
â€–
ğ’˜
â€–
)
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
,
	

which concludes the proof. âˆ

Lemma 7.

Under the assumptions of Theorem 1, if 
ğ’™
âˆ—
âˆˆ
ğ‘‹
âˆ—
 satisfies 
ğŸ
âŠ¤
â¢
ğ’‡
â¢
(
ğ’™
âˆ—
)
â¢
ğŸ
âŠ¤
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
 for all 
ğ‘¡
âˆˆ
â„•
, then

	
1
ğ‘‡
â¢
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
â¢
1
ğ‘‡
â¢
(
ğŸ
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
+
ğ›½
â¢
ğ‘š
2
â¢
â€–
ğ’™
1
âˆ’
ğ’™
âˆ—
â€–
2
)
.
		
(20)
Proof.

We first bound, for any 
ğ‘¡
âˆˆ
â„•
, 
ğŸ
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
+
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
 as follows

		
ğŸ
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
+
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
	
		
âˆ’
1
2
â¢
ğ›½
â¢
ğ‘š
â‹…
ğŸ
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
	
(
Lemma 
6


with 
â¢
ğ’˜
=
ğŸ
)
	
		
âˆ’
1
2
â¢
ğ›½
â¢
ğ‘š
â‹…
ğ’˜
ğ‘¡
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
.
	
(
Lemma 
5
)
	

Summing this over 
ğ‘¡
âˆˆ
[
ğ‘‡
]
 yields

		
1
2
â¢
ğ›½
â¢
ğ‘š
â¢
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
	
		
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğŸ
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘¡
+
1
)
)
	
	
=
	
ğŸ
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘‡
+
1
)
)
	
(
Telescoping sum
)
		
		
ğŸ
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
.
	
(
Assumption


ğŸ
âŠ¤
â¢
ğ’‡
â¢
(
ğ’™
âˆ—
)
â¢
ğŸ
âŠ¤
â¢
ğ’‡
â¢
(
ğ’™
ğ‘‡
+
1
)
)
			
(21)

Since 
ğŸ
â¢
ğ’˜
ğ‘¡
,

		
ğ’˜
ğ‘¡
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
	
		
ğ’˜
ğ‘¡
âŠ¤
â¢
ğ½
ğ‘¡
â¢
(
ğ’™
ğ‘¡
âˆ’
ğ’™
âˆ—
)
	
(
Lemma 
1
)
	
	
=
	
1
ğœ‚
â¢
(
ğ’™
ğ‘¡
âˆ’
ğ’™
ğ‘¡
+
1
)
âŠ¤
â¢
(
ğ’™
ğ‘¡
âˆ’
ğ’™
âˆ—
)
	
(
ğ’™
ğ‘¡
+
1
=
ğ’™
ğ‘¡
âˆ’
ğœ‚
â¢
ğ½
ğ‘¡
âŠ¤
â¢
ğ’˜
ğ‘¡
)
	
	
=
	
1
2
â¢
ğœ‚
â¢
(
â€–
ğ’™
ğ‘¡
âˆ’
ğ’™
ğ‘¡
+
1
â€–
2
+
â€–
ğ’™
ğ‘¡
âˆ’
ğ’™
âˆ—
â€–
2
âˆ’
â€–
ğ’™
ğ‘¡
+
1
âˆ’
ğ’™
âˆ—
â€–
2
)
	
(
Parallelogram


law
)
	
	
=
	
1
2
â¢
ğ›½
â¢
ğ‘š
â¢
ğ’˜
ğ‘¡
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
+
ğ›½
â¢
ğ‘š
2
â¢
(
â€–
ğ’™
ğ‘¡
âˆ’
ğ’™
âˆ—
â€–
2
âˆ’
â€–
ğ’™
ğ‘¡
+
1
âˆ’
ğ’™
âˆ—
â€–
2
)
.
	
(
ğœ‚
=
1
ğ›½
â¢
ğ‘š
)
	

Summing this over 
ğ‘¡
âˆˆ
[
ğ‘‡
]
 yields

		
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
	
		
1
2
â¢
ğ›½
â¢
ğ‘š
â¢
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
+
ğ›½
â¢
ğ‘š
2
â¢
(
â€–
ğ’™
1
âˆ’
ğ’™
âˆ—
â€–
2
âˆ’
â€–
ğ’™
ğ‘‡
+
1
âˆ’
ğ’™
âˆ—
â€–
2
)
	
(
Telescoping sum
)
	
		
1
2
â¢
ğ›½
â¢
ğ‘š
â¢
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
+
ğ›½
â¢
ğ‘š
2
â¢
â€–
ğ’™
1
âˆ’
ğ’™
âˆ—
â€–
2
	
		
ğŸ
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
+
ğ›½
â¢
ğ‘š
2
â¢
â€–
ğ’™
1
âˆ’
ğ’™
âˆ—
â€–
2
.
	
(
By (
21
)
)
	

Scaling down this inequality by 
ğ‘‡
 yields

	
1
ğ‘‡
â¢
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
â¢
1
ğ‘‡
â¢
(
ğŸ
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
+
ğ›½
â¢
ğ‘š
2
â¢
â€–
ğ’™
1
âˆ’
ğ’™
âˆ—
â€–
2
)
,
	

which concludes the proof. âˆ

We are now ready to prove Theorem 1.

Proof.

For all 
ğ‘¡
âˆˆ
â„•
, let 
ğ½
ğ‘¡
=
ğ’¥
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
, 
ğº
ğ‘¡
=
ğ½
ğ‘¡
â¢
ğ½
ğ‘¡
âŠ¤
. Then

	
ğ’™
ğ‘¡
+
1
	
=
ğ’™
ğ‘¡
âˆ’
ğœ‚
â¢
ğ’œ
UPGrad
â¢
(
ğ½
ğ‘¡
)
	
		
=
ğ’™
ğ‘¡
âˆ’
ğœ‚
â¢
ğ½
ğ‘¡
âŠ¤
â¢
ğ’˜
ğ‘¡
.
	

Substituting 
ğ’˜
=
ğŸ
 in the term 
ğŸ
2
â¢
ğ‘š
âˆ’
ğ’˜
â€–
ğ’˜
â€–
 of Lemma 6 yields

	
ğŸ
2
â¢
ğ‘š
âˆ’
ğ’˜
â€–
ğ’˜
â€–
	
=
âˆ’
ğŸ
2
â¢
ğ‘š
	
		
ğŸ
.
	

Therefore there exists some 
ğœ€
â¢
>
â¢
0
 such that any 
ğ’˜
âˆˆ
â„
ğ‘š
 with 
â€–
ğŸ
âˆ’
ğ’˜
â€–
â¢
<
â¢
ğœ€
 satisfies 
ğŸ
2
â¢
ğ‘š
â¢
ğ’˜
â€–
ğ’˜
â€–
. Denote by 
ğµ
ğœ€
â¢
(
ğŸ
)
=
{
ğ’˜
âˆˆ
â„
ğ‘š
:
â€–
ğŸ
âˆ’
ğ’˜
â€–
â¢
<
â¢
ğœ€
}
, i.e. for all 
ğ’˜
âˆˆ
ğµ
ğœ€
â¢
(
ğŸ
)
, 
ğŸ
2
â¢
ğ‘š
â¢
ğ’˜
â€–
ğ’˜
â€–
. By the non-conflicting property of 
ğ’œ
UPGrad
, 
ğŸ
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
 and therefore for all 
ğ’˜
âˆˆ
ğµ
ğœ€
â¢
(
ğŸ
)
,

		
ğ’˜
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
+
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
	
		
â€–
ğ’˜
â€–
ğ›½
â¢
ğ‘š
â¢
(
ğŸ
2
â¢
ğ‘š
âˆ’
ğ’˜
â€–
ğ’˜
â€–
)
â¢
ğº
ğ‘¡
â¢
ğ’˜
ğ‘¡
	
(
Lemma 
6
)
	
		
0
.
	

Since 
ğ’˜
âŠ¤
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
 is bounded and non-increasing, it converges. Since 
ğµ
ğœ€
â¢
(
ğŸ
)
 contains a basis of 
â„
ğ‘š
, 
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
 converges to some 
ğ’‡
âˆ—
âˆˆ
â„
ğ‘š
. By assumption on 
ğ’‡
, there exists 
ğ’™
âˆ—
 in the Pareto set satisfying 
ğ’‡
â¢
(
ğ’™
âˆ—
)
â¢
ğ’‡
âˆ—
.

We now prove that 
ğ’‡
â¢
(
ğ’™
âˆ—
)
=
ğ’‡
âˆ—
. Since 
ğ’‡
â¢
(
ğ’™
âˆ—
)
â¢
ğ’‡
âˆ—
, it is sufficient to show that 
ğŸ
âŠ¤
â¢
(
ğ’‡
âˆ—
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
â¢
0
.

First, the additional assumption of Lemma 7 applies since 
ğŸ
âŠ¤
â¢
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
 decreases to 
ğŸ
âŠ¤
â¢
ğ’‡
âˆ—
 which is larger than 
ğŸ
âŠ¤
â¢
ğ’‡
â¢
(
ğ’™
âˆ—
)
. Therefore

		
ğŸ
âŠ¤
â¢
(
ğ’‡
âˆ—
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
	
		
(
ğ‘š
ğ‘‡
â¢
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
)
âŠ¤
â¢
(
ğ’‡
âˆ—
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
	
(
ğ’‡
â¢
(
ğ’™
âˆ—
)
â¢
ğ’‡
âˆ—


ğŸ
â¢
ğ‘š
â¢
ğ’˜
ğ‘¡


 by (
19a
)
)
		
	
=
	
ğ‘š
ğ‘‡
â¢
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
(
ğ’‡
âˆ—
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
+
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
	
	
=
	
ğ‘š
ğ‘‡
â¢
(
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
(
ğ’‡
âˆ—
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
+
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
)
	
		
ğ‘š
ğ‘‡
â¢
(
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
(
ğ’‡
âˆ—
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
+
ğŸ
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
+
ğ›½
â¢
ğ‘š
2
â¢
â€–
ğ’™
1
âˆ’
ğ’™
âˆ—
â€–
2
)
	
(
Lemma 
7
)
			
(22)

Taking the limit as 
ğ‘‡
â†’
âˆ
, we get

		
ğŸ
âŠ¤
â¢
(
ğ’‡
âˆ—
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
	
		
lim
ğ‘‡
â†’
âˆ
ğ‘š
ğ‘‡
â¢
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
(
ğ’‡
âˆ—
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
)
	
		
lim
ğ‘‡
â†’
âˆ
ğ‘š
ğ‘‡
â¢
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
â€–
ğ’˜
ğ‘¡
â€–
â‹…
â€–
ğ’‡
âˆ—
âˆ’
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
â€–
	
(
Cauchy-Schwartz


inequality
)
	
	
=
	
0
,
	
(
ğ’˜
ğ‘¡
â¢
 bounded


ğ’‡
â¢
(
ğ’™
ğ‘¡
)
â†’
ğ’‡
âˆ—
)
	

which concludes the proof. âˆ

B.4Supplementary theoretical results

The proof of Theorem 1 provides some additional insights about the convergence rate as well as some notion of convergence in the non-convex case.

Convergence rate.

Combining the equality 
ğ’‡
âˆ—
=
ğ’‡
â¢
(
ğ’™
âˆ—
)
 and (22), we have

	
1
ğ‘‡
â¢
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
â¢
1
ğ‘‡
â¢
ğŸ
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
+
ğ›½
â¢
ğ‘š
2
â¢
ğ‘‡
â¢
â€–
ğ’™
1
âˆ’
ğ’™
âˆ—
â€–
2
	

Furthermore, the Cauchy-Schwartz inequality yields 
ğŸ
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
â¢
ğ‘š
â‹…
â€–
ğ’‡
â¢
(
ğ’™
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
â€–
, so

	
1
ğ‘‡
â¢
âˆ‘
ğ‘¡
âˆˆ
[
ğ‘‡
]
ğ’˜
ğ‘¡
âŠ¤
â¢
(
ğ’‡
â¢
(
ğ’™
ğ‘¡
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
)
â¢
ğ‘š
ğ‘‡
â¢
(
â€–
ğ’‡
â¢
(
ğ’™
1
)
âˆ’
ğ’‡
â¢
(
ğ’™
âˆ—
)
â€–
+
ğ›½
2
â¢
â€–
ğ’™
1
âˆ’
ğ’™
âˆ—
â€–
2
)
		
(23)

This hints a convergence rate of order 
ğ’ª
â¢
(
1
ğ‘‡
)
, whose constant depends on the initial point 
ğ’™
1
.

Non-convex setting.

The proof of (21) does not require the convexity of the objective function. This bound can be equivalently formulated as

	
1
ğ‘‡
â¢
âˆ‘
ğ‘¡
=
1
ğ‘‡
â€–
ğ½
ğ‘¡
âŠ¤
â¢
ğ‘¤
ğ‘¡
â€–
2
â¢
2
â¢
ğ›½
â¢
ğ‘š
ğ‘‡
â¢
ğŸ
âŠ¤
â¢
(
ğ‘“
â¢
(
ğ‘¥
1
)
âˆ’
ğ‘“
â¢
(
ğ‘¥
âˆ—
)
)
		
(24)

This shows the convergence of the updates in the non-convex setting, under the smoothness condition of Theorem 1.

Appendix CProperties of existing aggregators

In the following, we prove the properties of the aggregators from Table 1. Some aggregators, e.g. 
ğ’œ
RGW
, 
ğ’œ
GradDrop
 and 
ğ’œ
PCGrad
, are non-deterministic and are thus not technically functions but rather random variables whose distribution depends on the matrix 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
 to aggregate. Still, the properties of Section 2.2 can be easily adapted to a random setting. If 
ğ’œ
 is a random aggregator, then for any 
ğ½
, 
ğ’œ
â¢
(
ğ½
)
 is a random vector in 
â„
ğ‘›
. The aggregator is non-conflicting if 
ğ’œ
â¢
(
ğ½
)
 is in the dual cone of the rows of 
ğ½
 with probability 1. It is linear under scaling if for all 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, there is a â€“ possibly random â€“ matrix 
ğ”
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
 such that for all 
ğŸ
â¢
ğ’„
âˆˆ
â„
ğ‘š
, 
ğ’œ
â¢
(
diag
(
ğ’„
)
â‹…
ğ½
)
=
ğ”
âŠ¤
â‹…
ğ’„
. Finally, 
ğ’œ
 is weighted if for any 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
 there is a â€“ possibly random â€“ weighting 
ğ’˜
âˆˆ
â„
ğ‘š
 satisfying 
ğ’œ
â¢
(
ğ½
)
=
ğ½
âŠ¤
â‹…
ğ’˜
.

C.1Mean

ğ’œ
Mean
 simply averages the rows of the input matrix, i.e. for all 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
,

	
ğ’œ
Mean
â¢
(
ğ½
)
=
1
ğ‘š
â¢
ğ½
âŠ¤
â‹…
ğŸ
		
(25)
âœ—â€‰ Non-conflicting.

ğ’œ
Mean
â¢
(
[
âˆ’
2


4
]
)
=
[
1
]
, which conflicts with 
[
âˆ’
2
]
, so 
ğ’œ
Mean
 is not non-conflicting.

âœ“â€‰ Linear under scaling.

For any 
ğ’„
âˆˆ
â„
ğ‘š
, 
ğ’œ
Mean
â¢
(
diag
(
ğ’„
)
â‹…
ğ½
)
=
1
ğ‘š
â¢
ğ½
âŠ¤
â‹…
ğ’„
, which is linear in 
ğ’„
. 
ğ’œ
Mean
 is therefore linear under scaling.

âœ“â€‰ Weighted.

By (25), 
ğ’œ
Mean
 is weighted with constant weighting equal to 
1
ğ‘š
â¢
ğŸ
.

C.2MGDA

The optimization algorithm presented in DÃ©sidÃ©ri (2012), called MGDA, is tied to a particular method for aggregating the gradients. We thus refer to this aggregator as 
ğ’œ
MGDA
. The dual problem of this method was also introduced independently in Fliege & Svaiter (2000). We show the equivalence between the two solutions to make the analysis of 
ğ’œ
MGDA
 easier.

For all 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, the aggregation described in DÃ©sidÃ©ri (2012) is defined as

	
ğ’œ
MGDA
â¢
(
ğ½
)
	
=
ğ½
âŠ¤
â‹…
ğ’˜
		
(26)

	
with
ğ’˜
	
âˆˆ
arg
â¢
min
ğŸ
â¢
ğ’—
:


ğŸ
âŠ¤
â¢
ğ’—
=
1
â¡
â€–
ğ½
âŠ¤
â¢
ğ’—
â€–
2
		
(27)

In Equation (3) of Fliege & Svaiter (2000), the following problem is studied:

	
min
ğ›¼
âˆˆ
â„
,
ğ’™
âˆˆ
â„
ğ‘›
:


ğ½
â¢
ğ’™
â¢
ğ›¼
â¢
ğŸ
â¡
ğ›¼
+
1
2
â¢
â€–
ğ’™
â€–
2
		
(28)

We show that the problems in (27) and (28) are dual to each other. Furthermore, the duality gap is null since this is a convex problem. The Lagrangian of the problem in (28) is given by 
â„’
â¢
(
ğ›¼
,
ğ’™
,
ğ
)
=
ğ›¼
+
1
2
â¢
â€–
ğ’™
â€–
2
âˆ’
ğ
âŠ¤
â¢
(
ğ›¼
â¢
ğŸ
âˆ’
ğ½
â¢
ğ’™
)
. Differentiating w.r.t. 
ğ›¼
 and 
ğ’™
 gives respectively 
1
âˆ’
ğŸ
âŠ¤
â¢
ğ
 and 
ğ’™
+
ğ½
âŠ¤
â¢
ğ
. The dual problem is obtained by setting those two to 
ğŸ
 and then maximizing the Lagrangian on 
ğŸ
â¢
ğ
 and 
ğ›¼
, i.e.

		
arg
â¢
max
ğ›¼
,
ğŸ
â¢
ğ
:


ğŸ
âŠ¤
â¢
ğ
=
1
â¡
ğ›¼
+
1
2
â¢
â€–
ğ½
âŠ¤
â¢
ğ
â€–
2
âˆ’
ğ
âŠ¤
â¢
(
ğ›¼
â¢
ğŸ
+
ğ½
â¢
ğ½
âŠ¤
â¢
ğ
)
	
	
=
	
arg
â¢
max
ğ›¼
,
ğŸ
â¢
ğ
:


ğŸ
âŠ¤
â¢
ğ
=
1
â¡
ğ›¼
+
1
2
â¢
â€–
ğ½
âŠ¤
â¢
ğ
â€–
2
âˆ’
ğ›¼
â¢
ğ
âŠ¤
â¢
ğŸ
âˆ’
ğ
âŠ¤
â¢
ğ½
â¢
ğ½
âŠ¤
â¢
ğ
	
	
=
	
arg
â¢
min
ğŸ
â¢
ğ
:


ğŸ
âŠ¤
â¢
ğ
=
1
â¡
1
2
â¢
â€–
ğ½
âŠ¤
â¢
ğ
â€–
2
	

Therefore, (27) and (28) are equivalent, with 
ğ’™
=
âˆ’
ğ½
âŠ¤
â¢
ğ’˜
.

âœ“â€‰ Non-conflicting.

Observe that since in (28), 
ğ›¼
=
0
 and 
ğ’™
=
ğŸ
 is feasible, the objective is non-positive and therefore 
ğ›¼
â¢
0
. Substituting 
ğ’™
=
âˆ’
ğ½
âŠ¤
â¢
ğ’˜
 in 
ğ½
â‹…
ğ’™
â¢
ğ›¼
â¢
ğŸğŸ
 yields 
ğŸ
â¢
ğ½
â¢
ğ½
âŠ¤
â¢
ğ’˜
, i.e. 
ğŸ
â¢
ğ½
â‹…
ğ’œ
MGDA
â¢
(
ğ½
)
, so 
ğ’œ
MGDA
 is non-conflicting.

âœ—â€‰ Linear under scaling.

With 
ğ½
=
[
2
	
0


0
	
2


ğ‘
	
ğ‘
]
, if 
0
â¢
ğ‘
â¢
1
, 
ğ’œ
MGDA
â¢
(
ğ½
)
=
[
ğ‘


ğ‘
]
. However, if 
ğ‘
â¢
1
, 
ğ’œ
MGDA
â¢
(
ğ½
)
=
[
1


1
]
. This is not affine in 
ğ‘
, so 
ğ’œ
MGDA
 is not linear under scaling. In particular, if any row of 
ğ½
 is 
ğŸ
, 
ğ’œ
MGDA
â¢
(
ğ½
)
=
ğŸ
. This implies that the optimization will stop whenever one objective has converged.

âœ“â€‰ Weighted.

By (26), 
ğ’œ
MGDA
 is weighted.

C.3DualProj

The projection of a gradient of interest onto a dual cone was first described in Lopez-Paz & Ranzato (2017). When this gradient is the average of the rows of the Jacobian, we call this aggregator 
ğ’œ
DualProj
. Formally,

	
ğ’œ
DualProj
â¢
(
ğ½
)
=
1
ğ‘š
â‹…
ğœ‹
ğ½
â¢
(
ğ½
âŠ¤
â‹…
ğŸ
)
		
(29)

where 
ğœ‹
ğ½
 is the projection operator defined in (3).

âœ“â€‰ Non-conflicting.

By the constraint in (3), 
ğ’œ
DualProj
 is non-conflicting.

âœ—â€‰ Linear under scaling.

With 
ğ½
=
[
2
	
0


âˆ’
2
â¢
ğ‘
	
2
â¢
ğ‘
]
, if 
ğ‘
â¢
1
, 
ğ’œ
DualProj
â¢
(
ğ½
)
=
[
0


ğ‘
]
. However, if 
0.5
â¢
ğ‘
â¢
1
, 
ğ’œ
DualProj
â¢
(
ğ½
)
=
[
1
âˆ’
ğ‘


ğ‘
]
. This is not affine in 
ğ‘
, so 
ğ’œ
DualProj
 is not linear under scaling.

âœ“â€‰ Weighted.

By Proposition 1, 
ğ’œ
DualProj
â¢
(
ğ½
)
=
1
ğ‘š
â¢
ğ½
âŠ¤
â‹…
ğ’˜
, with 
ğ’˜
âˆˆ
arg
â¢
min
ğŸ
â¢
ğ’—
â¡
ğ’—
âŠ¤
â¢
ğ½
â¢
ğ½
âŠ¤
â¢
ğ’—
. 
ğ’œ
DualProj
 is thus weighted.

C.4PCGrad

ğ’œ
PCGrad
 is described in Yu et al. (2020). It projects each gradient onto the orthogonal hyperplane of other gradients in case of conflict with them, iteratively and in random order. When 
ğ‘š
â¢
2
, 
ğ’œ
PCGrad
 is deterministic and satisfies 
ğ’œ
PCGrad
=
ğ‘š
â‹…
ğ’œ
UPGrad
. Therefore, in this case, it satisfies all three properties. When 
ğ‘š
â¢
>
â¢
2
, 
ğ’œ
PCGrad
 is non-deterministic, so 
ğ’œ
PCGrad
â¢
(
ğ½
)
 is a random vector.

For any index 
ğ‘–
âˆˆ
[
ğ‘š
]
, let 
ğ’ˆ
ğ‘–
=
ğ½
âŠ¤
â‹…
ğ’†
ğ‘–
 and let 
ğ©
â¢
(
ğ‘–
)
 be a random vector distributed uniformly on the set of permutations of the elements in 
[
ğ‘š
]
âˆ–
{
ğ‘–
}
. For instance, if 
ğ‘š
=
3
, 
ğ©
â¢
(
2
)
=
[
1
	
3
]
âŠ¤
 with probability 
0.5
 and 
ğ©
â¢
(
2
)
=
[
3
	
1
]
âŠ¤
 with probability 
0.5
. For notation convenience, whenever 
ğ‘–
 is clear from context, we denote 
ğ‘—
ğ‘˜
=
ğ©
â¢
(
ğ‘–
)
ğ‘˜
. The iterative projection of 
ğ’œ
PCGrad
 is then defined recursively as:

	
ğ’ˆ
ğ‘–
,
1
PC
	
=
ğ’ˆ
ğ‘–
		
(30)

	
ğ’ˆ
ğ‘–
,
ğ‘˜
+
1
PC
	
=
ğ’ˆ
ğ‘–
,
ğ‘˜
PC
âˆ’
ğŸ™
â¢
{
ğ’ˆ
ğ‘–
,
ğ‘˜
PC
â‹…
ğ’ˆ
ğ‘—
ğ‘˜
â¢
<
â¢
0
}
â¢
ğ’ˆ
ğ‘–
,
ğ‘˜
PC
â‹…
ğ’ˆ
ğ‘—
ğ‘˜
â€–
ğ’ˆ
ğ‘—
ğ‘˜
â€–
2
â¢
ğ’ˆ
ğ‘—
ğ‘˜
		
(31)

We noticed that an equivalent formulation to the conditional projection of (31) is the projection onto the dual cone of 
{
ğ’ˆ
ğ‘—
ğ‘˜
}
:

	
ğ’ˆ
ğ‘–
,
ğ‘˜
+
1
PC
=
ğœ‹
ğ’ˆ
ğ‘—
ğ‘˜
âŠ¤
â¢
(
ğ’ˆ
ğ‘–
,
ğ‘˜
PC
)
		
(32)

Finally, the aggregation is given by

	
ğ’œ
PCGrad
â¢
(
ğ½
)
=
âˆ‘
ğ‘–
=
1
ğ‘š
ğ’ˆ
ğ‘–
,
ğ‘š
PC
.
		
(33)
âœ—â€‰ Non-conflicting.

If 
ğ½
=
[
1
	
0


0
	
1


âˆ’
0.5
	
âˆ’
1
]
, the only non-conflicting direction is 
ğŸ
. However, 
ğ’œ
PCGrad
â¢
(
ğ½
)
 is uniform over the set 
{
[
0.4


0.2
]
,
[
0.8


0.2
]
,
[
0.4


âˆ’
0.2
]
,
[
0.8


âˆ’
0.2
]
}
, i.e. 
ğ’œ
PCGrad
â¢
(
ğ½
)
 is in the dual cone of the rows of 
ğ½
 with probability 
0
. 
ğ’œ
PCGrad
 is thus not non-conflicting. Here, 
ğ”¼
â¢
[
ğ’œ
PCGrad
â¢
(
ğ½
)
]
=
[
0.6
	
0
]
âŠ¤
, so 
ğ’œ
PCGrad
 is neither non-conflicting in expectation.

âœ“â€‰ Linear under scaling.

To show that 
ğ’œ
PCGrad
 is linear under scaling, let 
ğŸ
â¢
ğ’„
âˆˆ
â„
ğ‘š
, 
ğ’ˆ
ğ‘–
â€²
=
ğ‘
ğ‘–
â¢
ğ’ˆ
ğ‘–
, 
ğ’ˆ
ğ‘–
,
1
â€²
â£
 PC
=
ğ’ˆ
ğ‘–
â€²
 and 
ğ’ˆ
ğ‘–
,
ğ‘˜
+
1
â€²
â£
 PC
=
ğœ‹
ğ’ˆ
ğ‘—
ğ‘˜
â€²
â£
âŠ¤
â¢
(
ğ’ˆ
ğ‘–
,
ğ‘˜
â€²
â£
 PC
)
. We show by induction that 
ğ’ˆ
ğ‘–
,
ğ‘˜
â€²
â£
 PC
=
ğ‘
ğ‘–
â¢
ğ’ˆ
ğ‘–
,
ğ‘˜
PC
.

The base case is given by 
ğ’ˆ
ğ‘–
,
1
â€²
â£
 PC
=
ğ’ˆ
ğ‘–
â€²
=
ğ‘
ğ‘–
â¢
ğ’ˆ
ğ‘–
=
ğ‘
ğ‘–
â¢
ğ’ˆ
ğ‘–
,
1
PC
.

Then, assuming the induction hypothesis 
ğ’ˆ
ğ‘–
,
ğ‘˜
â€²
â£
 PC
=
ğ‘
ğ‘–
â¢
ğ’ˆ
ğ‘–
,
ğ‘˜
PC
, we show 
ğ’ˆ
ğ‘–
,
ğ‘˜
+
1
â€²
â£
 PC
=
ğ‘
ğ‘–
â¢
ğ’ˆ
ğ‘–
,
ğ‘˜
+
1
PC
:

	
ğ’ˆ
ğ‘–
,
ğ‘˜
+
1
â€²
â£
 PC
=
ğœ‹
ğ‘
ğ‘—
ğ‘˜
â¢
ğ’ˆ
ğ‘—
ğ‘˜
âŠ¤
â¢
(
ğ‘
ğ‘–
â¢
ğ’ˆ
ğ‘–
,
ğ‘˜
 PC
)
	
(
Induction hypothesis
)
	
	
ğ’ˆ
ğ‘–
,
ğ‘˜
+
1
â€²
â£
 PC
=
ğ‘
ğ‘–
â¢
ğœ‹
ğ’ˆ
ğ‘—
ğ‘˜
âŠ¤
â¢
(
ğ’ˆ
ğ‘–
,
ğ‘˜
 PC
)
	
(
0
â¢
<
â¢
ğ‘
ğ‘–
â¢
 and 
â¢
0
â¢
<
â¢
ğ‘
ğ‘—
ğ‘˜
)
	
	
ğ’ˆ
ğ‘–
,
ğ‘˜
+
1
â€²
â£
 PC
=
ğ‘
ğ‘–
â¢
ğ’ˆ
ğ‘–
,
ğ‘˜
+
1
PC
	
(
By (
32
)
)
	

Therefore 
ğ’œ
PCGrad
â¢
(
diag
(
ğ’„
)
â‹…
ğ½
)
=
âˆ‘
ğ‘–
=
1
ğ‘š
ğ‘
ğ‘–
â¢
ğ’ˆ
ğ‘–
,
ğ‘š
PC
, so it can be written as 
ğ’œ
PCGrad
â¢
(
diag
(
ğ’„
)
â‹…
ğ½
)
=
ğ”
âŠ¤
â‹…
ğ’„
 with 
ğ”
=
[
ğ’ˆ
1
,
ğ‘š
PC
	
â‹¯
	
ğ’ˆ
ğ‘š
,
ğ‘š
PC
]
âŠ¤
. Therefore, 
ğ’œ
PCGrad
 is linear under scaling.

âœ“â€‰ Weighted.

For all 
ğ‘–
, 
ğ’ˆ
ğ‘–
,
ğ‘š
PC
 is always a random linear combination of rows of 
ğ½
. 
ğ’œ
PCGrad
 is thus weighted.

C.5GradDrop

The aggregator used by the GradDrop layer, which we denote 
ğ’œ
GradDrop
, is described in Chen et al. (2020). It is non-deterministic, so 
ğ’œ
GradDrop
â¢
(
ğ½
)
 is a random vector. Given 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, let 
|
ğ½
|
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
 be the element-wise absolute value of 
ğ½
. Let 
ğ‘ƒ
=
1
2
â¢
(
ğŸ
+
ğ½
âŠ¤
â‹…
ğŸ
|
ğ½
|
âŠ¤
â‹…
ğŸ
)
âˆˆ
â„
ğ‘›
, where the division is element-wise. Each coordinate 
ğ‘–
âˆˆ
[
ğ‘›
]
 is independently assigned to the set 
â„
+
 with probability 
ğ‘ƒ
ğ‘–
 and to the set 
â„
âˆ’
 otherwise. The aggregation at coordinate 
ğ‘–
âˆˆ
â„
+
 is given by the sum of all positive 
ğ½
ğ‘—
â¢
ğ‘–
, for 
ğ‘—
âˆˆ
[
ğ‘š
]
. The aggregation at coordinate 
ğ‘–
âˆˆ
â„
âˆ’
 is given by the sum of all negative 
ğ½
ğ‘—
â¢
ğ‘–
, for 
ğ‘—
âˆˆ
[
ğ‘š
]
. Formally,

	
ğ’œ
GradDrop
â¢
(
ğ½
)
	
=
(
âˆ‘
ğ‘–
âˆˆ
â„
+
ğ’†
ğ‘–
â¢
âˆ‘
ğ‘—
âˆˆ
[
ğ‘š
]
:


ğ½
ğ‘—
â¢
ğ‘–
â¢
>
â¢
0
ğ½
ğ‘—
â¢
ğ‘–
)
+
(
âˆ‘
ğ‘–
âˆˆ
â„
âˆ’
ğ’†
ğ‘–
â¢
âˆ‘
ğ‘—
âˆˆ
[
ğ‘š
]
:


ğ½
ğ‘—
â¢
ğ‘–
â¢
<
â¢
0
ğ½
ğ‘—
â¢
ğ‘–
)
		
(34)
âœ—â€‰ Non-conflicting.

If 
ğ½
=
[
âˆ’
2


1
]
, then 
ğ‘ƒ
=
[
1
/
3
]
. Therefore, 
â„™
â¢
[
ğ’œ
GradDrop
â¢
(
ğ½
)
=
[
âˆ’
2
]
]
=
2
/
3
 and 
â„™
â¢
[
ğ’œ
GradDrop
â¢
(
ğ½
)
=
[
1
]
]
=
1
/
3
, i.e. 
ğ’œ
GradDrop
â¢
(
ğ½
)
 is in the dual cone of the rows of 
ğ½
 with probability 
0
. Therefore, 
ğ’œ
GradDrop
 is not non-conflicting. Here, 
ğ”¼
â¢
[
ğ’œ
GradDrop
â¢
(
ğ½
)
]
=
[
âˆ’
1
]
âŠ¤
, so 
ğ’œ
GradDrop
 is neither non-conflicting in expectation.

âœ—â€‰ Linear under scaling.

If 
ğ½
=
[
1
	
âˆ’
1


âˆ’
1
	
1
]
, then 
ğ‘ƒ
=
1
2
â‹…
ğŸ
 and the aggregation is one of the four vectors 
[
Â±
1
	
Â±
1
]
âŠ¤
 with equal probability. Scaling the first line of 
ğ½
 by 
2
 yields 
ğ½
=
[
2
	
âˆ’
2


âˆ’
1
	
1
]
 and 
ğ‘ƒ
=
[
2
/
3
	
1
/
3
]
âŠ¤
, which cannot lead to a uniform distribution over four elements. Therefore, 
ğ’œ
GradDrop
 is not linear under scaling.

âœ—â€‰ Weighted.

With 
ğ½
=
[
1
	
âˆ’
1


âˆ’
1
	
1
]
, the span of 
ğ½
 does not include 
[
1
	
1
]
âŠ¤
 nor 
[
âˆ’
1
	
âˆ’
1
]
âŠ¤
. Therefore, 
ğ’œ
GradDrop
 is not weighted.

C.6IMTL-G

In Liu et al. (2021b), the authors describe a method to impartially balance gradients by weighting them. Let 
ğ’ˆ
ğ‘–
 be the 
ğ‘–
â€™th row of 
ğ½
 and let 
ğ’–
ğ‘–
=
ğ’ˆ
ğ‘–
â€–
ğ’ˆ
ğ‘–
â€–
. They want to find a combination 
ğ’ˆ
=
âˆ‘
ğ‘–
=
1
ğ‘š
ğ›¼
ğ‘–
â¢
ğ’ˆ
ğ‘–
 such that 
ğ’ˆ
âŠ¤
â¢
ğ’–
ğ‘–
 is equal for all 
ğ‘–
. Let 
ğ‘ˆ
=
[
ğ’–
1
âˆ’
ğ’–
2
	
â€¦
	
ğ’–
1
âˆ’
ğ’–
ğ‘š
]
âŠ¤
, 
ğ·
=
[
ğ’ˆ
1
âˆ’
ğ’ˆ
2
	
â€¦
	
ğ’ˆ
1
âˆ’
ğ’ˆ
ğ‘š
]
âŠ¤
. If 
ğœ¶
2
:
ğ‘š
=
[
ğ›¼
2
	
â€¦
	
ğ›¼
ğ‘š
]
âŠ¤
, then 
ğœ¶
2
:
ğ‘š
=
(
ğ‘ˆ
â¢
ğ·
âŠ¤
)
âˆ’
1
â¢
ğ‘ˆ
â‹…
ğ’ˆ
1
 and 
ğ›¼
1
=
1
âˆ’
âˆ‘
ğ‘–
=
2
ğ‘š
ğ›¼
ğ‘–
. Notice that this is defined only when the gradients are linearly independent. Thus, this is not strictly speaking an aggregator since it can only be computed on matrices of rank 
ğ‘š
. We thus propose a generalization defined for matrices of any rank that is equivalent when the matrix has rank 
ğ‘š
. In the original formulation, requiring 
ğ’ˆ
âŠ¤
â¢
ğ’–
ğ‘–
 to be equal to some 
ğ‘
âˆˆ
â„
 for all 
ğ‘–
, is equivalent to requiring that for all 
ğ‘–
, 
ğ’ˆ
âŠ¤
â¢
ğ’ˆ
ğ‘–
 is equal to 
ğ‘
â¢
â€–
ğ’ˆ
ğ‘–
â€–
. Writing 
ğ’ˆ
=
ğ½
âŠ¤
â¢
ğœ¶
 and letting 
ğ’…
âˆˆ
â„
ğ‘š
 be the vector of norms of the rows of 
ğ½
, the objective is thus to find 
ğœ¶
 satisfying 
ğ½
â¢
ğ½
âŠ¤
â¢
ğœ¶
âˆ
ğ’…
. Besides, to match the original formulation, the elements of 
ğœ¶
 should sum to 
1
.

Letting 
(
ğ½
â¢
ğ½
âŠ¤
)
â€ 
 be the Moore-Penrose pseudo inverse of 
ğ½
â¢
ğ½
âŠ¤
, we define

	
ğ’œ
IMTL-G
â¢
(
ğ½
)
	
=
ğ½
âŠ¤
â‹…
ğ’˜
		
(35)

	
with
ğ’˜
	
=
{
ğ’—
ğŸ
âŠ¤
â¢
ğ’—
,
	
if 
â¢
ğŸ
âŠ¤
â¢
ğ’—
â‰ 
ğŸ


ğŸ
,
	
otherwise
		
(36)

	
and
ğ’—
	
=
(
ğ½
â¢
ğ½
âŠ¤
)
â€ 
â‹…
ğ’…
.
		
(37)
âœ—â€‰ Non-conflicting.

If 
ğ½
=
[
1
	
âˆ’
1
	
âˆ’
1
]
âŠ¤
, then 
ğ’…
=
[
1
	
1
	
1
]
âŠ¤
, 
ğ½
â¢
ğ½
âŠ¤
=
[
1
	
âˆ’
1
	
âˆ’
1


âˆ’
1
	
1
	
1


âˆ’
1
	
1
	
1
]
, and thus 
ğ’—
=
1
9
â¢
[
âˆ’
1
	
1
	
1
]
âŠ¤
. Therefore, 
ğ’˜
=
[
âˆ’
1
	
1
	
1
]
âŠ¤
, 
ğ’œ
IMTL-G
â¢
(
ğ½
)
=
[
âˆ’
3
]
âŠ¤
 and 
ğ½
â‹…
ğ’œ
IMTL-G
â¢
(
ğ½
)
=
[
âˆ’
3
	
3
	
3
]
âŠ¤
. 
ğ’œ
IMTL-G
 is thus not non-conflicting.

It should be noted that when 
ğ½
 has rank 
ğ‘š
, 
ğ’œ
IMTL-G
 seems to be non-conflicting. Thus, it would be possible to make a different non-conflicting generalization, for instance, by deciding 
ğŸ
 when 
ğ½
 is not full rank.

âœ—â€‰ Linear under scaling.

With 
ğ½
=
[
ğ‘
	
0


0
	
1
]
 and 
ğ‘
â¢
>
â¢
0
, we have 
ğ’—
=
[
1
/
ğ‘
2
	
0


0
	
1
]
â‹…
[
ğ‘


1
]
=
[
1
/
ğ‘


1
]
 and 
ğ’œ
IMTL-G
â¢
(
ğ½
)
=
[
ğ‘
	
0


0
	
1
]
â‹…
[
1
/
ğ‘


1
]
â‹…
1
1
ğ‘
+
1
=
1
1
ğ‘
+
1
â‹…
[
1


1
]
. This is not affine in 
ğ‘
, so 
ğ’œ
IMTL-G
 is not linear under scaling.

âœ“â€‰ Weighted.

By (35), 
ğ’œ
IMTL-G
 is weighted.

C.7CAGrad

ğ’œ
CAGrad
 is described in Liu et al. (2021a). It is parameterized by 
ğ‘
âˆˆ
[
0
,
1
[
. If 
ğ‘
=
0
, this is equivalent to 
ğ’œ
Mean
. Therefore, we restrict our analysis to the case 
ğ‘
â¢
>
â¢
0
. For any 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, let 
ğ’ˆ
Â¯
 be the average gradient 
1
ğ‘š
â¢
ğ½
âŠ¤
â‹…
ğŸ
, and let 
ğ’†
ğ‘–
âŠ¤
â¢
ğ½
 denote the 
ğ‘–
â€™th row of 
ğ½
. The aggregation is then defined as

	
ğ’œ
CAGrad
â¢
(
ğ½
)
âˆˆ
arg
â¢
max
ğ’…
âˆˆ
â„
ğ‘›
:


â€–
ğ’…
âˆ’
ğ’ˆ
Â¯
â€–
â¢
ğ‘
â¢
â€–
ğ’ˆ
Â¯
â€–
â¡
min
ğ‘–
âˆˆ
[
ğ‘š
]
â¡
ğ’†
ğ‘–
âŠ¤
â¢
ğ½
â¢
ğ’…
		
(38)
âœ—â€‰ Non-conflicting.

Let 
ğ½
=
[
2
	
0


âˆ’
2
â¢
ğ‘
âˆ’
2
	
2
]
, with 
ğ‘
 satisfying 
âˆ’
ğ‘
+
ğ‘
â¢
ğ‘
2
+
1
â¢
<
â¢
0
. We have 
ğ’ˆ
Â¯
=
[
âˆ’
ğ‘
	
1
]
âŠ¤
 and 
â€–
ğ’ˆ
Â¯
â€–
=
ğ‘
2
+
1
. Observe that any 
ğ’…
âˆˆ
â„
ğ‘›
 satisfying the constraint 
â€–
ğ’…
âˆ’
ğ’ˆ
Â¯
â€–
â¢
ğ‘
â¢
â€–
ğ’ˆ
Â¯
â€–
 has first coordinate at most 
âˆ’
ğ‘
+
ğ‘
â¢
ğ‘
2
+
1
. Because 
âˆ’
ğ‘
+
ğ‘
â¢
ğ‘
2
+
1
â¢
<
â¢
0
, any feasible 
ğ‘‘
 has a negative first coordinate, making 
ğ‘‘
 conflict with the first row of 
ğ½
. For any 
ğ‘
âˆˆ
[
0
,
1
[
, 
âˆ’
ğ‘
+
ğ‘
â¢
ğ‘
2
+
1
â¢
<
â¢
0
 is equivalent to 
ğ‘
2
1
âˆ’
ğ‘
2
â¢
<
â¢
ğ‘
. Thus, this provides a counter-example to the non-conflicting property for any 
ğ‘
âˆˆ
[
0
,
1
[
, i.e. 
ğ’œ
CAGrad
 is not non-conflicting.

If we generalize to the case 
ğ‘
â¢
1
, as suggested in the original paper, then 
ğ’…
=
ğŸ
 becomes feasible, which yields 
min
ğ‘–
âˆˆ
[
ğ‘š
]
â¡
ğ’†
ğ‘–
âŠ¤
â¢
ğ½
â¢
ğ’…
=
0
. Therefore the optimal 
ğ’…
 satisfies 
0
â¢
min
ğ‘–
âˆˆ
[
ğ‘š
]
â¡
ğ’†
ğ‘–
âŠ¤
â¢
ğ½
â¢
ğ’…
, i.e. 
ğŸ
â¢
ğ½
â¢
ğ’…
. With 
ğ‘
â¢
1
, 
ğ’œ
CAGrad
 would thus be non-conflicting.

âœ—â€‰ Linear under scaling (sketch of proof).

Let 
ğ½
=
[
2
	
0


0
	
2
â¢
ğ‘
]
, then 
ğ’ˆ
Â¯
=
[
1
	
ğ‘
]
âŠ¤
 and 
â€–
ğ’ˆ
Â¯
â€–
=
1
+
ğ‘
2
. One can show that the constraint 
â€–
ğ’…
âˆ’
ğ’ˆ
Â¯
â€–
â¢
ğ‘
â¢
â€–
ğ’ˆ
Â¯
â€–
 needs to be satisfied with equality since, otherwise, we can scale 
ğ’…
 to make the objective larger. Substituting 
ğ½
 in 
min
ğ‘–
âˆˆ
[
ğ‘š
]
â¡
ğ’†
ğ‘–
âŠ¤
â¢
ğ½
â¢
ğ’…
 yields 
2
â¢
min
â¡
(
ğ‘‘
1
,
ğ‘
â¢
ğ‘‘
2
)
. For any 
ğ‘
 satisfying 
ğ‘
â¢
1
+
ğ‘
2
+
1
â¢
<
â¢
ğ‘
2
, it can be shown that the optimal 
ğ’…
 satisfies 
ğ‘‘
1
â¢
<
â¢
ğ‘
â¢
ğ‘‘
2
. In that case the inner minimum over 
ğ‘–
 is 
2
â¢
ğ‘‘
1
 and, to satisfy 
â€–
ğ’…
âˆ’
ğ’ˆ
Â¯
â€–
=
ğ‘
â¢
â€–
ğ’ˆ
Â¯
â€–
, the KKT conditions over the Lagrangian yield 
ğ’…
âˆ’
ğ’ˆ
Â¯
âˆ
âˆ‡
ğ’…
ğ‘‘
1
=
[
1
	
0
]
âŠ¤
. This yields 
ğ’…
=
[
ğ‘
â‹…
â€–
ğ’ˆ
Â¯
â€–
+
1


ğ‘
]
=
[
ğ‘
â¢
1
+
ğ‘
2
+
1


ğ‘
]
. This is not affine in 
ğ‘
; therefore, 
ğ’œ
CAGrad
 is not linear under scaling.

âœ“â€‰ Weighted.

In Liu et al. (2021a), 
ğ’œ
CAGrad
 is formulated via its dual: 
ğ’œ
CAGrad
â¢
(
ğ½
)
=
1
ğ‘š
â¢
ğ½
âŠ¤
â¢
(
ğŸ
+
ğ‘
â¢
â€–
ğ½
âŠ¤
â¢
ğŸ
â€–
â€–
ğ½
âŠ¤
â¢
ğ’˜
â€–
â¢
ğ’˜
)
, with 
ğ’˜
âˆˆ
arg
â¢
min
ğ’˜
âˆˆ
Î”
â¢
(
ğ‘š
)
â¡
ğŸ
âŠ¤
â¢
ğ½
â¢
ğ½
âŠ¤
â¢
ğ’˜
+
ğ‘
â‹…
â€–
ğ½
âŠ¤
â¢
ğŸ
â€–
â‹…
â€–
ğ½
âŠ¤
â¢
ğ’˜
â€–
, where is 
Î”
â¢
(
ğ‘š
)
 the probability simplex of dimension 
ğ‘š
. Therefore, 
ğ’œ
CAGrad
 is weighted.

C.8RGW

ğ’œ
RGW
 is defined in Lin et al. (2021) as the weighted sum of the rows of the input matrix, with a random weighting. The weighting is obtained by sampling 
ğ‘š
 i.i.d. normally distributed random variables and applying a softmax. Formally,

	
ğ’œ
RGW
â¢
(
ğ½
)
	
=
ğ½
âŠ¤
â‹…
softmax
â¢
(
ğ°
)
		
(39)

	
with
ğ°
	
âˆ¼
ğ’©
â¢
(
ğŸ
,
ğ¼
)
		
(40)
âœ—â€‰ Non-conflicting.

When 
ğ½
=
[
1


âˆ’
2
]
, the only non-conflicting solution is 
ğŸ
. However, 
â„™
â¢
[
ğ’œ
RGW
â¢
(
ğ½
)
=
ğŸ
]
=
0
, i.e. 
ğ’œ
RGW
â¢
(
ğ½
)
 is in the dual cone of the rows of 
ğ½
 with probability 
0
. 
ğ’œ
RGW
 is thus not non-conflicting. Here,

	
ğ”¼
â¢
[
ğ’œ
RGW
â¢
(
ğ½
)
]
	
=
ğ”¼
â¢
[
ğ‘’
w
1
âˆ’
2
â¢
ğ‘’
w
2
ğ‘’
w
1
+
ğ‘’
w
2
]
	
(
By (
39
)
)
	
		
=
âˆ’
ğ”¼
â¢
[
ğ‘’
w
1
ğ‘’
w
1
+
ğ‘’
w
2
]
	
(
ğ°
âˆ¼
ğ’©
â¢
(
ğŸ
,
ğ¼
)
)
	
		
<
â¢
0
	
(
0
â¢
ğ‘’
ğ°
)
	

so 
ğ’œ
RGW
 is neither non-conflicting in expectation.

âœ“â€‰ Linear under scaling.

ğ’œ
RGW
â¢
(
diag
(
ğ’„
)
â‹…
ğ½
)
=
(
diag
(
ğ’„
)
â‹…
ğ½
)
âŠ¤
â‹…
softmax
â¢
(
ğ°
)
=
ğ½
âŠ¤
â‹…
diag
(
ğ’„
)
â‹…
softmax
â¢
(
ğ°
)
=
ğ½
âŠ¤
â‹…
diag
(
softmax
â¢
(
ğ°
)
)
â‹…
ğ’„
. We thus have 
ğ’œ
RGW
â¢
(
diag
(
ğ’„
)
â‹…
ğ½
)
=
ğ”
âŠ¤
â‹…
ğ’„
 with 
ğ”
âŠ¤
=
ğ½
âŠ¤
â‹…
diag
(
softmax
â¢
(
ğ°
)
)
. Therefore, 
ğ’œ
RGW
 is linear under scaling.

âœ“â€‰ Weighted.

By (39), 
ğ’œ
RGW
 is weighted.

C.9Nash-MTL

Nash-MTL is described in Navon et al. (2022). Unfortunately, we were not able to verify the proof of Claim 3.1, and we believe that the official implementation of Nash-MTL may mismatch the desired objective by which it is defined. Therefore, we only analyze the initial objective even though our experiments for this aggregator are conducted with the official implementation.

Let 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
 and 
ğœ€
â¢
>
â¢
0
. Let also 
ğµ
ğœ€
=
{
ğ’…
âˆˆ
â„
ğ‘›
:
â€–
ğ’…
â€–
â¢
ğœ€
,
ğŸ
â¢
ğ½
â¢
ğ’…
}
. With 
ğ’†
ğ‘–
âŠ¤
â¢
ğ½
 denoting the 
ğ‘–
â€™th row of 
ğ½
, 
ğ’œ
Nash-MTL
 is then defined as

	
ğ’œ
Nash-MTL
â¢
(
ğ½
)
=
arg
â¢
max
ğ’…
âˆˆ
ğµ
ğœ€
â¢
âˆ‘
ğ‘–
âˆˆ
[
ğ‘š
]
log
â¡
(
ğ’†
ğ‘–
âŠ¤
â¢
ğ½
â¢
ğ’…
)
		
(41)
âœ“â€‰ Non-conflicting.

By the constraint, 
ğ’œ
Nash-MTL
 is non-conflicting.

âœ—â€‰ Linear under scaling.

If an aggregator 
ğ’œ
 is linear under scaling, it should be the case that 
ğ’œ
â¢
(
ğ‘
â¢
ğ½
)
=
ğ‘
â¢
ğ’œ
â¢
(
ğ½
)
 for any scalar 
ğ‘
â¢
>
â¢
0
 and any 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
. However, 
log
â¡
(
ğ‘
â¢
ğ’†
ğ‘–
âŠ¤
â¢
ğ½
â¢
ğ’…
)
=
log
â¡
(
ğ’†
ğ‘–
âŠ¤
â¢
ğ½
â¢
ğ’…
)
+
log
â¡
(
ğ‘
)
. This means that scaling by a scalar does not impact aggregation. Since this is not the trivial 
ğŸ
 aggregator, 
ğ’œ
Nash-MTL
 is not linear under scaling.

âœ“â€‰ Weighted.

Suppose towards contradiction that 
ğ’…
 is both optimal for (41) and not in the span of 
ğ½
âŠ¤
. Let 
ğ’…
â€²
 be the projection of 
ğ’…
 onto the span of 
ğ½
âŠ¤
. Since 
â€–
ğ’…
â€²
â€–
â¢
<
â¢
â€–
ğ’…
â€–
â¢
<
â¢
ğœ€
 and 
ğ½
â¢
ğ’…
=
ğ½
â¢
ğ’…
â€²
, we have 
ğ½
â¢
ğ’…
â¢
ğ½
â¢
(
â€–
ğ’…
â€–
â€–
ğ’…
â€²
â€–
â¢
ğ’…
â€²
)
, contradicting the optimality of 
ğ’…
. Therefore, 
ğ’œ
Nash-MTL
 is weighted.

C.10Aligned-MTL

The Aligned-MTL method for balancing the Jacobian is described in Senushkin et al. (2023). For simplicity, we fix the vector of preferences to 
1
ğ‘š
â¢
ğŸ
, but the proofs can be adapted for any non-trivial vector. Given 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
, let 
ğ‘‰
â¢
Î£
2
â¢
ğ‘‰
âŠ¤
 be the eigen-decomposition of 
ğ½
â¢
ğ½
âŠ¤
, let 
Î£
â€ 
 be the diagonal matrix whose non-zero elements are the inverse of corresponding non-zero diagonal elements of 
Î£
 and let 
ğœ
min
=
min
ğ‘–
âˆˆ
[
ğ‘š
]
,
Î£
ğ‘–
â¢
ğ‘–
â‰ 
0
â¡
Î£
ğ‘–
â¢
ğ‘–
. The aggregation is then defined as

	
ğ’œ
Aligned-MTL
â¢
(
ğ½
)
	
=
1
ğ‘š
â¢
ğ½
âŠ¤
â‹…
ğ’˜
		
(42)

	
with
ğ’˜
	
=
ğœ
min
â‹…
ğ‘‰
â¢
Î£
â€ 
â¢
ğ‘‰
âŠ¤
â‹…
ğŸ
		
(43)
âœ—â€‰ Non-conflicting.

If the SVD of 
ğ½
 is 
ğ‘‰
â¢
Î£
â¢
ğ‘ˆ
âŠ¤
, then 
ğ½
âŠ¤
â¢
ğ’˜
=
ğœ
min
â¢
ğ‘ˆ
â¢
ğ‘ƒ
â¢
ğ‘‰
âŠ¤
â¢
ğŸ
 with 
ğ‘ƒ
=
Î£
â€ 
â¢
Î£
 a diagonal projection matrix with 
1
s corresponding to non zero elements of 
Î£
 and 
0
s everywhere else. Further, 
ğ½
â‹…
ğ’œ
Aligned-MTL
â¢
(
ğ½
)
=
ğœ
min
ğ‘š
â‹…
ğ‘‰
â¢
Î£
â¢
ğ‘‰
âŠ¤
â¢
ğŸ
. If 
ğ‘‰
=
1
2
â¢
[
3
	
1


âˆ’
1
	
3
]
 and 
Î£
=
[
1
	
0


0
	
0
]
, we have 
ğ½
â‹…
ğ’œ
Aligned-MTL
â¢
(
ğ½
)
=
1
2
â‹…
ğ‘‰
â¢
Î£
â¢
ğ‘‰
âŠ¤
â¢
ğŸ
=
1
8
â¢
[
3
âˆ’
3
	
1
âˆ’
3
]
âŠ¤
 which is not non-negative. 
ğ’œ
Aligned-MTL
 is thus not non-conflicting.

âœ—â€‰ Linear under scaling.

If 
ğ½
=
[
1
	
0


0
	
ğ‘
]
, then 
ğ‘ˆ
=
ğ‘‰
=
ğ¼
, 
Î£
=
ğ½
. For 
0
â¢
<
â¢
ğ‘
â¢
1
, 
ğœ
min
=
ğ‘
, therefore 
ğ’œ
Aligned-MTL
â¢
(
ğ½
)
=
ğ‘
2
â‹…
ğŸ
. For 
1
â¢
<
â¢
ğ‘
, 
ğœ
min
=
1
 and therefore 
ğ’œ
Aligned-MTL
â¢
(
ğ½
)
=
1
2
â‹…
ğŸ
, which makes 
ğ’œ
Aligned-MTL
 not linear under scaling.

âœ“â€‰ Weighted.

By (42), 
ğ’œ
Aligned-MTL
 is weighted.

Appendix DExperimental settings

For all of our experiments, we used PyTorch (Paszke et al., 2019). We have developed an open-source library5 on top of it to enable Jacobian descent easily. This library is designed to be reusable for many other use cases than the experiments presented in our work. To separate them from the library, the experiments have been conducted with a different code repository6 mainly using PyTorch and our library.

D.1Learning rate selection

The learning rate has a very important impact on the speed of optimization. To make the comparisons as fair as possible, we always show the results corresponding to the best learning rate. We have selected the area under the loss curve as the criterion to compare the learning rates. This choice is arbitrary but seems to work well in practice: a lower area under the loss curve means the optimization is fast (quick loss decrease) and stable (few bumps in the loss curve). Concretely, for each random rerun and each aggregator, we first try 22 learning rates from 
10
âˆ’
5
 to 
10
2
, increasing by a factor 
10
1
3
 every time. The two best learning rates from this range then define a refined range of plausible good learning rates, going from the smallest of those two multiplied by 
10
âˆ’
1
3
 to the largest of those two multiplied by 
10
1
3
. This margin makes it unlikely for the best learning rate to lie out of the refined range. After this, 50 learning rates from the refined range are tried. These learning rates are evenly spaced in the exponent domain. The one with the best area under the loss curve is then selected and presented in the plots. For simplicity, we have always used a constant learning rate, i.e. no learning rate scheduler was used.

This approach has the advantage of being simple and precise, thus giving trustworthy results. However, it requires 72 trainings for each aggregator, random rerun, and dataset, i.e. a total of 
43776
 trainings for all of our experiments. For this reason, we have opted to work on small subsets of the original datasets.

D.2Random reruns and standard error of the mean

To get an idea of confidence in our results, every experiment is performed 8 times on a different seed and a different subset, of size 1024, of the training dataset. The seed used for run 
ğ‘–
âˆˆ
[
8
]
 is always simply set to 
ğ‘–
. Because each random rerun includes the full learning rate selection method described in Appendix D.1, it is sensible to consider the 8 sets of results as i.i.d. For each point of both the loss curves and the cosine similarity curves, we thus compute the estimated standard error of the mean with the usual formula 
1
8
â¢
âˆ‘
ğ‘–
âˆˆ
[
8
]
(
ğ‘£
ğ‘–
âˆ’
ğ‘£
Â¯
)
2
8
âˆ’
1
, where 
ğ‘£
ğ‘–
 is the value of a point of the curve for random rerun 
ğ‘–
, and 
ğ‘£
Â¯
 is the average value of this point over the 8 runs.

D.3Model architectures

In all experiments, the models are simple convolutional neural networks. All convolutions always have a stride of 1
Ã—
1, a kernel size of 3
Ã—
3, a learnable bias, and no padding. All linear layers always have a learnable bias. The activation function is the exponential linear unit (Clevert et al., 2015). The full architectures are given in Tables 2, 3, 4 and 5. Note that these architectures have been fixed arbitrarily, i.e. they were not optimized through some hyper-parameter selection. The weights of the model have been initialized with the default initialization scheme of PyTorch.

Table 2:Architecture used for SVHN
Conv2d (3 input channels, 16 output channels, 1 group), ELU 
Conv2d (16 input channels, 32 output channels, 16 groups)
MaxPool2d (stride of 2
Ã—
2, kernel size of 2
Ã—
2), ELU 
Conv2d (32 input channels, 32 output channels, 32 groups)
MaxPool2d (stride of 3
Ã—
3, kernel size of 3
Ã—
3), ELU, Flatten 
Linear (512 input features, 64 output features), ELU 
Linear (64 input features, 10 outputs)
Table 3:Architecture used for CIFAR-10
Conv2d (3 input channels, 32 output channels, 1 group), ELU 
Conv2d (32 input channels, 64 output channels, 32 groups)
MaxPool2d (stride of 2
Ã—
2, kernel size of 2
Ã—
 2), ELU 
Conv2d (64 input channels, 64 output channels, 64 groups)
MaxPool2d (stride of 3
Ã—
3, kernel size of 3
Ã—
3), ELU, Flatten 
Linear (1024 input features, 128 output features), ELU 
Linear (128 input features, 10 outputs)
Table 4:Architecture used for EuroSAT
Conv2d (3 input channels, 32 output channels, 1 group)
MaxPool2d (stride of 2
Ã—
2, kernel size of 2
Ã—
2), ELU 
Conv2d (32 input channels, 64 output channels, 32 groups)
MaxPool2d (stride of 2
Ã—
2, kernel size of 2
Ã—
2), ELU 
Conv2d (64 input channels, 64 output channels, 64 groups)
MaxPool2d (stride of 3
Ã—
3, kernel size of 3
Ã—
3), ELU, Flatten 
Linear (1024 input features, 128 output features), ELU 
Linear (128 input features, 10 outputs)
Table 5:Architecture used for MNIST, Fashion-MNIST and Kuzushiji-MNIST
Conv2d (1 input channel, 32 output channels, 1 group), ELU 
Conv2d (32 input channels, 64 output channels, 1 group)
MaxPool2d (stride of 2
Ã—
2, kernel size of 2
Ã—
2), ELU 
Conv2d (64 input channels, 64 output channels, 1 group)
MaxPool2d (stride of 3
Ã—
3, kernel size of 3
Ã—
3), ELU, Flatten 
Linear (576 input features, 128 output features), ELU 
Linear (128 input features, 10 outputs)
D.4Optimizer

For all experiments except those described in Appendix A.4, we always use the basic SGD optimizer of PyTorch, without any regularization or momentum. Here, SGD refers to the PyTorch optimizer that updates the parameters of the model in the opposite direction of the gradient, which, in our case, is replaced by the aggregation of the Jacobian matrix. In the rest of this paper, SGD refers to the whole stochastic gradient descent algorithm. In the experiments of Appendix A.4, we instead use Adam to study its interactions with JD.

D.5Loss function

The loss function is always the usual cross-entropy, with the default parameters of PyTorch.

D.6Preprocessing

The inputs are always normalized per channel based on the mean and standard deviation computed on the entire training split of the dataset.

D.7Iterations and computational budget

The numbers of epochs and the corresponding numbers of iterations for all datasets are provided in Table 6, along with the required number of NVIDIA L4 GPU-hours, to run all 72 learning rates for the 11 aggregators on a single seed. The total computational budget to run the main experiments on 8 seeds was thus around 760 GPU-hours. Additionally, we used a total of about 100 GPU hours for the experiments varying the batch size and using Adam, and about 200 more GPU hours were used for early investigations.

Table 6:Numbers of epochs, iterations, and GPU-hours for each dataset
Dataset	Epochs	Iterations	GPU-Hours
SVHN	25	800	17
CIFAR-10	20	640	15
EuroSAT	30	960	32
MNIST	8	256	6
Fashion-MNIST	25	800	17
Kuzushiji-MNIST	10	320	8
Appendix EComputation time and memory usage
E.1Memory considerations of SSJD for IWRM

The main overhead of SSJD on the IWRM objective comes from having to store the full Jacobian in memory. Remarkably, when we use SGD with ERM, every activation is a tensor whose first dimension is the batch size. Automatic differentiation engines thus have to compute the Jacobian anyway. Since the gradients can be averaged at each layer as soon as they are obtained, the full Jacobian does not have to be stored. In the naive implementation of SSJD, however, storing the Jacobian costs memory, which given the high parallelization ability of GPUs, increases the computational time. With the Gramian-based method proposed in Section 6, only the Gramian, which is typically small, has to be stored: the memory requirement would then be similar to that of SGD.

E.2Time complexity of the unconflicting projection of gradients

Let 
ğ½
âˆˆ
â„
ğ‘š
Ã—
ğ‘›
 be the matrix to aggregate. Apart from computing the Gramian 
ğ½
â¢
ğ½
âŠ¤
, applying 
ğ’œ
UPGrad
 to 
ğ½
 requires solving 
ğ‘š
 instances of the quadratic program (5) of Proposition 1. Solvers for such problems of dimension 
ğ‘š
 typically have a computational complexity upper bounded by 
ğ’ª
â¢
(
ğ‘š
4
)
 or less in recent implementations (e.g. 
ğ’ª
â¢
(
ğ‘š
3.67
â¢
log
â¡
ğ‘š
)
). This induces a 
ğ’ª
â¢
(
ğ‘š
5
)
 time complexity for extracting the weights of 
ğ’œ
UPGrad
. Note that solving these 
ğ‘š
 problems in parallel would reduce this complexity to 
ğ’ª
â¢
(
ğ‘š
4
)
.

E.3Empirical computational times

In Table 7, we compare the computation time of SGD with that of SSJD for all the aggregators that we experimented with. Since we used the same architecture for MNIST, Fashion-MNIST and Kuzushiji-MNIST, we only report the results for one of them. Several factors affect this computation time. First, the batch size affects the number of rows in the Jacobian to aggregate. Increasing the batch size thus requires more GPU memory and the aggregation of a taller matrix. Then, some aggregators, e.g. 
ğ’œ
Nash-MTL
 and 
ğ’œ
MGDA
, seem to greatly increase the run time. When the aggregation is the bottleneck, a faster implementation will be necessary to make them usable in practice. Lastly, the current implementation of JD in our library is still fairly inefficient in terms of memory management, which in turn limits how well the GPU can parallelize. Also, our implementation of 
ğ’œ
UPGrad
 does not solve the 
ğ‘š
 quadratic programs in parallel. Therefore, these results just give a rough indication of the current computation times.

Table 7:Time required in seconds for one epoch of training on the ERM objective with SGD and on the IWRM objective with SSJD and different aggregators, on an NVIDIA L4 GPU. The batch size is always 32.
Objective	Method	SVHN	CIFAR-10	EuroSAT	MNIST
ERM	SGD	0.79	0.50	0.81	0.47
IWRM	SSJDâ€“
ğ’œ
Mean
	1.41	1.76	2.93	1.64
IWRM	SSJDâ€“
ğ’œ
MGDA
	5.50	5.22	6.91	5.22
IWRM	SSJDâ€“
ğ’œ
DualProj
	1.51	1.88	3.02	1.76
IWRM	SSJDâ€“
ğ’œ
PCGrad
	2.78	3.13	4.18	3.01
IWRM	SSJDâ€“
ğ’œ
GradDrop
	1.57	1.90	3.06	1.78
IWRM	SSJDâ€“
ğ’œ
IMTL-G
	1.48	1.79	2.94	1.69
IWRM	SSJDâ€“
ğ’œ
CAGrad
	1.93	2.26	3.42	2.17
IWRM	SSJDâ€“
ğ’œ
RGW
	1.42	1.76	2.89	1.73
IWRM	SSJDâ€“
ğ’œ
Nash-MTL
	7.88	8.12	9.33	7.91
IWRM	SSJDâ€“
ğ’œ
Aligned-MTL
	1.53	1.98	2.97	1.71
IWRM	SSJDâ€“
ğ’œ
UPGrad
	1.80	2.01	3.21	1.90
Report Issue
Report Issue for Selection
Generated by L A T E xml 
Instructions for reporting errors

We are continuing to improve HTML versions of papers, and your feedback helps enhance accessibility and mobile support. To report errors in the HTML that will help us improve conversion and rendering, choose any of the methods listed below:

Click the "Report Issue" button.
Open a report feedback form via keyboard, use "Ctrl + ?".
Make a text selection and click the "Report Issue for Selection" button near your cursor.
You can use Alt+Y to toggle on and Alt+Shift+Y to toggle off accessible reporting links at each section.

Our team has already identified the following issues. We appreciate your time reviewing and reporting rendering errors we may not have found yet. Your efforts will help us improve the HTML versions for all readers, because disability should not be a barrier to accessing research. Thank you for your continued support in championing open access for all.

Have a free development cycle? Help support accessibility at arXiv! Our collaborators at LaTeXML maintain a list of packages that need conversion, and welcome developer contributions.
 I 